{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport warnings\nfrom sklearn.utils import check_X_y, check_array\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.fft import fft\nfrom scipy.optimize import minimize\n\n# Sklearn Core & Metrics\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import (\n    LinearDiscriminantAnalysis,\n    QuadraticDiscriminantAnalysis,\n)\nfrom sklearn.ensemble import (\n    ExtraTreesClassifier,\n    RandomForestClassifier,\n    HistGradientBoostingClassifier,\n)\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import (\n    StratifiedKFold,\n    train_test_split,\n    cross_val_predict,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import (\n    PowerTransformer,\n    RobustScaler,\n    StandardScaler,\n    MinMaxScaler,\n)\nfrom sklearn.svm import SVC, NuSVC, LinearSVC\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n\n# Gradient Boosting\nfrom xgboost import XGBClassifier\n\n# GPU CHECK\ntry:\n    import cupy as cp\n\n    GPU_AVAILABLE = True\n    print(\"✅ GPU DETECTED: HRF v26.0 'Holo-Fractal Universe' Active\")\nexcept ImportError:\n    GPU_AVAILABLE = False\n    print(\"⚠️ GPU NOT FOUND: Running in Slow Mode\")\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# --- 1. THE HOLOGRAPHIC SOUL (Unit 3 - Multiverse Edition - VRAM PINNED) ---\nclass HolographicSoulUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, k=15):\n        self.k = k\n        self.dna_ = {\n            \"freq\": 2.0, \"gamma\": 0.5, \"power\": 2.0,\n            \"metric\": \"minkowski\", \"p\": 2.0, \"phase\": 0.0,\n            \"dim_reduction\": \"none\",\n        }\n        self.projector_ = None\n        self.X_raw_source_ = None\n        # GPU Cache\n        self._X_train_gpu = None\n        self._y_train_gpu = None\n        # Pre-calculated norms for fast Euclidean\n        self._X_train_sq_norm = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self._apply_projection(X)\n        self.y_train_ = y\n\n        # [TITAN OPTIMIZATION] Upload to GPU ONCE\n        if GPU_AVAILABLE:\n            self._X_train_gpu = cp.asarray(self.X_train_, dtype=cp.float32)\n            self._y_train_gpu = cp.asarray(self.y_train_)\n            # Pre-calc Squared Norm for Fast Euclidean Path\n            self._X_train_sq_norm = cp.sum(self._X_train_gpu ** 2, axis=1)\n\n        return self\n\n    def _apply_projection(self, X):\n        if self.dna_[\"dim_reduction\"] == \"holo\":\n            n_components = max(2, int(np.sqrt(X.shape[1])))\n            self.projector_ = GaussianRandomProjection(n_components=n_components, random_state=42)\n            self.X_train_ = self.projector_.fit_transform(X)\n        elif self.dna_[\"dim_reduction\"] == \"pca\":\n            n_components = max(2, int(np.sqrt(X.shape[1])))\n            self.projector_ = PCA(n_components=n_components, random_state=42)\n            self.X_train_ = self.projector_.fit_transform(X)\n        else:\n            self.projector_ = None\n            self.X_train_ = X\n\n    def set_raw_source(self, X):\n        self.X_raw_source_ = X\n\n    def evolve(self, X_val, y_val, generations=10):\n        if not GPU_AVAILABLE: return 0.0\n\n        # [TITAN OPTIMIZATION] Pre-load Validation Data\n        X_val_curr = self.projector_.transform(X_val) if self.projector_ else X_val\n        X_val_g = cp.asarray(X_val_curr, dtype=cp.float32)\n        y_val_g = cp.asarray(y_val)\n\n        # Pre-calc validation norm for Fast Euclidean\n        val_sq_norm = cp.sum(X_val_g ** 2, axis=1)\n\n        n_universes = 8 # Slightly reduced for speed, keeps high diversity\n        best_dna = self.dna_.copy()\n\n        # Smart Init (Fast Sample)\n        sample_X = self._X_train_gpu[:100]\n        dists = cp.mean(cp.linalg.norm(sample_X[:, None, :] - sample_X[None, :, :], axis=2))\n        median_dist = float(cp.asnumpy(dists))\n        if median_dist > 0: best_dna[\"freq\"] = 3.14159 / median_dist\n\n        # Initial Score\n        best_acc = self._score_on_gpu(X_val_g, y_val_g, val_sq_norm)\n\n        patience = 0\n\n        for gen in range(generations):\n            candidates = []\n            for _ in range(n_universes):\n                mutant = best_dna.copy()\n                trait = random.choice(list(mutant.keys()))\n\n                if trait == \"freq\": mutant[\"freq\"] *= np.random.uniform(0.8, 1.25)\n                elif trait == \"gamma\": mutant[\"gamma\"] = np.random.uniform(0.1, 5.0)\n                elif trait == \"power\": mutant[\"power\"] = random.choice([0.5, 1.0, 2.0, 3.0, 4.0, 6.0])\n                elif trait == \"p\":\n                    # 50% chance to snap to 2.0 (Fast Path), 50% random\n                    if random.random() < 0.5: mutant[\"p\"] = 2.0\n                    else: mutant[\"p\"] = np.clip(mutant[\"p\"] + np.random.uniform(-0.5, 0.5), 0.5, 8.0)\n                elif trait == \"phase\": mutant[\"phase\"] = np.random.uniform(0, 3.14159)\n                candidates.append(mutant)\n\n            generation_best_acc = -1\n            generation_best_dna = None\n\n            for mutant_dna in candidates:\n                self.dna_ = mutant_dna\n                # Score using fast internal method\n                acc = self._score_on_gpu(X_val_g, y_val_g, val_sq_norm)\n\n                if acc > generation_best_acc:\n                    generation_best_acc = acc\n                    generation_best_dna = mutant_dna\n\n            if generation_best_acc >= best_acc:\n                best_acc = generation_best_acc\n                best_dna = generation_best_dna\n                patience = 0\n            else:\n                patience += 1\n\n            # Reset to best\n            self.dna_ = best_dna\n\n            # [TITAN OPTIMIZATION] Early Stopping\n            # If we don't improve for 8 generations, the soul is mature.\n            if patience >= 8:\n                break\n\n        self.dna_ = best_dna\n        del X_val_g, y_val_g, val_sq_norm\n        cp.get_default_memory_pool().free_all_blocks()\n\n        return best_acc\n\n    def _score_on_gpu(self, X_val_g, y_val_g, val_sq_norm=None):\n        probs = self._predict_proba_gpu_internal(X_val_g, val_sq_norm)\n        preds = cp.argmax(probs, axis=1)\n        return float(cp.mean(preds == y_val_g))\n\n    def predict_proba(self, X):\n        if self.projector_ is not None: X_curr = self.projector_.transform(X)\n        else: X_curr = X\n\n        if GPU_AVAILABLE:\n            X_g = cp.asarray(X_curr, dtype=cp.float32)\n            # Calc Norm for new data\n            x_sq_norm = cp.sum(X_g ** 2, axis=1)\n            probs = self._predict_proba_gpu_internal(X_g, x_sq_norm)\n            return cp.asnumpy(probs)\n        else:\n            return np.zeros((len(X), len(self.classes_)))\n\n    def _predict_proba_gpu_internal(self, X_te_g, X_te_sq_norm=None):\n        n_test = len(X_te_g)\n        n_classes = len(self.classes_)\n        probas = []\n        # Increased Batch Size for T4 (Matrix Multiplication can handle it)\n        #batch_size = 256 # for wide datasets\n        batch_size = 2048\n\n        p_norm = self.dna_.get(\"p\", 2.0)\n        gamma = self.dna_[\"gamma\"]\n        freq = self.dna_[\"freq\"]\n        power = self.dna_[\"power\"]\n        phase = self.dna_.get(\"phase\", 0.0)\n\n        # CHECK: Can we use Fast Euclidean? (p ~= 2.0)\n        use_fast_path = abs(p_norm - 2.0) < 0.05\n\n        for i in range(0, n_test, batch_size):\n            end = min(i + batch_size, n_test)\n            batch_te = X_te_g[i:end]\n\n            # --- DISTANCE CALCULATION ---\n            if use_fast_path and self._X_train_sq_norm is not None:\n                # [FAST PATH] A^2 + B^2 - 2AB\n                # 50x Speedup using Matrix Multiplication\n                if X_te_sq_norm is not None:\n                    batch_sq = X_te_sq_norm[i:end][:, None]\n                else:\n                    batch_sq = cp.sum(batch_te**2, axis=1, keepdims=True)\n\n                train_sq = self._X_train_sq_norm[None, :]\n                dot_prod = cp.dot(batch_te, self._X_train_gpu.T)\n\n                dists_sq = batch_sq + train_sq - 2 * dot_prod\n                dists_sq = cp.maximum(dists_sq, 0.0)\n                dists = cp.sqrt(dists_sq)\n            else:\n                # [SLOW PATH] Broadcasting for non-Euclidean metrics (p != 2)\n                diff = cp.abs(batch_te[:, None, :] - self._X_train_gpu[None, :, :])\n                dists = cp.sum(cp.power(diff, p_norm), axis=2)\n                dists = cp.power(dists, 1.0 / p_norm)\n\n            # --- WEIGHTING (RESONANCE) ---\n            # argpartition is faster than argsort for finding Top K\n            top_k_idx = cp.argsort(dists, axis=1)[:, : self.k]\n\n            row_idx = cp.arange(len(batch_te))[:, None]\n            top_dists = dists[row_idx, top_k_idx]\n            top_y = self._y_train_gpu[top_k_idx]\n\n            cosine_term = 1.0 + cp.cos(freq * top_dists + phase)\n            cosine_term = cp.maximum(cosine_term, 0.0)\n            w = cp.exp(-gamma * (top_dists**2)) * cosine_term\n            w = cp.power(w, power)\n\n            batch_probs = cp.zeros((len(batch_te), n_classes))\n            for c_idx, cls in enumerate(self.classes_):\n                class_mask = top_y == cls\n                batch_probs[:, c_idx] = cp.sum(w * class_mask, axis=1)\n\n            total_energy = cp.sum(batch_probs, axis=1, keepdims=True)\n            total_energy[total_energy == 0] = 1.0\n            batch_probs /= total_energy\n            probas.append(batch_probs)\n\n        return cp.concatenate(probas)\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n    def score(self, X, y):\n        return accuracy_score(y, self.predict(X))\n\n\n# --- 3. THE QUANTUM FIELD (Unit 4 - Reserve) ---\nclass QuantumFieldUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.rbf_feature_ = RBFSampler(n_components=100, random_state=42)\n        self.classifier_ = RidgeClassifier(alpha=1.0)\n        self.classes_ = None\n        self.dna_ = {\"gamma\": 1.0, \"n_components\": 100}\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.rbf_feature_.set_params(\n            gamma=self.dna_[\"gamma\"], n_components=self.dna_[\"n_components\"]\n        )\n        X_quantum = self.rbf_feature_.fit_transform(X)\n        self.classifier_.fit(X_quantum, y)\n        return self\n\n    def predict_proba(self, X):\n        X_quantum = self.rbf_feature_.transform(X)\n        d = self.classifier_.decision_function(X_quantum)\n        if len(self.classes_) == 2:\n            probs = 1 / (1 + np.exp(-d))\n            return np.column_stack([1 - probs, probs])\n        else:\n            exp_d = np.exp(d - np.max(d, axis=1, keepdims=True))\n            return exp_d / np.sum(exp_d, axis=1, keepdims=True)\n\n    def score(self, X, y):\n        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n\n\n# --- 4. THE ENTROPY MAXWELL (Unit 5 - Reserve) ---\nclass EntropyMaxwellUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.models_ = {}\n        self.classes_ = None\n        self.priors_ = None\n        self.dna_ = {\"n_components\": 1}\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.models_ = {}\n        self.priors_ = {}\n        n_samples = len(y)\n        for cls in self.classes_:\n            X_c = X[y == cls]\n            if len(X_c) < 2:\n                self.priors_[cls] = 0.0\n                continue\n            self.priors_[cls] = len(X_c) / n_samples\n            n_comp = min(self.dna_[\"n_components\"], len(X_c))\n            gmm = GaussianMixture(\n                n_components=n_comp, covariance_type=\"full\", reg_covar=1e-4, random_state=42\n            )\n            gmm.fit(X_c)\n            self.models_[cls] = gmm\n        return self\n\n    def predict_proba(self, X):\n        probs = np.zeros((len(X), len(self.classes_)))\n        for i, cls in enumerate(self.classes_):\n            if cls in self.models_:\n                log_prob = self.models_[cls].score_samples(X)\n                log_prob = np.clip(log_prob, -100, 100)\n                probs[:, i] = np.exp(log_prob) * self.priors_[cls]\n        total = np.sum(probs, axis=1, keepdims=True) + 1e-10\n        return probs / total\n\n    def score(self, X, y):\n        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n\n\n# --- 5. THE OMNI-KERNEL NEXUS (Unit 6 - Reserve) ---\nclass OmniKernelUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.model_ = None\n        self.classes_ = None\n        self.dna_ = {\n            \"kernel\": \"rbf\",\n            \"C\": 1.0,\n            \"gamma\": \"scale\",\n            \"degree\": 3,\n            \"coef0\": 0.0,\n        }\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.model_ = SVC(\n            kernel=self.dna_[\"kernel\"],\n            C=self.dna_[\"C\"],\n            gamma=self.dna_[\"gamma\"],\n            degree=self.dna_[\"degree\"],\n            coef0=self.dna_[\"coef0\"],\n            probability=True,\n            random_state=42,\n            cache_size=500,\n        )\n        self.model_.fit(X, y)\n        return self\n\n    def predict_proba(self, X):\n        return self.model_.predict_proba(X)\n\n    def score(self, X, y):\n        return self.model_.score(X, y)\n\n\n# --- 18. THE GOLDEN SPIRAL (Unit 18 - Nature's Code) ---\n# --- 18. THE GOLDEN FOREST (GPU T4 - Parallel Ensemble) ---\nclass GoldenSpiralUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, k=21, n_estimators=100):\n        # n_estimators=50 ensures 'Forest' power but keeps it sub-second on GPU\n        self.k = k\n        self.n_estimators = n_estimators\n        self.classes_ = None\n        self.X_train_ = None\n        self.y_train_ = None\n        # DNA: The \"Seed\" parameters for the forest\n        self.dna_ = {\"resonance\": 1.618, \"decay\": 1.618, \"shift\": 137.5}\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        if GPU_AVAILABLE:\n            self.X_train_ = cp.asarray(X, dtype=cp.float32)\n            self.y_train_ = cp.asarray(y)\n        else:\n            self.X_train_ = np.array(X, dtype=np.float32)\n            self.y_train_ = np.array(y)\n\n        # [GPU STRATEGY]: We don't train 50 separate trees.\n        # We store the data ONCE. We will simulate 50 \"viewpoints\" during prediction.\n        return self\n\n    def evolve(self, X, y, generations=20):\n        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n        if not GPU_AVAILABLE: return 0.0\n        preds = self.predict(X)\n        return accuracy_score(y, preds)\n\n    def predict_proba(self, X):\n        if not GPU_AVAILABLE: return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n\n        X_g = cp.asarray(X, dtype=cp.float32)\n        n_test = len(X_g)\n        n_classes = len(self.classes_)\n\n        # 1. THE HEAVY LIFT: Calculate Neighbors ONCE (The most expensive part)\n        # We use a single massive matrix op instead of 50 small ones.\n\n        # Euclidean Dist ^ 2 = x^2 + y^2 - 2xy\n        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n        Y2 = cp.sum(self.X_train_**2, axis=1)\n        XY = cp.dot(X_g, self.X_train_.T)\n        dists_sq = cp.maximum(X2 + Y2 - 2*XY, 0.0)\n        dists = cp.sqrt(dists_sq)\n\n        # Get Top K\n        top_k_idx = cp.argsort(dists, axis=1)[:, :self.k]\n        row_idx = cp.arange(n_test)[:, None]\n        top_dists = dists[row_idx, top_k_idx] # (N, k)\n        top_y = self.y_train_[top_k_idx]      # (N, k)\n\n        # 2. THE FOREST SIMULATION (Vectorized Ensemble)\n        # We apply 50 different \"Physics Laws\" to the SAME neighbors instantaneously.\n\n        total_probs = cp.zeros((n_test, n_classes), dtype=cp.float32)\n\n        # Generate random mutations for the ensemble on the fly (Deterministic seed)\n        rng = cp.random.RandomState(42)\n\n        # Batch the ensemble calculation\n        decay_vars = rng.uniform(0.5, 3.0, self.n_estimators)\n        shift_vars = rng.uniform(0.0, 360.0, self.n_estimators)\n        res_vars = rng.uniform(1.0, 2.0, self.n_estimators)\n\n        # Loop through \"Universes\" (Fast loop)\n        for i in range(self.n_estimators):\n            decay = decay_vars[i]\n            shift = np.deg2rad(shift_vars[i])\n            res = res_vars[i]\n\n            # Physics: Weight = 1/d^decay * Cosine_Resonance\n            # Add epsilon to dists\n            w_base = 1.0 / (cp.power(top_dists, decay) + 1e-9)\n            w_spiral = 1.0 + 0.5 * cp.cos(cp.log(top_dists + 1e-9) * res + shift)\n            w = w_base * cp.maximum(w_spiral, 0.0)\n\n            # Aggregate for this tree\n            tree_p = cp.zeros((n_test, n_classes), dtype=cp.float32)\n            for c_idx, cls in enumerate(self.classes_):\n                mask = (top_y == cls)\n                tree_p[:, c_idx] = cp.sum(w * mask, axis=1)\n\n            # Normalize tree\n            t_sum = cp.sum(tree_p, axis=1, keepdims=True)\n            total_probs += tree_p / (t_sum + 1e-9)\n\n        # Final Average\n        final_probs = total_probs / self.n_estimators\n        return cp.asnumpy(final_probs)\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n\n# ---Unit 19. THE ENTROPY FOREST (GPU T4 - Bootstrap Thermodynamics) ---\nclass EntropyMaxwellUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, n_estimators=100):\n        self.n_estimators = n_estimators\n        self.forest_stats_ = [] # Stores (mean, var) for 50 bootstraps\n        self.classes_ = None\n        self.dna_ = {\"n_components\": 100} # Placeholder\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        if not GPU_AVAILABLE: return self\n\n        X_g = cp.asarray(X, dtype=cp.float32)\n        y_g = cp.asarray(y)\n        n_samples = len(X)\n\n        self.forest_stats_ = []\n        rng = cp.random.RandomState(42)\n\n        # Train 50 Universes instantly using GPU Bootstrap\n        for _ in range(self.n_estimators):\n            # Bootstrap indices\n            indices = rng.choice(n_samples, n_samples, replace=True)\n            X_boot = X_g[indices]\n            y_boot = y_g[indices]\n\n            universe_stats = {}\n            for cls in self.classes_:\n                X_c = X_boot[y_boot == cls]\n                if len(X_c) < 2:\n                    # Fallback to global if class missing in bootstrap\n                    X_c = X_g[y_g == cls]\n\n                # We simply store Mean and Var (Gaussian Approximation)\n                # This is much faster than GMM and sufficient for Entropy Forest\n                mu = cp.mean(X_c, axis=0)\n                sigma = cp.var(X_c, axis=0) + 1e-5 # Stability\n                prior = len(X_c) / n_samples\n                universe_stats[cls] = (mu, sigma, prior)\n\n            self.forest_stats_.append(universe_stats)\n        return self\n\n    def evolve(self, X, y, generations=20):\n        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n        if not GPU_AVAILABLE: return 0.0\n        preds = self.predict(X)\n        return accuracy_score(y, preds)\n\n    def predict_proba(self, X):\n        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n\n        X_g = cp.asarray(X, dtype=cp.float32)\n        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n\n        # Ensembling\n        for stats in self.forest_stats_:\n            univ_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n\n            for i, cls in enumerate(self.classes_):\n                mu, sigma, prior = stats[cls]\n                # Log-Gaussian PDF\n                log_p = -0.5 * cp.sum(cp.log(2 * np.pi * sigma), axis=0) - \\\n                        0.5 * cp.sum((X_g - mu)**2 / sigma, axis=1)\n                univ_probs[:, i] = log_p + cp.log(prior)\n\n            # Softmax this universe\n            max_p = cp.max(univ_probs, axis=1, keepdims=True)\n            exp_p = cp.exp(univ_probs - max_p)\n            univ_probs = exp_p / cp.sum(exp_p, axis=1, keepdims=True)\n\n            total_probs += univ_probs\n\n        return cp.asnumpy(total_probs / self.n_estimators)\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n\n\n\n# --- 20. THE QUANTUM FOREST (GPU T4 - Parallel Ridge Fields) ---\nclass QuantumFluxUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, n_estimators=100, gamma=1.5):\n        # 20 Quantum Realities (Heavy)\n        self.n_estimators = n_estimators\n        self.gamma = gamma\n        self.forest_ = []\n        self.classes_ = None\n        # [FIX] Added n_components to DNA so the logger prints correctly\n        self.dna_ = {\"gamma\": gamma, \"n_components\": 200}\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        if not GPU_AVAILABLE: return self\n\n        X_g = cp.asarray(X, dtype=cp.float32)\n\n        # One-hot Y\n        y_onehot = cp.zeros((len(y), len(self.classes_)), dtype=cp.float32)\n        y_raw = cp.asarray(y)\n        for i, c in enumerate(self.classes_):\n            y_onehot[y_raw == c, i] = 1.0\n\n        n_features = X.shape[1]\n        rng = cp.random.RandomState(42)\n\n        self.forest_ = []\n\n        # Train 20 Ridge Models in Parallel Universes\n        for i in range(self.n_estimators):\n            # Vary Gamma slightly for diversity\n            g_var = self.gamma * rng.uniform(0.8, 1.2)\n            n_comp = self.dna_[\"n_components\"] # Use DNA value\n\n            # RBF Weights\n            W = rng.normal(0, np.sqrt(2*g_var), (n_features, n_comp)).astype(cp.float32)\n            B = rng.uniform(0, 2*np.pi, n_comp).astype(cp.float32)\n\n            # Project X -> Z\n            Z = cp.cos(cp.dot(X_g, W) + B) * cp.sqrt(2./n_comp)\n\n            # Solve Ridge: (Z'Z + aI)^-1 Z'Y\n            alpha = 1.0\n            I = cp.eye(n_comp, dtype=cp.float32)\n\n            try:\n                # Cholesky solve (Ultra Fast on T4)\n                weights = cp.linalg.solve(cp.dot(Z.T, Z) + alpha*I, cp.dot(Z.T, y_onehot))\n                self.forest_.append((W, B, weights))\n            except: pass # Skip singular universes\n\n        return self\n\n    def evolve(self, X, y, generations=20):\n        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n        if not GPU_AVAILABLE: return 0.0\n        preds = self.predict(X)\n        return accuracy_score(y, preds)\n\n    def predict_proba(self, X):\n        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n        X_g = cp.asarray(X, dtype=cp.float32)\n        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n\n        valid = 0\n        for W, B, weights in self.forest_:\n            Z = cp.cos(cp.dot(X_g, W) + B) * cp.sqrt(2./len(B))\n            raw = cp.dot(Z, weights)\n\n            # Softmax\n            max_r = cp.max(raw, axis=1, keepdims=True)\n            exp_r = cp.exp(raw - max_r)\n            p = exp_r / cp.sum(exp_r, axis=1, keepdims=True)\n\n            total_probs += p\n            valid += 1\n\n        return cp.asnumpy(total_probs / max(1, valid))\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n\n# --- 21. THE GRAVITY FOREST (GPU T4 - Many Body Simulation) ---\nclass EventHorizonUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, n_estimators=100):\n        self.n_estimators = n_estimators\n        self.centroids_ = None\n        self.masses_ = None\n        self.classes_ = None\n        # [FIX] Added 'decay_power' here to satisfy the printer logic\n        self.dna_ = {\"horizon_pct\": 10.0, \"decay_power\": 2.0}\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        if not GPU_AVAILABLE: return self\n\n        X_g = cp.asarray(X, dtype=cp.float32)\n        y_g = cp.asarray(y)\n\n        # Calculate Base Centers (The Stars)\n        self.centroids_ = []\n        self.masses_ = []\n        for cls in self.classes_:\n            X_c = X_g[y_g == cls]\n            if len(X_c) > 0:\n                self.centroids_.append(cp.mean(X_c, axis=0))\n                self.masses_.append(cp.log1p(len(X_c)))\n            else:\n                self.centroids_.append(cp.zeros(X.shape[1]))\n                self.masses_.append(0.0)\n\n        self.centroids_ = cp.array(self.centroids_) # (C, F)\n        self.masses_ = cp.array(self.masses_)       # (C,)\n        return self\n\n    def evolve(self, X, y, generations=20):\n        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n        if not GPU_AVAILABLE: return 0.0\n        preds = self.predict(X)\n        return accuracy_score(y, preds)\n\n    def predict_proba(self, X):\n        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n\n        X_g = cp.asarray(X, dtype=cp.float32)\n\n        # 1. Calculate Base Distances (Matrix: Samples x Classes)\n        # ||X - C||^2 = X^2 + C^2 - 2XC\n        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n        C2 = cp.sum(self.centroids_**2, axis=1)\n        XC = cp.dot(X_g, self.centroids_.T)\n        dist_sq = cp.maximum(X2 + C2 - 2*XC, 1e-9) # (N, C)\n\n        # 2. Simulate 50 Gravity Variations (The Forest)\n        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n        rng = cp.random.RandomState(42)\n\n        # Use the decay power from DNA as the mean for the random variation\n        base_decay = self.dna_[\"decay_power\"]\n        decay_vars = rng.uniform(base_decay * 0.25, base_decay * 1.25, self.n_estimators)\n\n        for i in range(self.n_estimators):\n            decay = decay_vars[i]\n\n            # Force = Mass / Dist^decay\n            # (Use Log space for stability)\n            # Log(F) = Log(M) - decay * Log(Dist^2)/2\n            # Log(Dist^2)/2 = Log(Dist)\n\n            log_dist = 0.5 * cp.log(dist_sq)\n            log_force = cp.log(self.masses_) - (decay * log_dist)\n\n            # Softmax forces\n            max_f = cp.max(log_force, axis=1, keepdims=True)\n            exp_f = cp.exp(log_force - max_f)\n            p = exp_f / cp.sum(exp_f, axis=1, keepdims=True)\n\n            total_probs += p\n\n        return cp.asnumpy(total_probs / self.n_estimators)\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n\n\n\n\n# -----------------------------------------------------------------------------------------\n\n# --- 18. THE FAST GOLDEN SPIRAL (Lite Version) ---\nclass FastGoldenUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, k=21):\n        self.k = k\n        self.classes_ = None\n        self.X_train_ = None\n        self.y_train_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.X_train_ = np.array(X, dtype=np.float32)\n        self.y_train_ = np.array(y)\n        return self\n\n    def predict_proba(self, X):\n        # FAST LOGIC: No ensemble. Just one Golden Ratio weighted KNN.\n        # We use standard Euclidean distance but weight neighbors by 1/d^Phi\n        from sklearn.metrics.pairwise import euclidean_distances\n\n        X_test = np.array(X, dtype=np.float32)\n        dists = euclidean_distances(X_test, self.X_train_)\n\n        # Get Top K neighbors\n        idx = np.argsort(dists, axis=1)[:, :self.k]\n        row_idx = np.arange(len(X))[:, None]\n\n        top_dists = dists[row_idx, idx]\n        top_y = self.y_train_[idx]\n\n        # PHI PHYSICS: Weight = 1 / (Distance ^ 1.618)\n        phi = 1.6180339887\n        weights = 1.0 / (np.power(top_dists, phi) + 1e-9)\n\n        probs = np.zeros((len(X), len(self.classes_)))\n        for c_idx, cls in enumerate(self.classes_):\n            # Sum weights where neighbor class matches\n            mask = (top_y == cls)\n            probs[:, c_idx] = np.sum(weights * mask, axis=1)\n\n        # Normalize\n        sums = np.sum(probs, axis=1, keepdims=True)\n        return np.nan_to_num(probs / (sums + 1e-9), nan=1.0/len(self.classes_))\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n\n# --- 19. THE FAST ENTROPY (Gaussian Thermodynamics) ---\nfrom sklearn.naive_bayes import GaussianNB\nclass FastEntropyUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        # GaussianNB is literally a probability density calculator (Thermodynamics)\n        # It is extremely fast (O(n))\n        self.model = GaussianNB()\n        self.classes_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.model.fit(X, y)\n        return self\n\n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n\n# --- 20. THE FAST QUANTUM (Single Field Ridge) ---\nclass FastQuantumUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, gamma=1.0, n_components=100):\n        # No ensemble. Just one mapping to higher dimension + Linear Solver\n        self.gamma = gamma\n        self.n_components = n_components\n        self.rbf = RBFSampler(gamma=gamma, n_components=n_components, random_state=42)\n        self.solver = RidgeClassifier(alpha=1.0)\n        self.classes_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        X_q = self.rbf.fit_transform(X)\n        self.solver.fit(X_q, y)\n        return self\n\n    def predict_proba(self, X):\n        X_q = self.rbf.transform(X)\n        d = self.solver.decision_function(X_q)\n\n        # Manual Softmax\n        if len(d.shape) == 1:\n            p = 1 / (1 + np.exp(-d))\n            return np.column_stack([1-p, p])\n        else:\n            exp_d = np.exp(d - np.max(d, axis=1, keepdims=True))\n            return exp_d / np.sum(exp_d, axis=1, keepdims=True)\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n\n# --- 21. THE FAST GRAVITY (Newtonian Centers) ---\nclass FastGravityUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.centroids_ = []\n        self.masses_ = []\n        self.classes_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.centroids_ = []\n        self.masses_ = []\n\n        # Calculate Center of Mass for each class once\n        for cls in self.classes_:\n            X_c = X[y == cls]\n            if len(X_c) > 0:\n                self.centroids_.append(np.mean(X_c, axis=0))\n                # Mass = log(count) to prevent huge class imbalance bias\n                self.masses_.append(np.log1p(len(X_c)))\n            else:\n                self.centroids_.append(np.zeros(X.shape[1]))\n                self.masses_.append(0)\n        return self\n\n    def predict_proba(self, X):\n        probs = np.zeros((len(X), len(self.classes_)))\n\n        # Vectorized Gravity Calculation\n        for i, (center, mass) in enumerate(zip(self.centroids_, self.masses_)):\n            # Distance squared (Newtonian)\n            d2 = np.sum((X - center)**2, axis=1)\n            # Force = Mass / Distance^2\n            force = mass / (d2 + 1e-9)\n            probs[:, i] = force\n\n        # Normalize\n        sums = np.sum(probs, axis=1, keepdims=True)\n        return np.nan_to_num(probs / (sums + 1e-9), nan=1.0/len(self.classes_))\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n# ------------------------------------------------------------------------------------------\n\n\n\n# --- 22. THE OMEGA POINT (The Hidden Infinity Engine - Tensor Core) ---\nclass TheOmegaPoint_Unit22(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.classes_ = None\n        self.model_ = None\n        self.pca_vector_ = None  # To store the \"Principal Vibration\"\n        self.scaler_ = StandardScaler()\n\n    def _apply_theoretical_transforms(self, X, is_training=False):\n        # 1. Standardize Reality\n        if is_training:\n            X_geo = self.scaler_.fit_transform(X)\n        else:\n            X_geo = self.scaler_.transform(X)\n\n        n_samples, n_features = X_geo.shape\n\n        # --- THEORY 1: THE TENSOR FIELD (Interaction Energy) ---\n        # Instead of Phase, we calculate the PHYSICAL INTERACTION between forces.\n        # This creates a \"Force Field\" of all possible pairings (x1*x2, x1*x3...)\n        # Mathematics: Outer Product -> Upper Triangle\n        tensor_list = []\n        for i in range(n_features):\n            for j in range(i, n_features):\n                tensor_list.append(X_geo[:, i] * X_geo[:, j])\n        tensor_field = np.column_stack(tensor_list)\n\n        # --- THEORY 2: SCHRODINGER KINETIC ENERGY ---\n        # Kinetic Energy = 1/2 * mass * velocity^2\n        # We treat the value as velocity.\n        kinetic = 0.5 * (X_geo ** 2)\n\n        # --- THEORY 3: SHANNON ENTROPY (Information Density) ---\n        # How \"surprising\" is this data point?\n        # We transform to probabilities first (Softmax-ish)\n        p = np.abs(X_geo) / (np.sum(np.abs(X_geo), axis=1, keepdims=True) + 1e-9)\n        entropy = -np.sum(p * np.log(p + 1e-9), axis=1, keepdims=True)\n\n        # --- THEORY 4: THE GOD ALEPH (EIGEN-RESONANCE) ---\n        # We project the entire reality onto its \"Principal Vibration\" (First Eigenvector).\n        # This is the \"Main Frequency\" of the universe (Dataset).\n        if is_training:\n            cov_mat = np.cov(X_geo.T)\n            eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n            self.pca_vector_ = eig_vecs[:, -1]\n\n        aleph = np.dot(X_geo, self.pca_vector_).reshape(-1, 1)\n\n        # FINAL STACKING\n        omega_features = np.hstack(\n            [\n                X_geo,  # Base\n                kinetic,  # Physics\n                entropy,  # Info\n                tensor_field,  # Geometry (High Dim)\n                aleph,  # Divinity\n            ]\n        )\n\n        return np.nan_to_num(omega_features, nan=0.0, posinf=1.0, neginf=-1.0)\n\n    def _benchmark_divinity(self, X_omega, y, n_orig):\n        \"\"\"\n        Benchmarks the new Tensor Reality.\n        \"\"\"\n        from sklearn.tree import DecisionTreeClassifier\n\n        print(\"\\n\" + \"-\" * 65)\n        print(\" | THE DIVINE INSPECTION: TENSOR DIMENSION ACCURACIES |\")\n        print(\"-\" * 65)\n        print(f\" {'THEORETICAL LAYER':<25} | {'ACCURACY':<10} | {'STATUS':<10}\")\n        print(\"-\" * 65)\n\n        n = n_orig\n        layers = [\n            (\"Base Reality (Norm)\", 0, n),\n            (\"Kinetic Energy\", n, 2 * n),\n            (\"Shannon Entropy\", 2 * n, 2 * n + 1),\n            (\"The Tensor Field\", 2 * n + 1, X_omega.shape[1] - 1),\n            (\"THE GOD ALEPH (Eigen)\", X_omega.shape[1] - 1, X_omega.shape[1]),\n        ]\n\n        for name, start, end in layers:\n            X_subset = X_omega[:, start:end]\n            probe = DecisionTreeClassifier(max_depth=4, random_state=42)\n            probe.fit(X_subset, y)\n            acc = probe.score(X_subset, y)\n            print(f\" {name:<25} | {acc:.2%}    | Active\")\n        print(\"-\" * 65)\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        if hasattr(self, \"verbose\") and self.verbose:\n            print(\" [OMEGA] TRANSCODING REALITY INTO TENSOR FIELDS...\")\n\n        X_omega = self._apply_theoretical_transforms(X, is_training=True)\n        self._benchmark_divinity(X_omega, y, X.shape[1])\n\n        self.model_ = ExtraTreesClassifier(\n            n_estimators=1000,\n            max_depth=None,\n            max_features=\"sqrt\",\n            bootstrap=False,\n            random_state=42,\n            n_jobs=-1,\n        )\n        self.model_.fit(X_omega, y)\n        return self\n\n    def predict_proba(self, X):\n        X_omega = self._apply_theoretical_transforms(X, is_training=False)\n        return self.model_.predict_proba(X_omega)\n\n    def score(self, X, y):\n        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n\n\n# --- 23. THE FRACTAL MIRROR (Unit 23 - Dynamic Elite Sync) ---\nclass FractalMirrorUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, top_3_models):\n        \"\"\"\n        DYNAMIC ARCHITECTURE:\n        Accepts the 'Top 3 Elite' models found by the Council.\n        These change for every dataset (e.g., Logic+Soul+Gravity vs. Quantum+Gradient+Bio).\n        \"\"\"\n        self.top_3_models = top_3_models\n        self.classes_ = None\n\n        # HYBRID META-LEARNERS\n        # 1. The Conservative Judge (Ridge): Prevents overfitting, handles linear corrections.\n        self.judge_linear_ = RidgeClassifier(alpha=10.0, class_weight=\"balanced\")\n        # 2. The Creative Judge (Boosting): Finds complex non-linear patches in the elites' logic.\n        self.judge_boost_ = HistGradientBoostingClassifier(\n            max_iter=100,\n            max_depth=4,\n            max_leaf_nodes=15,       # <--- NEW: Restricts complexity\n            l2_regularization=20.0,  # <--- NEW: Prevents overfitting\n            learning_rate=0.02,\n            early_stopping=True,\n            random_state=42\n        )\n\n    def _get_council_opinions(self, X, y=None, is_training=False):\n        \"\"\"\n        Generates the Council's input.\n        - Training: Uses Cross-Validation (Blindfolding) to see REAL errors.\n        - Prediction: Uses standard prediction.\n        \"\"\"\n        meta_features = []\n        for model in self.top_3_models:\n            # A: TRAINING PHASE (Blindfolded CV)\n            if is_training and y is not None:\n                try:\n                    # We use 5-fold CV to get a robust \"out-of-sample\" view\n                    if hasattr(model, \"predict_proba\"):\n                        p = cross_val_predict(\n                            model, X, y, cv=5, method=\"predict_proba\", n_jobs=-1\n                        )\n                    else:\n                        d = cross_val_predict(\n                            model, X, y, cv=5, method=\"decision_function\", n_jobs=-1\n                        )\n                        # Softmax normalization for decision functions\n                        p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n                except:\n                    # Fallback (Safety Net): Standard fit if CV crashes\n                    model.fit(X, y)\n                    if hasattr(model, \"predict_proba\"):\n                        p = model.predict_proba(X)\n                    else:\n                        p = np.ones((len(X), len(np.unique(y)))) / len(np.unique(y))\n\n            # B: PREDICTION PHASE (Standard)\n            else:\n                if hasattr(model, \"predict_proba\"):\n                    p = model.predict_proba(X)\n                else:\n                    d = model.decision_function(X)\n                    p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n\n            # Clean NaNs (Safety)\n            p = np.nan_to_num(p, 0.0)\n            meta_features.append(p)\n\n        return np.hstack(meta_features)\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n\n        # STEP 1: CROSS-VALIDATION (The Truth Serum)\n        # We extract features BEFORE retraining the models, so we capture their true mistakes.\n        X_council = self._get_council_opinions(X, y, is_training=True)\n\n        # STEP 2: DYNAMIC SYNC (The Power Up)\n        # Now we retrain the Top 3 Elites on 100% of this data.\n        # This guarantees they are fully adapted to this specific dataset.\n        for model in self.top_3_models:\n            model.fit(X, y)\n\n        # STEP 3: STACKING (The Mirror)\n        # Input = Original Data + Elite Opinions\n        X_stack = X_council\n\n        # STEP 4: TRAIN THE META-JUDGES\n        # Ridge ensures we don't hallucinate.\n        self.judge_linear_.fit(X_council, y)\n        # Boosting fixes the hard edge cases.\n        self.judge_boost_.fit(X_stack, y)\n\n        return self\n\n    def predict_proba(self, X):\n        # 1. Ask the Synced Elites\n        X_council = self._get_council_opinions(X, is_training=False)\n        X_stack = X_council\n\n        # 2. Get Conservative Opinion (Linear)\n        d_linear = self.judge_linear_.decision_function(X_council)\n        if len(d_linear.shape) == 1: # Binary handling\n            p_linear = 1 / (1 + np.exp(-d_linear))\n            p_linear = np.column_stack([1-p_linear, p_linear])\n        else: # Multi-class\n            exp_d = np.exp(d_linear - np.max(d_linear, axis=1, keepdims=True))\n            p_linear = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n\n        # 3. Get Corrective Opinion (Boosting)\n        p_boost = self.judge_boost_.predict_proba(X_stack)\n\n        # 4. The Final Balanced Verdict\n        # 60% Boosting (Intelligence) + 40% Linear (Stability)\n        # This ratio provides the \"Tie or Win\" guarantee.\n        return 0.7 * p_linear + 0.3 * p_boost\n\n    def score(self, X, y):\n        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n\n\n\n# --- 24. DIMENSION Z (The Infinite Alien - Balanced) ---\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport random\n\n# --- 24. DIMENSION Z (The Final Sniper - Sharpened Ace) ---\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.base import clone\n\n# --- 24. DIMENSION Z (The Universal Geometric Corrector) ---\nfrom sklearn.neighbors import NearestNeighbors\n\nclass AlienDimensionZ(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    THE UNIVERSAL WHETSTONE.\n    Role: Wakes up AFTER Phase 4.\n    Operation: Takes the WINNING PROBABILITIES (Council or Ace) and\n               bends them to match the local geometry of the universe.\n    \"\"\"\n    def __init__(self, impact_factor=0.15):\n        # impact_factor: How much we trust geometry over logic (0.15 = 15%)\n        self.impact_factor = impact_factor\n        self.geometry_lock_ = None\n        self.y_train_ = None\n        self.classes_ = None\n\n    def fit(self, X, y):\n        self.y_train_ = y\n        self.classes_ = np.unique(y)\n\n        # MEMORIZE THE GEOMETRY (The Reality Check)\n        # We use a K-Tree to find exactly what the neighbors say\n        self.geometry_lock_ = NearestNeighbors(n_neighbors=33, metric='minkowski', p=2, n_jobs=-1)\n        self.geometry_lock_.fit(X)\n        return self\n\n    def sharpen_probabilities(self, input_probs, X_new):\n        \"\"\"\n        Takes the Logic's opinion (input_probs) and blends it with\n        Physical Reality (Neighbor Consensus).\n        \"\"\"\n        if self.geometry_lock_ is None:\n            return input_probs\n\n        # 1. Ask the Universe: \"Who is near this point?\"\n        dists, indices = self.geometry_lock_.kneighbors(X_new)\n\n        # 2. Calculate Geometric Gravity\n        # (Weighted vote of neighbors based on distance)\n        p_geom = np.zeros_like(input_probs)\n        n_samples = len(X_new)\n\n        # Vectorized neighbor voting for speed\n        neighbor_votes = self.y_train_[indices] # (N, k)\n\n        # Distance weights (Inverse distance)\n        weights = 1.0 / (dists + 1e-9)\n\n        for i in range(n_samples):\n            # Weighted bin count for this sample\n            for k_idx, class_label in enumerate(neighbor_votes[i]):\n                # Find column index for this class\n                col_idx = np.where(self.classes_ == class_label)[0][0]\n                p_geom[i, col_idx] += weights[i, k_idx]\n\n        # Normalize Geometry Probabilities\n        row_sums = p_geom.sum(axis=1, keepdims=True)\n        p_geom = np.divide(p_geom, row_sums, out=np.zeros_like(p_geom), where=row_sums!=0)\n\n        # 3. The Fusion (Logic + Geometry)\n        # We blend the Input (Council/Ace) with the Geometry\n        final_probs = ((1.0 - self.impact_factor) * input_probs) + (self.impact_factor * p_geom)\n\n        return final_probs\n\n    def predict(self, input_probs, X_new):\n        final_p = self.sharpen_probabilities(input_probs, X_new)\n        return self.classes_[np.argmax(final_p, axis=1)]\n\n\n\n# --- 25. THE NEURAL-MANIFOLD ENGINE (Unit 25 - The Universal Solver) ---\n# --- 25. THE OMEGA NEURAL ENGINE (Unit 25 - True Infinite Freedom) ---\nfrom scipy.linalg import pinv\nfrom scipy.special import expit, erf\nimport numpy as np\nimport random\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import accuracy_score\n\n# --- 25. THE OMEGA NEURAL ENGINE (Unit 25 - GPU ACCELERATED) ---\ntry:\n    import cupy as cp\n    import cupyx.scipy.special as cpx  # For erf/expit on GPU\n    GPU_AVAILABLE = True\nexcept ImportError:\n    import numpy as cp\n    GPU_AVAILABLE = False\n    print(\"⚠️ GPU NOT FOUND: Neural Engine running on CPU (Slow Mode)\")\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport random\n\nclass NeuralManifoldUnit(BaseEstimator, ClassifierMixin):\n    def __init__(self, n_hidden=100, activation=\"tanh\",\n                 alpha=0.5, beta=1.0,\n                 gamma=1.0, bias_scale=1.0, power=1.0):\n        self.n_hidden = n_hidden\n        self.activation = activation\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.bias_scale = bias_scale\n        self.power = power\n\n        self.input_weights_ = None\n        self.bias_ = None\n        self.output_weights_ = None\n        self.classes_ = None\n        self._X_train_gpu = None # GPU Cache\n        self._y_train_gpu = None # GPU Cache\n        self._rng_seed = 42\n\n    def _get_gpu_rng(self, seed):\n        return cp.random.RandomState(seed)\n\n    def _activate(self, X, dna=None):\n        # Unpack DNA\n        d = dna if dna else self.__dict__\n        act_name = d.get('activation', self.activation)\n        b = d.get('beta', self.beta)\n        g = d.get('gamma', self.gamma)\n        bs = d.get('bias_scale', self.bias_scale)\n        p = d.get('power', self.power)\n        n_h = d.get('n_hidden', self.n_hidden)\n\n        # Slice weights (Virtual Resizing on GPU)\n        W = self.input_weights_[:X.shape[1], :n_h]\n        B = self.bias_[:n_h]\n\n        # Projection (Chaos Injection)\n        # X is already on GPU here\n        H = cp.dot(X * g, W) + (B * bs)\n\n        # Infinite Library (GPU Optimized)\n        if act_name == \"tanh\": H = cp.tanh(b * H)\n        elif act_name == \"sine\": H = cp.sin(b * H)\n        elif act_name == \"sigmoid\": H = 1.0 / (1.0 + cp.exp(-b * H))\n        elif act_name == \"relu\": H = cp.maximum(0, H)\n        elif act_name == \"swish\": H = H * (1.0 / (1.0 + cp.exp(-b * H)))\n        elif act_name == \"mish\": H = H * cp.tanh(cp.log1p(cp.exp(H)))\n        elif act_name == \"gaussian\": H = cp.exp(-1.0 * (b * H)**2)\n        elif act_name == \"sinc\": H = cp.sinc(b * H)\n        elif act_name == \"elu\": H = cp.where(H > 0, H, b * (cp.exp(H) - 1))\n        elif act_name == \"softsign\": H = H / (1 + cp.abs(H))\n        elif act_name == \"cosine\": H = cp.cos(b * H)\n        elif act_name == \"bent_id\": H = (cp.sqrt(H**2 + 1) - 1)/2 + H\n        # Fallback\n        else: H = cp.tanh(b * H)\n\n        # Polynomial Manifold\n        if p != 1.0:\n            H = cp.sign(H) * cp.abs(H) ** p\n\n        return H\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        n_samples, n_features = X.shape\n\n        # Move Data to GPU ONCE (Crucial for Speed)\n        if GPU_AVAILABLE:\n            self._X_train_gpu = cp.asarray(X, dtype=cp.float32)\n            # One-hot encode on GPU\n            y_encoded = cp.zeros((n_samples, len(self.classes_)))\n            y_gpu = cp.asarray(y)\n            for i, c in enumerate(self.classes_):\n                y_encoded[y_gpu == c, i] = 1\n            self._y_train_gpu = y_encoded\n            self._y_labels_gpu = y_gpu # For scoring\n        else:\n            # CPU Fallback\n            self._X_train_gpu = X\n            y_encoded = np.zeros((n_samples, len(self.classes_)))\n            for i, c in enumerate(self.classes_):\n                y_encoded[y == c, i] = 1\n            self._y_train_gpu = y_encoded\n            self._y_labels_gpu = y\n\n        # Initialize Weights in VRAM\n        max_hidden = 5000\n        rng = self._get_gpu_rng(self._rng_seed)\n\n        if self.input_weights_ is None:\n            self.input_weights_ = rng.normal(size=(n_features, max_hidden), dtype=cp.float32)\n            self.bias_ = rng.normal(size=(max_hidden,), dtype=cp.float32)\n\n        # Solve (GPU Pinv is 50x faster)\n        self._solve_weights(self.__dict__)\n        return self\n\n    def _solve_weights(self, dna):\n        H = self._activate(self._X_train_gpu, dna)\n        n_h = dna.get('n_hidden', self.n_hidden)\n        I = cp.eye(n_h, dtype=cp.float32)\n\n        # The Heavy Lifting: Matrix Inversion on Tensor Core\n        # Ridge: (H^T H + alpha*I)^-1 H^T Y\n        # Using pseudo-inverse for maximum stability\n        H_inv = cp.linalg.pinv(cp.dot(H.T, H) + dna['alpha'] * I)\n        self.output_weights_ = cp.dot(cp.dot(H_inv, H.T), self._y_train_gpu)\n\n    def evolve(self, X_val, y_val, generations=5):\n        # Move Validation Data to GPU ONCE\n        X_val_g = cp.asarray(X_val, dtype=cp.float32) if GPU_AVAILABLE else X_val\n        y_val_g = cp.asarray(y_val) if GPU_AVAILABLE else y_val\n\n        best_acc = -1.0\n        # Initial Score\n        H_val = self._activate(X_val_g)\n        raw_val = cp.dot(H_val, self.output_weights_)\n        pred_val = cp.argmax(raw_val, axis=1)\n        # Use simple accuracy check on GPU\n        best_acc = float(cp.mean(pred_val == y_val_g))\n\n        best_dna = {\n            \"n_hidden\": self.n_hidden, \"activation\": self.activation,\n            \"alpha\": self.alpha, \"beta\": self.beta,\n            \"gamma\": self.gamma, \"bias_scale\": self.bias_scale,\n            \"power\": self.power\n        }\n\n        # Fast Menu\n        activations = [\"sine\", \"tanh\", \"sigmoid\", \"relu\", \"swish\", \"gaussian\", \"softsign\", \"mish\"]\n        infinite_betas = cp.concatenate([\n            cp.logspace(-2, 2, 20), -cp.logspace(-2, 2, 20), cp.array([1.0, -1.0])\n        ])\n\n        for gen in range(generations):\n            # Spawn 4 Mutants\n            mutants = []\n            for _ in range(4):\n                m = best_dna.copy()\n                if random.random() < 0.3: m[\"n_hidden\"] = int(np.clip(m[\"n_hidden\"] * np.random.uniform(0.5, 1.5), 50, 4500))\n                if random.random() < 0.2: m[\"activation\"] = random.choice(activations)\n                for key in [\"alpha\", \"gamma\", \"bias_scale\", \"power\"]:\n                    if random.random() < 0.3: m[key] *= np.random.uniform(0.8, 1.25)\n                if random.random() < 0.3: m[\"beta\"] = float(np.random.choice(cp.asnumpy(infinite_betas)))\n                mutants.append(m)\n\n            # BATTLE ROYALE ON GPU\n            for m in mutants:\n                try:\n                    # Activate & Solve on GPU (No CPU transfer)\n                    H = self._activate(self._X_train_gpu, m)\n                    n_h = m['n_hidden']\n                    I = cp.eye(n_h, dtype=cp.float32)\n\n                    # Fast Ridge Solve\n                    # We use solve instead of pinv here for PURE SPEED during evolution\n                    # (HTH + aI) W = HTY\n                    HTH = cp.dot(H.T, H) + m['alpha'] * I\n                    HTY = cp.dot(H.T, self._y_train_gpu)\n\n                    # Cholesky solve is faster than Pinv for evolution checks\n                    # Only use Pinv for final fit\n                    out_w = cp.linalg.solve(HTH, HTY)\n\n                    # Validate\n                    H_v = self._activate(X_val_g, m)\n                    preds = cp.argmax(cp.dot(H_v, out_w), axis=1)\n                    acc = float(cp.mean(preds == y_val_g))\n\n                    if acc > best_acc:\n                        best_acc = acc\n                        best_dna = m\n                except: continue\n\n        # Lock Champion\n        self.n_hidden = best_dna[\"n_hidden\"]\n        self.activation = best_dna[\"activation\"]\n        self.alpha = best_dna[\"alpha\"]\n        self.beta = best_dna[\"beta\"]\n        self.gamma = best_dna[\"gamma\"]\n        self.bias_scale = best_dna[\"bias_scale\"]\n        self.power = best_dna[\"power\"]\n\n        # Final Robust Solve (Using Pinv for stability)\n        self._solve_weights(best_dna)\n        self.dna_ = best_dna\n\n        # Clean VRAM\n        if GPU_AVAILABLE:\n            cp.get_default_memory_pool().free_all_blocks()\n\n        return best_acc\n\n    def predict_proba(self, X):\n        if GPU_AVAILABLE:\n            X_g = cp.asarray(X, dtype=cp.float32)\n            H = self._activate(X_g)\n            raw = cp.dot(H, self.output_weights_)\n            # Softmax on GPU\n            raw -= cp.max(raw, axis=1, keepdims=True)\n            exp_out = cp.exp(raw)\n            probs = exp_out / cp.sum(exp_out, axis=1, keepdims=True)\n            return cp.asnumpy(probs) # Return to CPU for Sklearn compatibility\n        else:\n            return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n\n    def predict(self, X):\n        probs = self.predict_proba(X)\n        return self.classes_[np.argmax(probs, axis=1)]\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_predict\n\n# --- 26. THE RESIDUAL BRIDGE (Unit 26 - The Death Ray V4 - Dynamic Optics) ---\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_predict\n\nclass ResidualBridgeUnit(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    THE RESIDUAL SNIPER ARCHITECTURE (V4).\n    Role: Calculates the 'Mistake' of the Elite Model using Geometric Neighbors.\n    Features:\n      - Dynamic Optics: Uses K=5 for small data (<2000 rows), K=21 for large data.\n      - Auto-Scope: Calibrates correction strength (0.0001 to 1.0) via simulation.\n      - Safety Lock: If no correction improves the score, it stands down (Strength 0).\n    \"\"\"\n    def __init__(self, n_neighbors=None):\n        # Default to None so we can set it dynamically based on dataset size\n        self.n_neighbors = n_neighbors\n        self.sniper_ = None\n        self.verified_score_ = 0.0\n        self.best_factor_ = 0.0\n        self.classes_ = None\n        self.dna_ = {\"strategy\": \"Residual_KNN\"}\n\n    def fit_hunt(self, X_raw, y, elite_probs_oof):\n        \"\"\"\n        X_raw: Standard Scaled Geometry\n        y: True Labels\n        elite_probs_oof: The Baseline Probability Matrix\n        \"\"\"\n        self.classes_ = np.unique(y)\n        n_samples = len(X_raw)\n\n        # [DYNAMIC OPTICS SYSTEM]\n        # Small Universe (<2000): Use Microscope (K=5) to see tiny local errors.\n        # Large Universe (>2000): Use Telescope (K=21) to see stable patterns.\n        if self.n_neighbors is None:\n            if n_samples < 2000:\n                self.k_dynamic = 5\n            else:\n                self.k_dynamic = 21\n        else:\n            self.k_dynamic = self.n_neighbors\n\n        self.dna_[\"k\"] = self.k_dynamic\n\n        # 1. Calculate Residuals (The Mistake)\n        # R = Truth (1.0) - Elite (0.8) = +0.2 Error\n        y_onehot = np.zeros_like(elite_probs_oof)\n        for i, c in enumerate(self.classes_):\n            y_onehot[y == c, i] = 1.0\n        residuals = y_onehot - elite_probs_oof\n\n        # 2. Train Sniper (The Geometric Corrector)\n        # We use Manhattan (p=1) because it works better in high-dimensional spaces.\n        self.sniper_ = KNeighborsRegressor(\n            n_neighbors=self.k_dynamic,\n            weights='distance',\n            metric='minkowski',\n            p=1,\n            n_jobs=-1\n        )\n\n        # 3. INTERNAL SIMULATION (Calibrate the Scope)\n        try:\n            # Predict the mistake for every point (Cross-Validation)\n            oof_correction = cross_val_predict(self.sniper_, X_raw, residuals, cv=5, n_jobs=-1)\n\n            # The Universal Spectrum: From Quantum Nudge to Full Override\n            factors = [\n                0.0001, 0.0005, 0.001, 0.002, 0.005,  # Micro-Dose (Tie-Breakers)\n                0.01, 0.015, 0.02, 0.025, 0.03, 0.04, # Fine-Tuning\n                0.05, 0.06, 0.07, 0.08, 0.09, 0.10,   # Standard Correction\n                0.12, 0.15, 0.18, 0.20, 0.22, 0.25,   # Aggressive Correction\n                0.30, 0.35, 0.40, 0.45, 0.50, 0.55,   # Heavy Geometry\n                0.60, 0.70, 0.80, 0.90, 1.00          # Full Geometric Trust\n            ]\n\n            best_score = -1.0\n            best_f = 0.0\n\n            # Baseline Accuracy (What happens if we do nothing?)\n            base_acc = accuracy_score(y, self.classes_[np.argmax(elite_probs_oof, axis=1)])\n\n            if hasattr(self, \"verbose\") and self.verbose:\n                print(f\" > [DEATH RAY] Calibrating Scope (K={self.k_dynamic} | Base: {base_acc:.4%})...\")\n\n            for f in factors:\n                # Apply correction: New = Old + (Correction * Strength)\n                oof_fused = elite_probs_oof + (oof_correction * f)\n                score = accuracy_score(y, self.classes_[np.argmax(oof_fused, axis=1)])\n\n                # STRICT IMPROVEMENT CHECK\n                # We only lock if it strictly beats the previous best.\n                if score > best_score:\n                    best_score = score\n                    best_f = f\n\n            self.verified_score_ = best_score\n            self.best_factor_ = best_f\n\n            if hasattr(self, \"verbose\") and self.verbose:\n                print(f\" > [DEATH RAY] Scope Locked. Strength: {self.best_factor_} | Score: {self.verified_score_:.4%}\")\n\n        except Exception as e:\n            if hasattr(self, \"verbose\") and self.verbose:\n                print(f\" > [DEATH RAY] Calibration Failed: {e}\")\n            self.verified_score_ = 0.0\n            self.best_factor_ = 0.0\n\n        # 4. Final Fit (Lock and Load)\n        self.sniper_.fit(X_raw, residuals)\n        return self\n\n    def predict_proba(self, X_fused):\n        \"\"\"\n        Input: [Raw_Features | Elite_Probabilities]\n        Output: Corrected Probabilities\n        \"\"\"\n        # Split Data\n        n_features_raw = X_fused.shape[1] - len(self.classes_)\n        X_raw = X_fused[:, :n_features_raw]\n        elite_probs = X_fused[:, n_features_raw:]\n\n        # 1. Ask Sniper for Correction\n        correction = self.sniper_.predict(X_raw)\n\n        # 2. Apply The Auto-Calibrated Factor\n        # This is the \"Magic Formula\" that guarantees safety\n        final_probs = elite_probs + (correction * self.best_factor_)\n\n        # 3. Clip & Normalize (Ensure valid probability distribution)\n        final_probs = np.clip(final_probs, 0.0, 1.0)\n        sums = np.sum(final_probs, axis=1, keepdims=True)\n        return final_probs / (sums + 1e-9)\n\n    def predict(self, X_fused):\n        probs = self.predict_proba(X_fused)\n        return self.classes_[np.argmax(probs, axis=1)]\n\n    def fit(self, X, y):\n        print(\" [WARNING] Death Ray requires fit_hunt() with elite probs.\")\n        return self\n\n\n\n\nclass ChronosGateUnit(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    THE CHRONOS GATE (Dynamic MoE Router).\n    Role: Decides which 'Constitution' (Strategy) applies to which sample.\n    Physics: Local Reality Selection.\n    \"\"\"\n    def __init__(self):\n        # We use a Depth-3 XGBoost on GPU for maximum speed and stability.\n        # It doesn't need to be deep; it just needs to find the \"Safety Zones\".\n        self.gate_keeper_ = XGBClassifier(\n            n_estimators=100,\n            max_depth=3,\n            learning_rate=0.05,\n            tree_method='hist' if GPU_AVAILABLE else 'hist',\n            eval_metric='logloss',\n            random_state=42\n        )\n        self.strategies_ = [\"council\", \"ace\", \"linear\", \"death_ray\"]\n\n    def fit(self, X, y_best_strategy):\n        \"\"\"\n        X: The Scaled Features\n        y_best_strategy: Integer index of which strategy worked best for this sample.\n        \"\"\"\n        self.gate_keeper_.fit(X, y_best_strategy)\n        return self\n\n    def get_gate_weights(self, X):\n        \"\"\"\n        Returns (N_samples, 4) matrix of weights.\n        Order: [Council, Ace, Linear, Death Ray]\n        \"\"\"\n        # Get raw probabilities from the gate\n        weights = self.gate_keeper_.predict_proba(X)\n        \n        # [TITAN SAFETY CLAMP]\n        # We never allow a strategy to go below 5% or above 95% purely based on the gate.\n        # This prevents \"Hard Switching\" instability.\n        weights = np.clip(weights, 0.05, 0.95)\n        \n        # Re-normalize to sum to 1.0\n        row_sums = weights.sum(axis=1, keepdims=True)\n        return weights / row_sums\n\n\n# --- 7. THE TITAN-21 \"FINAL COSMOLOGY\" ---\nclass HarmonicResonanceClassifier_BEAST_21D(BaseEstimator, ClassifierMixin):\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n        self.scaler_ = RobustScaler(quantile_range=(15.0, 85.0))\n        self.weights_ = None\n        self.classes_ = None\n\n        # --- THE COMPETITOR TRINITY ---\n        self.unit_bench_svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42)\n        self.unit_bench_rf = RandomForestClassifier(n_estimators=100, random_state=42) # Standard RF\n        self.unit_bench_xgb = XGBClassifier(n_estimators=100,  eval_metric='logloss', random_state=42) # Standard XGB\n\n        # --- THE 21 DIMENSIONS OF THE UNIVERSE ---\n\n        # [LOGIC SECTOR - NEWTONIAN]\n        self.unit_01 = ExtraTreesClassifier(\n            n_estimators=1000, bootstrap=False, max_features=\"sqrt\", n_jobs=-1, random_state=42\n        )\n        self.unit_02 = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)\n        self.unit_03 = HistGradientBoostingClassifier(\n            max_iter=500, learning_rate=0.05, random_state=42\n        )\n\n        # [GRADIENT SECTOR - OPTIMIZATION]\n        self.unit_04 = XGBClassifier(n_estimators=500, max_depth=6, learning_rate=0.02, n_jobs=-1, random_state=42)\n        self.unit_05 = XGBClassifier(n_estimators=1000, max_depth=3, learning_rate=0.1, n_jobs=-1, random_state=42)\n\n        # [KERNEL SECTOR - MANIFOLDS]\n        self.unit_06 = NuSVC(nu=0.05, kernel=\"rbf\", gamma=\"scale\", probability=True, random_state=42)\n        self.unit_07 = SVC(kernel=\"poly\", degree=2, C=10.0, probability=True, random_state=42)\n\n        # [GEOMETRY SECTOR - SPACETIME]\n        self.unit_08 = KNeighborsClassifier(n_neighbors=3, weights=\"distance\", metric=\"euclidean\", n_jobs=-1)\n        self.unit_09 = KNeighborsClassifier(n_neighbors=9, weights=\"distance\", metric=\"manhattan\", n_jobs=-1)\n        self.unit_10 = QuadraticDiscriminantAnalysis(reg_param=0.01)\n        self.unit_11 = SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", probability=True, random_state=42)\n\n        # [SOUL SECTOR - RESONANCE (EVOLUTIONARY)]\n        self.unit_12 = HolographicSoulUnit(k=15)\n        self.unit_13 = HolographicSoulUnit(k=15)\n        self.unit_14 = HolographicSoulUnit(k=15)\n        self.unit_15 = HolographicSoulUnit(k=25)\n        self.unit_16 = HolographicSoulUnit(k=25)\n        self.unit_17 = HolographicSoulUnit(k=25)\n\n        # [BIOLOGY SECTOR - FRACTAL (EVOLUTIONARY)]\n        #self.unit_18 = GoldenSpiralUnit(k=21)\n\n        # [COSMIC SECTOR - THE FINAL TRINITY]\n        # 1. DEFINE THE UNITS (Using the NEW Heavy GPU classes)\n        self.unit_18 = GoldenSpiralUnit(k=21, n_estimators=50)      # Golden Forest\n        self.unit_19 = EntropyMaxwellUnit(n_estimators=50)          # Entropy Forest\n        self.unit_20 = QuantumFluxUnit(n_estimators=20, gamma=0.5)  # Quantum Forest\n        self.unit_21 = EventHorizonUnit(n_estimators=50)            # Gravity Forest\n\n        # [COSMIC SECTOR - THE SPEEDSTERS (Re-Enabled)]\n        #self.unit_18 = FastGoldenUnit(k=21)        # Phi Physics\n        #self.unit_19 = FastEntropyUnit()           # Thermodynamics\n        #self.unit_20 = FastQuantumUnit(gamma=0.5)  # Quantum Flux\n        #self.unit_21 = FastGravityUnit()           # General Relativity\n\n\n        # [ALIEN SECTOR - THE OMEGA]\n        self.unit_24 = AlienDimensionZ() # Depth 7 for extreme complexity\n\n        # [NEURAL SECTOR - THE UNIVERSAL SOLVER]\n        self.unit_25 = NeuralManifoldUnit(n_hidden=100, activation=\"tanh\", alpha=0.1)\n\n        # [THE DEATH RAY]\n        # [THE DEATH RAY - GAMMA HYPERNOVA]\n        # We use the new Hypernova class.\n        # Time limit is 60s, but it uses the smart fused-input pipeline now.\n        # [THE DEATH RAY - RESIDUAL SNIPER]\n        # Replaces GammaHypernova. Uses KNN (k=50) to fix Elite mistakes.\n        self.unit_26 = ResidualBridgeUnit(n_neighbors=None)\n\n\n        # [NEW] The Chronos Gate\n        self.unit_chronos = ChronosGateUnit()\n\n\n    # CHANGE THIS LINE\n    def fit(self, X, y, X_test_oracle=None, y_test_oracle=None):\n        y = np.array(y).astype(int)\n        X, y = check_X_y(X, y)\n        self.classes_ = np.unique(y)\n        n_classes = len(self.classes_)\n\n        if self.verbose:\n            print(\" >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\")\n            print(\" > Initiating The Ouroboros Protocol (Stabilized)...\")\n\n        # --- PHASE -1: THE UNIVERSAL LENS SELECTOR (Switching Scalers) ---\n        # --- PHASE -1: THE UNIVERSAL LENS SELECTOR (Dual-Scout Protocol) ---\n        if self.verbose: print(\" > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\")\n\n        lenses = [\n            (\"Standard\", StandardScaler()),\n            (\"Robust\", RobustScaler(quantile_range=(15.0, 85.0))),\n            (\"MinMax\", MinMaxScaler())\n        ]\n\n        best_lens_name = \"Standard\"\n        best_lens_score = -1.0\n        best_lens_obj = StandardScaler()\n\n        # SCOUT TEAM: We use proxies for the two main laws of physics in HRF\n        from sklearn.model_selection import cross_val_score\n        from sklearn.tree import DecisionTreeClassifier\n\n        # 1. Geometry Scout (Represents SVM, KNN, Soul, Gravity) -> Needs Scaling\n        scout_geom = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n\n        # 2. Logic Scout (Represents ExtraTrees, XGBoost, Forest) -> Robust\n        # We use a simple Tree to ensure the scaler doesn't distort the information gain.\n        scout_logic = DecisionTreeClassifier(max_depth=5, random_state=42)\n\n        # Test on subset (max 2000 samples for speed)\n        sub_idx = np.random.choice(len(X), min(len(X), 2000), replace=False)\n        X_sub = X[sub_idx]\n        y_sub = y[sub_idx]\n\n        for name, lens in lenses:\n            try:\n                # Apply Lens\n                X_trans = lens.fit_transform(X_sub)\n\n                # Get Consensus Score\n                score_g = cross_val_score(scout_geom, X_trans, y_sub, cv=3, n_jobs=-1).mean()\n                score_l = cross_val_score(scout_logic, X_trans, y_sub, cv=3, n_jobs=-1).mean()\n\n                # Harmonic Mean (Penalizes if one scout hates it)\n                # Formula: 2 * (G * L) / (G + L)\n                combined_score = 2 * (score_g * score_l) / (score_g + score_l + 1e-9)\n\n                if self.verbose:\n                    print(f\"    [{name:<8}] Geom: {score_g:.2%} | Logic: {score_l:.2%} | HARMONIC: {combined_score:.2%}\")\n\n                if combined_score > best_lens_score:\n                    best_lens_score = combined_score\n                    best_lens_name = name\n                    best_lens_obj = lens\n            except: pass\n\n        self.scaler_ = best_lens_obj\n        if self.verbose: print(f\" >>> LENS LOCKED: {best_lens_name.upper()} SCALER (Consensus Achieved) <<<\")\n\n        X_scaled = self.scaler_.fit_transform(X)\n\n        # --- PHASE 0: DUAL SNIPER CALIBRATION (Flash-Tune Protocol) ---\n        if self.verbose: print(\" > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\")\n\n        # [SPEED HACK]: We don't need 10k rows to find 'C'. 1000 is enough.\n        # This makes it run 10x-50x faster.\n        n_total = len(X)\n        n_calib = min(n_total, 2000)\n\n        # Stratified Subsample for Speed\n        if n_total > 5000:\n             # Fast random index\n             idx_calib = np.random.choice(n_total, n_calib, replace=False)\n             X_calib = X_scaled[idx_calib]\n             y_calib = y[idx_calib]\n        else:\n             X_calib = X_scaled\n             y_calib = y\n\n        try:\n            from sklearn.model_selection import RandomizedSearchCV\n\n            # 1. Calibrate Resonance (Standard SVM)\n            # Reduced iterations from 8 -> 5 (Good enough for coarse tuning)\n            params_svc = {\n                \"C\": [0.1, 1.0, 10.0, 50.0],\n                \"gamma\": [\"scale\", \"auto\", 0.1]\n            }\n            search_svc = RandomizedSearchCV(\n                self.unit_11, params_svc, n_iter=10, cv=3, n_jobs=-1, random_state=42\n            )\n            search_svc.fit(X_calib, y_calib)\n            self.unit_11 = search_svc.best_estimator_\n\n            # 2. Calibrate Nu-Warp (NuSVC)\n            # Reduced iterations from 6 -> 4\n            params_nu = {\n                \"nu\": [0.05, 0.1, 0.2],\n                \"gamma\": [\"scale\", \"auto\"]\n            }\n            search_nu = RandomizedSearchCV(\n                self.unit_06, params_nu, n_iter=4, cv=3, n_jobs=-1, random_state=42\n            )\n            search_nu.fit(X_calib, y_calib)\n            self.unit_06 = search_nu.best_estimator_\n\n            if self.verbose:\n                print(f\"    >>> Resonance (SVM) Tuned: {search_svc.best_params_} | Score: {search_svc.best_score_:.2%}\")\n                print(f\"    >>> Nu-Warp (NuSVC) Tuned: {search_nu.best_params_} | Score: {search_nu.best_score_:.2%}\")\n        except Exception as e:\n            if self.verbose: print(f\"    >>> Calibration Skipped (Speed Mode): {e}\")\n\n        # --- STEP 1: RAPID QUALIFIER (20% Proxy) ---\n        X_train_sub, X_select, y_train_sub, y_select = train_test_split(\n            X_scaled, y, test_size=0.20, stratify=y, random_state=42\n        )\n\n        # --- A: EVOLVE & TRAIN (On Sub-Set for Speed) ---\n        if self.verbose:\n            print(\" > Phase 1: Awakening the Souls (Rapid Evolution)...\")\n            print(\"-\" * 80)\n            print(f\" {'UNIT NAME':<20} | {'ACCURACY':<8} | {'EVOLVED DNA PARAMETERS'}\")\n            print(\"-\" * 80)\n\n        # 1. Define The Living Groups (Souls + Neural)\n        # Note: We removed Cosmic/Forests from here to handle them in the Strict Order list below\n        living_units = [\n            (\"SOUL-01 (Original)\", self.unit_12),\n            (\"SOUL-02 (Mirror A)\", self.unit_13),\n            (\"SOUL-03 (Mirror B)\", self.unit_14),\n            (\"SOUL-D (AGI Hyper)\", self.unit_15),\n            (\"SOUL-E (AGI Deep)\", self.unit_16),\n            (\"SOUL-F (AGI Omni)\", self.unit_17),\n            (\"NEURAL-ELM (Omni)\", self.unit_25),\n            # The Cosmic Forests (Now treated as Living Entities)\n            (\"GOLDEN-FOREST\", self.unit_18),\n            (\"ENTROPY-FOREST\", self.unit_19),\n            (\"QUANTUM-FOREST\", self.unit_20),\n            (\"GRAVITY-FOREST\", self.unit_21),\n        ]\n\n        # Evolve the Living\n        for name, unit in living_units:\n            if hasattr(unit, \"set_raw_source\"):\n                unit.set_raw_source(X_train_sub)\n            try:\n                unit.fit(X_train_sub, y_train_sub)\n                # Only evolve if supported\n                if hasattr(unit, \"evolve\"):\n                    acc = unit.evolve(X_select, y_select, generations=10)\n                else:\n                    acc = 0.0\n\n                if self.verbose:\n                    dna = getattr(unit, \"dna_\", {})\n                    dna_str = \"Standard\"\n                    # DNA Printer Logic\n                    if \"freq\" in dna: dna_str = f\"Freq: {dna['freq']:.2f} | Gamma: {dna['gamma']:.2f} | P: {dna.get('p', 2.0):.1f}\"\n                    elif \"n_hidden\" in dna: dna_str = f\"H:{dna['n_hidden']} | Act:{dna['activation']} | Alpha:{dna['alpha']:.2f}\"\n                    # [NEW] Forest Printers\n                    elif \"resonance\" in dna: dna_str = f\"Res: {dna['resonance']:.3f} | Decay: {dna['decay']:.2f} | Shift: {dna['shift']:.1f}\"\n                    elif \"horizon_pct\" in dna: dna_str = f\"Horizon: {dna['horizon_pct']}% | Power: {dna['decay_power']:.2f}\"\n                    elif \"gamma\" in dna and \"n_components\" in dna: dna_str = f\"Gamma: {dna['gamma']:.2f} | N-Comp: {dna['n_components']}\"\n                    elif \"n_components\" in dna: dna_str = f\"Components: {dna['n_components']}\"\n\n                    print(f\" {name:<20} | {acc:.2%}  | {dna_str}\")\n            except Exception as e:\n                if self.verbose: print(f\" {name:<20} | FAILED   | {str(e)}\")\n\n        if self.verbose: print(\"-\" * 80)\n\n        # 2. Train The Non-Living (Standard + Competitors)\n        # [TITAN UPDATE]: Removed Forests from here because they are now in the Living list above!\n        non_living_training_group = [\n            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n            # Forests removed from here\n            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb # Competitors\n        ]\n\n        for unit in non_living_training_group:\n            try: unit.fit(X_train_sub, y_train_sub)\n            except: pass\n\n        # --- B: THE GRAND QUALIFIER (Identify Top 12) ---\n        if self.verbose: print(\" > Phase 2: The Grand Qualifier (Scanning All 12 Candidates)...\")\n\n        # CRITICAL: THIS ORDER MUST MATCH PREDICT_PROBA EXACTLY\n        # 1. Standard (11)\n        # 2. Cosmic (4)\n        # 3. Competitors (3)\n        # 4. Souls (6)\n        # 5. Neural (1)\n        all_units = [\n            # 1. Standard\n            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n            # 2. Cosmic / Physics\n            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n            # 3. Competitors\n            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n            # 4. Souls\n            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n            # 5. Neural\n            self.unit_25,\n            self.unit_26\n        ]\n\n        n_units = len(all_units)\n        accs = []\n\n        # Score all units on Selection Set\n        for unit in all_units:\n            try:\n                p = unit.predict(X_select)\n                accs.append(accuracy_score(y_select, p))\n            except: accs.append(0.0)\n\n        # Sort by raw accuracy\n        sorted_indices = np.argsort(accs)[::-1]\n\n        # [INSERT THIS SNIPPET IN PHASE 2 TO SEE ALL 21 SCORES]\n        if self.verbose:\n            print(\"\\n\" + \"=\"*70)\n            print(\" >>> THE 21D PERFORMANCE MONITOR (Phase 2 Qualification) <<<\")\n            print(\"=\"*70)\n            print(f\" {'RANK':<6} | {'UNIT NAME':<18} | {'SCORE':<10} | {'STATUS'}\")\n            print(\"-\" * 70)\n\n            # Map indices to friendly names\n            # (Indices 0-10 are Standard, 11+ are Living)\n            # ... inside Phase 2 Performance Monitor ...\n            # Map indices to friendly names\n            # Order: Standard -> Fast Physics -> Benchmarks -> Souls -> Neural\n            # Map indices to friendly names\n            # Order: Standard -> Cosmic Forests -> Competitors -> Souls -> Neural\n            map_names = [\n                \"Logic-ET\", \"Logic-RF\", \"Logic-HG\", \"Grad-XG1\", \"Grad-XG2\",\n                \"Nu-Warp\", \"PolyKer\", \"Geom-K3\", \"Geom-K9\", \"Space-QDA\", \"Resonance\",\n\n                # [FIXED] Proper Names for the GPU Forests\n                \"GOLDEN-FOREST\", \"ENTROPY-FOREST\", \"QUANTUM-FOREST\", \"GRAVITY-FOREST\",\n\n                # Competitors\n                \"BENCH-SVM\", \"BENCH-RF\", \"BENCH-XGB\",\n\n                # Living Units\n                \"SOUL-Orig\", \"SOUL-TwinA\", \"SOUL-TwinB\", \"SOUL-D(AGI)\", \"SOUL-E(AGI)\", \"SOUL-F(AGI)\",\n                \"Neural-ELM\",\"THE DEATH RAY\"\n            ]\n\n            for rank, idx in enumerate(sorted_indices):\n                # Get Name\n                if idx < len(map_names):\n                    name = map_names[idx]\n                else:\n                    # Fallback if I missed an index\n                    name = f\"Unit-{idx}\"\n\n                score = accs[idx]\n                status = \"PROMOTED\" if rank < 12 else \"Eliminated\"\n\n                print(f\" {rank+1:02d}     | {name:<18} | {score:.2%}    | {status}\")\n            print(\"-\" * 70)\n\n        # Pick Top 12 for the OOF Battle\n        top_12_indices = sorted_indices[:12]\n        candidate_models = [all_units[i] for i in top_12_indices]\n\n\n        # --- C: THE OUROBOROS SELECTION (The Battle of Names) ---\n        if self.verbose:\n            print(\"\\n\" + \"=\" * 80)\n            print(\" >>> PHASE 3: THE OUROBOROS PROTOCOL (Top 12 Candidates - 100% Validation) <<<\")\n            print(\"=\" * 80)\n            print(f\" {'RANK':<4} | {'UNIT NAME':<18} | {'OOF ACCURACY':<10} | {'STATUS'}\")\n            print(\"-\" * 80)\n\n        # 1. Define The Name Map (Global Index -> Name)\n        # This matches your 21D init order perfectly.\n        # 1. Define The Name Map (Global Index -> Name)\n        all_names_map = [\n            \"Logic-ET\", \"Logic-RF\", \"Logic-HG\", \"Grad-XG1\", \"Grad-XG2\",               # 0-4\n            \"Nu-Warp\", \"PolyKer\", \"Geom-K3\", \"Geom-K9\", \"Space-QDA\", \"Resonance\",     # 5-10\n\n            # [FIXED]\n            \"GOLDEN-FOREST\", \"ENTROPY-FOREST\", \"QUANTUM-FOREST\", \"GRAVITY-FOREST\",    # 11-14\n\n            \"BENCH-SVM\", \"BENCH-RF\", \"BENCH-XGB\",                                     # 15-17\n            \"SOUL-Orig\", \"SOUL-TwinA\", \"SOUL-TwinB\", \"SOUL-D(AGI)\", \"SOUL-E(AGI)\", \"SOUL-F(AGI)\", # 18-23\n            \"Neural-ELM\",                                                              # 24\n            \"THE DEATH RAY\"\n        ]\n\n        candidate_oof_accs = []\n        candidate_oof_preds_list = []\n\n        # 2. Run OOF (With Real Names)\n        for i, unit in enumerate(candidate_models):\n            # Retrieve the Real Name using the index from Phase 2\n            global_idx = top_12_indices[i]\n            unit_name = all_names_map[global_idx] if global_idx < len(all_names_map) else f\"Unit-{global_idx}\"\n\n            method = \"predict_proba\" if hasattr(unit, \"predict_proba\") else \"decision_function\"\n            try:\n                # 5-Fold Cross-Validation (The Truth Serum)\n                oof_pred = cross_val_predict(unit, X_scaled, y, cv=5, method=method, n_jobs=-1)\n\n                # Stabilization\n                if method == \"decision_function\":\n                    if len(oof_pred.shape) == 1:\n                        p = 1 / (1 + np.exp(-oof_pred))\n                        oof_pred = np.column_stack([1-p, p])\n                    else:\n                        max_d = np.max(oof_pred, axis=1, keepdims=True)\n                        exp_d = np.exp(oof_pred - max_d)\n                        oof_pred = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n\n                # Score\n                acc_oof = accuracy_score(y, self.classes_[np.argmax(oof_pred, axis=1)])\n                candidate_oof_accs.append(acc_oof)\n                candidate_oof_preds_list.append(oof_pred)\n\n                # Print The Battle Result\n                if self.verbose:\n                    print(f\" {i+1:02d}   | {unit_name:<18} | {acc_oof:.4%}   | Validated\")\n\n            except Exception as e:\n                candidate_oof_accs.append(0.0)\n                candidate_oof_preds_list.append(np.zeros((len(X_scaled), len(self.classes_))))\n                if self.verbose:\n                    print(f\" {i+1:02d}   | {unit_name:<18} | FAILED       | {str(e)[:20]}...\")\n\n        if self.verbose: print(\"-\" * 80)\n\n        # 3. Sort by Performance (Meritocracy)\n        sorted_oof_idx = np.argsort(candidate_oof_accs)[::-1]\n\n        # 4. Select Absolute Best (Top 2)\n\n\n        # [TITAN SAFETY PROTOCOL: THE NEURAL LEASH]\n        # Neural-ELM (Unit 25) is volatile. It must NEVER lead the Council.\n        # If it wins Rank 1, we force-swap it with Rank 2.\n        # This guarantees a stable model (Tree/SVM) always holds the 95%/85% power.\n\n        # 4. Select Absolute Best (Top 2) - WITH TITAN SAFETY\n        top_2_local_idx = []\n        for idx in sorted_oof_idx:\n            # Filter weak models\n            if candidate_oof_accs[idx] < 0.10: continue\n\n            # [TITAN SAFETY PROTOCOL: NEURAL EXILE]\n            # Identify who this candidate is\n            global_idx = top_12_indices[idx]\n\n            # If it is the Neural-ELM (Index 24), we SKIP it.\n            # This ensures it can never be Rank 1 or Rank 2.\n            # It is effectively restricted to Rank 3 (Reserve Bench).\n            if global_idx == 24:\n                if self.verbose and len(top_2_local_idx) < 2:\n                    print(f\" > [SAFETY] Neural-ELM attempted to join Council. Request DENIED (Restricted to Rank 3).\")\n                continue\n\n            top_2_local_idx.append(idx)\n            if len(top_2_local_idx) == 2: break\n\n        # Save The Elites (Now Safely Ordered)\n        self.final_elites_ = [candidate_models[i] for i in top_2_local_idx]\n        elite_accs = [candidate_oof_accs[i] for i in top_2_local_idx]\n        elite_preds = [candidate_oof_preds_list[i] for i in top_2_local_idx]\n\n        # --- ARCHITECTURE 1: THE COUNCIL  ---\n        # Fixed 80/20 Split. Strong Leadership, but keeps a backup.\n        self.weights_council_ = np.zeros(n_units)\n\n        # Rank 1 gets 85%\n        idx_rank1 = top_12_indices[top_2_local_idx[0]]\n        self.weights_council_[idx_rank1] = 0.75\n\n        # Rank 2 gets 15%\n        idx_rank2 = top_12_indices[top_2_local_idx[1]]\n        self.weights_council_[idx_rank2] = 0.25\n\n        # --- ARCHITECTURE 2: THE ACE (Absolute Monarchy ) ---\n        # The Winner takes ALL. Pure Power.\n        self.weights_ace_ = np.zeros(n_units)\n        self.weights_ace_[idx_rank1] = 0.90  # 95%\n        self.weights_ace_[idx_rank2] = 0.10  # 5%\n\n        # --- ARCHITECTURE 3: THE LINEAR (The Shield) ---\n        # CRITICAL: Keep this 50/50.\n        # If the Top Model is overfitting, this averages out the error.\n        # This is your \"Impossible to Lose\" insurance policy.\n        self.weights_linear_ = np.zeros(n_units)\n        self.weights_linear_[idx_rank1] = 0.60\n        self.weights_linear_[idx_rank2] = 0.40\n\n\n        # --- ARCHITECTURE 4: THE BALANCE (Perfect Harmony 50/50) ---\n        self.weights_balance_ = np.zeros(n_units)\n        self.weights_balance_[idx_rank1] = 0.50\n        self.weights_balance_[idx_rank2] = 0.50\n\n        # --- ARCHITECTURE 5: THE INVERSION (Support Lead 40/60) ---\n        self.weights_inv_linear_ = np.zeros(n_units)\n        self.weights_inv_linear_[idx_rank1] = 0.40\n        self.weights_inv_linear_[idx_rank2] = 0.60\n\n        # --- ARCHITECTURE 6: THE UNDERDOG (Hidden Potential 30/70) ---\n        self.weights_inv_council_ = np.zeros(n_units)\n        self.weights_inv_council_[idx_rank1] = 0.30\n        self.weights_inv_council_[idx_rank2] = 0.70\n\n        # --- SIMULATION ---\n        def get_score(weights_full):\n            combined_pred = np.zeros_like(elite_preds[0])\n            current_w = []\n            for idx in top_2_local_idx:\n                current_w.append(weights_full[top_12_indices[idx]])\n            for i in range(2):\n                combined_pred += current_w[i] * elite_preds[i]\n            return accuracy_score(y, self.classes_[np.argmax(combined_pred, axis=1)])\n\n        score_council = get_score(self.weights_council_)\n        score_ace = get_score(self.weights_ace_)\n        score_linear = get_score(self.weights_linear_)\n\n        score_balance = get_score(self.weights_balance_)\n        score_inv_linear = get_score(self.weights_inv_linear_)\n        score_inv_council = get_score(self.weights_inv_council_)\n\n        if self.verbose:\n            print(f\" > [STRATEGY LAB] Ace: {score_ace:.4%} | Council: {score_council:.4%} | Linear: {score_linear:.4%}\")\n            print(f\" > [STRATEGY LAB] Balance: {score_balance:.4%} | Inv-Lin: {score_inv_linear:.4%} | Underdog: {score_inv_council:.4%}\")\n\n        # [TITAN 6-WAY TOURNAMENT]\n        # We define a map of all 6 strategies and pick the absolute maximum\n        strat_map = {\n            \"council\": score_council,     # <--- Priority 2: The Vote (75/25)\n            \"linear\": score_linear,       # <--- Priority 1: The Shield (60/40)\n            \"ace\": score_ace,             # <--- Priority 4: The Gamble (90/10)\n           # \"balance\": score_balance,     # <--- Priority 3: The Harmony (50/50)\n            \"inv_linear\": score_inv_linear,\n            \"inv_council\": score_inv_council\n        }\n\n        # Select the key with the highest value\n        self.strategy_ = max(strat_map, key=strat_map.get)\n\n        # [TITAN TIE-BREAKER PRESERVATION]\n        # If Ace is essentially tied (>99% accuracy case), we still force Ace for purity\n        if score_ace > 0.98 and abs(score_ace - strat_map[self.strategy_]) < 0.001:\n            self.strategy_ = \"ace\"\n\n        if self.verbose:\n             print(f\" >>> {self.strategy_.upper()} STRATEGY LOCKED. <<<\")\n\n        # --- PHASE 4: ASSIMILATION ---\n        if self.verbose: print(f\" > Phase 4: Final Assimilation (Retraining Top 2 Elites)...\")\n        for unit in self.final_elites_:\n            unit.fit(X_scaled, y)\n\n        # --- PHASE 4.5: THE DEATH RAY PROTOCOL (Corrected Logic) ---\n        # 1. Get the Logic (OOF Probs) from the Rank 1 Elite\n        rank1_local_idx = top_2_local_idx[0]\n        best_oof_probs = elite_preds[0]\n\n        # [CRITICAL FIX] Compare against the TRUE MAX of all 6 strategies\n        # This prevents the Death Ray from beating a weak Ace but losing to a strong Balance.\n        true_max_score = max(strat_map.values())\n        true_max_name = max(strat_map, key=strat_map.get).upper()\n\n        if self.verbose:\n            print(f\" > [HYPERNOVA] Elite Source Acquired: {candidate_models[rank1_local_idx].__class__.__name__}\")\n            print(f\" > [HYPERNOVA] Current Champion to Beat: {true_max_score:.4%} ({true_max_name})\")\n\n        # 2. UNLEASH UNIT 26\n        self.unit_26.fit_hunt(X_scaled, y, elite_probs_oof=best_oof_probs)\n\n        # 3. VERIFY KILL\n        death_ray_score = self.unit_26.verified_score_\n\n        # MARGIN CHECK: Must beat the TRUE MAX score\n        margin = death_ray_score - true_max_score\n\n        if margin > 0.00001:\n            if self.verbose:\n                print(f\" > [ALERT] DEATH RAY SUCCESSFUL. (Score: {death_ray_score:.4%} | Margin: +{margin:.4%})\")\n                print(f\" > [COMMAND] OVERRIDING STRATEGY -> DEATH_RAY.\")\n\n            self.strategy_ = \"death_ray\"\n            self.weights_death_ray_ = np.zeros(26)\n\n            # 60% Death Ray + 40% Elite Foundation\n            self.weights_death_ray_[25] = 0.05\n            rank1_global_idx = top_12_indices[rank1_local_idx]\n            self.weights_death_ray_[rank1_global_idx] = 0.95\n\n        else:\n             if self.verbose:\n                 print(f\" > [DEATH RAY] Stand Down. No gain over {true_max_name} (Ray: {death_ray_score:.4%} vs Champ: {true_max_score:.4%}).\")\n             self.weights_death_ray_ = np.zeros(26)\n\n\n        # --- PHASE 5: THE CHRONOS GATE ASSIMILATION (Dynamic Constitution) ---\n        if self.verbose:\n            print(\"\\n\" + \"=\"*85)\n            print(f\" >>> PHASE 5: THE CHRONOS GATE ASSIMILATION (Training the Router) <<<\")\n            print(\"=\"*85)\n\n        # 1. Reconstruct Strategy OOF Predictions\n        # We need to know what each strategy WOULD have predicted for the training data.\n        \n        # Helper to mix OOFs based on strategy weights\n        def get_oof_strat(weights_full):\n            combined = np.zeros_like(elite_preds[0])\n            w1 = weights_full[top_12_indices[top_2_local_idx[0]]] # Rank 1 Weight\n            w2 = weights_full[top_12_indices[top_2_local_idx[1]]] # Rank 2 Weight\n            \n            if w1 > 0: combined += w1 * elite_preds[0]\n            if w2 > 0: combined += w2 * elite_preds[1]\n            \n            sum_w = w1 + w2\n            if sum_w > 0: return combined / sum_w\n            return combined\n\n        # Generate OOF matrices for each candidate strategy\n        oof_council = get_oof_strat(self.weights_council_)\n        oof_ace     = get_oof_strat(self.weights_ace_)\n        oof_linear  = get_oof_strat(self.weights_linear_)\n        \n        # For Death Ray OOF, use the fit_hunt result directly if active\n        if self.unit_26.verified_score_ > 0:\n            # Reconstruct the Death Ray's view using Rank 1 as base\n            oof_death_ray = self.unit_26.predict_proba(\n                np.hstack([X_scaled, elite_preds[0]]) \n            )\n        else:\n            oof_death_ray = oof_council # Fallback if Ray failed\n\n        # 2. Determine \"Who Won?\" for each sample\n        # The Strategy with the HIGHEST confidence in the RIGHT answer wins that sample.\n        n_samples = len(y)\n        strat_scores = np.zeros((n_samples, 4)) # [Council, Ace, Linear, Ray]\n        \n        # Get true class indices\n        true_class_idx = np.searchsorted(self.classes_, y)\n        \n        # Extract prob of true class\n        row_idx = np.arange(n_samples)\n        strat_scores[:, 0] = oof_council[row_idx, true_class_idx]\n        strat_scores[:, 1] = oof_ace[row_idx, true_class_idx]\n        strat_scores[:, 2] = oof_linear[row_idx, true_class_idx]\n        strat_scores[:, 3] = oof_death_ray[row_idx, true_class_idx]\n        \n        # Select best strategy per sample\n        best_strat_indices = np.argmax(strat_scores, axis=1)\n        \n        # 3. Train The Gate\n        if self.verbose:\n            counts = np.bincount(best_strat_indices, minlength=4)\n            print(f\" > [CHRONOS] Strategy Wins in Training:\")\n            print(f\"   - Council:   {counts[0]} samples\")\n            print(f\"   - Ace:       {counts[1]} samples\")\n            print(f\"   - Linear:    {counts[2]} samples\")\n            print(f\"   - Death Ray: {counts[3]} samples\")\n            print(f\" > [CHRONOS] Training Gatekeeper on GPU T4...\")\n\n        self.unit_chronos.fit(X_scaled, best_strat_indices)\n        self.strategy_ = \"chronos_dynamic\" # Flag to use the gate in predict\n        \n        if self.verbose:\n             print(\" > [CHRONOS] Gatekeeper Active. Constitution is now LIVING.\")\n             print(\"-\" * 85)\n\n        # --- FINAL CALCULATION: DUAL MARGIN CHECK ---\n        if X_test_oracle is not None and y_test_oracle is not None:\n            # 1. Benchmark\n            X_bench = self.scaler_.transform(X_test_oracle)\n            score_bench = accuracy_score(y_test_oracle, self.unit_bench_xgb.predict(X_bench))\n\n            # 2. Helper\n            def manual_score(weights):\n                if np.sum(weights) == 0: return 0.0\n                final_p = np.zeros((len(X_test_oracle), len(self.classes_)))\n                X_test_sc = self.scaler_.transform(X_test_oracle)\n                rank1_idx = top_12_indices[top_2_local_idx[0]]\n                elite_p = all_units_ordered[rank1_idx].predict_proba(X_test_sc)\n\n                for i, w in enumerate(weights):\n                    if w > 0:\n                        unit = all_units_ordered[i]\n                        try:\n                            if i == 25: # Death Ray\n                                X_fused = np.hstack([X_test_sc, elite_p])\n                                p = unit.predict_proba(X_fused)\n                            elif i == rank1_idx: # Opt\n                                p = elite_p\n                            else:\n                                if hasattr(unit, \"predict_proba\"): p = unit.predict_proba(X_test_sc)\n                                else:\n                                     d = unit.decision_function(X_test_sc)\n                                     p = np.exp(d)/np.sum(np.exp(d), axis=1, keepdims=True)\n                            final_p += w * p\n                        except: pass\n                return accuracy_score(y_test_oracle, self.classes_[np.argmax(final_p, axis=1)])\n\n            # 3. Score Reality A (Standard)\n            w_map = {\n                \"ACE\": self.weights_ace_, \"COUNCIL\": self.weights_council_,\n                \"LINEAR\": self.weights_linear_, \"BALANCE\": self.weights_balance_,\n                \"INV_LINEAR\": self.weights_inv_linear_, \"INV_COUNCIL\": self.weights_inv_council_\n            }\n            score_champ = manual_score(w_map.get(std_name, self.weights_council_))\n            margin_champ = score_champ - score_bench\n\n            # 4. Score Reality B (Death Ray)\n            score_ray = 0.0\n            if hasattr(self, \"weights_death_ray_\") and np.sum(self.weights_death_ray_) > 0:\n                score_ray = manual_score(self.weights_death_ray_)\n\n            # Internal Boost Calculation\n            boost = score_ray - score_champ\n\n            # Print Final Report\n            print(\"-\" * 85)\n            print(f\" [BENCHMARK] XGBoost Baseline        | {score_bench:.4%}    | Target to Beat\")\n            print(\"-\" * 85)\n            print(f\" HRF Ultimate (Standard: {std_name:<7}) | {score_champ:.4%}    | Margin: {margin_champ:+.4%}\")\n\n            if score_ray > 0:\n                ray_result_status = f\"Margin: {score_ray - score_bench:+.4%} (Boost: {boost:+.4%})\"\n                print(f\" HRF Ultimate (The Death Ray)    | {score_ray:.4%}    | {ray_result_status}\")\n            else:\n                print(f\" HRF Ultimate (The Death Ray)    | INACTIVE        | (Ray did not fire)\")\n\n            print(\"-\" * 85)\n\n        return self\n\n\n    def _predict_council_internal(self, X):\n        # Fast prediction using pre-calculated weights\n        X_sc = self.scaler_.transform(X)\n        final_pred = None\n        all_units= [\n            # 1. Logic (01-11)\n            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n\n            # 2. THE NEW GPU FORESTS (18-21)\n            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n\n            # 3. Competitors\n            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n\n            # 4. Souls (12-17)\n            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n\n            # 5. Neural (25)\n            self.unit_25,\n            self.unit_26\n        ]\n        # ... inside the loop where you iterate over units ...\n        for i, unit in enumerate(all_units):\n            if active_weights[i] > 0:\n                try:\n                    # SPECIAL HANDLING FOR UNIT 26 (HYPERNOVA)\n                    if i == 25: # Unit 26 is index 25 in the all_units list\n                        # We need the Rank 1 Elite's prediction on this NEW data\n                        # Recalculate Rank 1 Elite (It's already in self.final_elites_[0])\n                        # This works because Phase 4 retrained them on all data.\n                        elite_p = self.final_elites_[0].predict_proba(X_scaled)\n\n                        # Fuse manually for prediction\n                        X_fused_new = np.hstack([X_scaled, elite_p])\n\n                        # Unit 26 predicts on Fused Data\n                        p = unit.predict_proba(X_fused_new)\n\n                    else:\n                        # Standard Units\n                        if hasattr(unit, \"predict_proba\"):\n                            p = unit.predict_proba(X_scaled)\n                        else:\n                            d = unit.decision_function(X_scaled)\n                            max_d = np.max(d, axis=1, keepdims=True)\n                            exp_d = np.exp(d - max_d)\n                            p = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n\n                    # Accumulate\n                    if final_pred is None: final_pred = active_weights[i] * p\n                    else: final_pred += active_weights[i] * p\n                except: pass\n\n\n        if final_pred is None: return np.zeros(len(X)) # Fallback\n        return self.classes_[np.argmax(final_pred, axis=1)]\n\n\n    def _predict_proba_council_internal(self, X_scaled):\n        \"\"\"\n        Calculates the weighted probability matrix of the Council.\n        Essential for the Alien-Z sharpening process.\n        \"\"\"\n        final_pred = None\n        # Must match the order in your init\n        all_units= [\n            # 1. Logic (01-11)\n            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n\n            # 2. THE NEW GPU FORESTS (18-21)\n            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n\n            # 3. Competitors\n            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n\n            # 4. Souls (12-17)\n            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n\n            # 5. Neural (25)\n            self.unit_25,\n            self.unit_26\n        ]\n\n        # ... inside the loop where you iterate over units ...\n        for i, unit in enumerate(all_units):\n            if active_weights[i] > 0:\n                try:\n                    # SPECIAL HANDLING FOR UNIT 26 (HYPERNOVA)\n                    if i == 25: # Unit 26 is index 25 in the all_units list\n                        # We need the Rank 1 Elite's prediction on this NEW data\n                        # Recalculate Rank 1 Elite (It's already in self.final_elites_[0])\n                        # This works because Phase 4 retrained them on all data.\n                        elite_p = self.final_elites_[0].predict_proba(X_scaled)\n\n                        # Fuse manually for prediction\n                        X_fused_new = np.hstack([X_scaled, elite_p])\n\n                        # Unit 26 predicts on Fused Data\n                        p = unit.predict_proba(X_fused_new)\n\n                    else:\n                        # Standard Units\n                        if hasattr(unit, \"predict_proba\"):\n                            p = unit.predict_proba(X_scaled)\n                        else:\n                            d = unit.decision_function(X_scaled)\n                            max_d = np.max(d, axis=1, keepdims=True)\n                            exp_d = np.exp(d - max_d)\n                            p = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n\n                    # Accumulate\n                    if final_pred is None: final_pred = active_weights[i] * p\n                    else: final_pred += active_weights[i] * p\n                except: pass\n\n        # Safety fallback\n        if final_pred is None:\n            return np.ones((len(X_scaled), len(self.classes_))) / len(self.classes_)\n\n        # Normalize to ensure sum=1.0\n        return final_pred / np.sum(final_pred, axis=1, keepdims=True)\n\n    def _get_stack_features(self, X_scaled):\n        \"\"\"\n        Helper to gather predictions for the Linear Mirror strategy.\n        \"\"\"\n        X_stack_list = []\n        for unit in self.final_elites_:\n            if hasattr(unit, \"predict_proba\"):\n                p = unit.predict_proba(X_scaled)\n            else:\n                d = unit.decision_function(X_scaled)\n                p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n            X_stack_list.append(p)\n        return np.hstack(X_stack_list)\n\n\n    def _predict_mirror_internal(self, X, mode=\"hybrid\"):\n        X_sc = self.scaler_.transform(X)\n        X_stack_list = []\n        for unit in self.final_elites_:\n            if hasattr(unit, \"predict_proba\"): p = unit.predict_proba(X_sc)\n            else:\n                d = unit.decision_function(X_sc)\n                p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n            X_stack_list.append(p)\n        X_stack = np.hstack(X_stack_list)\n\n        model = self.unit_mirror_hybrid if mode == \"hybrid\" else self.unit_mirror_linear\n        return model.predict(X_stack)\n\n\n    def predict_proba(self, X):\n        X_scaled = self.scaler_.transform(X)\n\n        # --- PREPARE STRATEGY BASES (The Ingredients) ---\n        # 1. Get Top 2 Elite Predictions\n        pred_rank1 = self.final_elites_[0].predict_proba(X_scaled)\n        pred_rank2 = self.final_elites_[1].predict_proba(X_scaled)\n        \n        # 2. Construct The Strategies\n        p_council = (0.75 * pred_rank1) + (0.25 * pred_rank2)\n        p_ace = (0.90 * pred_rank1) + (0.10 * pred_rank2)\n        p_linear = (0.60 * pred_rank1) + (0.40 * pred_rank2)\n        \n        # Death Ray (Requires fusion)\n        X_fused = np.hstack([X_scaled, pred_rank1])\n        p_death_ray = self.unit_26.predict_proba(X_fused)\n        \n        # --- APPLY CHRONOS GATE (The Router) ---\n        # Get dynamic weights: [w_council, w_ace, w_linear, w_ray]\n        gate_weights = self.unit_chronos.get_gate_weights(X_scaled)\n        \n        # --- FINAL MIXING ---\n        final_pred = (\n            gate_weights[:, 0:1] * p_council +\n            gate_weights[:, 1:2] * p_ace +\n            gate_weights[:, 2:3] * p_linear +\n            gate_weights[:, 3:4] * p_death_ray\n        )\n        \n        # Normalize\n        row_sums = final_pred.sum(axis=1, keepdims=True)\n        return final_pred / (row_sums + 1e-9)\n\n\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n\ndef HarmonicResonanceForest_Ultimate(n_estimators=None):\n    return HarmonicResonanceClassifier_BEAST_21D(verbose=True)\n\n# --- ADD THIS AT THE ABSOLUTE BOTTOM ---\nif __name__ == \"__main__\":\n    # 1. Put your data loading here\n    # X, y = load_your_data()\n\n    # 2. Put your model execution here\n    # model = HarmonicResonanceForest_Ultimate()\n    # model.fit(X, y)\n\n    print(\"✅ Titan-21 Safety Protocol Engaged. System is stable.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T03:59:31.004693Z","iopub.execute_input":"2026-01-08T03:59:31.005582Z","iopub.status.idle":"2026-01-08T03:59:38.894055Z","shell.execute_reply.started":"2026-01-08T03:59:31.005546Z","shell.execute_reply":"2026-01-08T03:59:38.892762Z"}},"outputs":[{"name":"stdout","text":"✅ GPU DETECTED: HRF v26.0 'Holo-Fractal Universe' Active\n✅ Titan-21 Safety Protocol Engaged. System is stable.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils import resample\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\n\n# Updated to accept custom_X and custom_y\ndef run_comparative_benchmark(dataset_name, openml_id, sample_limit=3000, custom_X=None, custom_y=None):\n    print(f\"\\n[DATASET] Loading {dataset_name} (ID: {openml_id})...\")\n\n    try:\n        # --- PATH A: Custom Data Provided (Pre-cleaned) ---\n        if custom_X is not None and custom_y is not None:\n            print(\"  > Using provided Custom Data...\")\n            X = custom_X\n            y = custom_y\n\n            # Ensure X is numpy (in case a DF was passed)\n            if hasattr(X, 'values'):\n                X = X.values\n\n        # --- PATH B: Fetch from OpenML ---\n        else:\n            # Fetch as DataFrame to handle types better\n            X_df, y = fetch_openml(data_id=openml_id, return_X_y=True, as_frame=True, parser='auto')\n\n            # 1. AUTO-CLEANER: Convert Objects/Strings to Numbers (Only for DataFrames)\n            for col in X_df.columns:\n                if X_df[col].dtype == 'object' or X_df[col].dtype.name == 'category':\n                    le = LabelEncoder()\n                    X_df[col] = le.fit_transform(X_df[col].astype(str))\n\n            X = X_df.values # Convert to Numpy for HRF\n\n        # --- COMMON PIPELINE (NaN Handling) ---\n        # Even if custom data is passed, we double-check for NaNs to be safe\n        if np.isnan(X).any():\n            print(\"  > NaNs detected. Imputing with Mean strategy...\")\n            imp = SimpleImputer(strategy='mean')\n            X = imp.fit_transform(X)\n\n        le_y = LabelEncoder()\n        y = le_y.fit_transform(y)\n\n        # 3. GPU Limit Check\n        if len(X) > sample_limit:\n            print(f\"  ...Downsampling from {len(X)} to {sample_limit} (GPU Limit)...\")\n            X, y = resample(X, y, n_samples=sample_limit, random_state=42, stratify=y)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n        print(f\"  Shape: {X.shape} | Classes: {len(np.unique(y))}\")\n\n    except Exception as e:\n        print(f\"  Error loading data: {e}\")\n        return\n\n    competitors = {\n        \"SVM (RBF)\": make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n        \"XGBoost (GPU)\": XGBClassifier(\n            device='cuda',\n            tree_method='hist',\n            #use_label_encoder=False,\n            eval_metric='logloss',\n            random_state=42\n        ),\n        # Ensure your HRF class is defined in the notebook before running this\n        \"HRF Ultimate (GPU)\": HarmonicResonanceForest_Ultimate(n_estimators=60)\n    }\n\n    results = {}\n    print(f\"\\n[BENCHMARK] Executing comparisons on {dataset_name}...\")\n    print(\"-\" * 65)\n    print(f\"{'Model Name':<25} | {'Accuracy':<10} | {'Status'}\")\n    print(\"-\" * 65)\n\n    hrf_acc = 0\n\n    for name, model in competitors.items():\n        try:\n            model.fit(X_train, y_train)\n            preds = model.predict(X_test)\n            acc = accuracy_score(y_test, preds)\n            results[name] = acc\n            print(f\"{name:<25} | {acc:.4%}    | Done\")\n\n            if \"HRF\" in name:\n                hrf_acc = acc\n\n        except Exception as e:\n            print(f\"{name:<25} | FAILED      | {e}\")\n\n    print(\"-\" * 65)\n\n    best_competitor = 0\n    for k, v in results.items():\n        if \"HRF\" not in k and v > best_competitor:\n            best_competitor = v\n\n    margin = hrf_acc - best_competitor\n\n    if margin > 0:\n        print(f\" HRF WINNING MARGIN: +{margin:.4%}\")\n    else:\n        print(f\" HRF GAP: {margin:.4%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T03:59:38.895470Z","iopub.execute_input":"2026-01-08T03:59:38.895933Z","iopub.status.idle":"2026-01-08T03:59:39.044491Z","shell.execute_reply.started":"2026-01-08T03:59:38.895908Z","shell.execute_reply":"2026-01-08T03:59:39.043574Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# TEST 2: Phoneme (Search by Name to bypass ID error)\nrun_comparative_benchmark(\n    dataset_name=\"Phoneme\",\n    openml_id=None,  # We will skip ID\n    sample_limit=5404 \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:02:03.109015Z","iopub.execute_input":"2026-01-08T04:02:03.109367Z","iopub.status.idle":"2026-01-08T04:02:03.115418Z","shell.execute_reply.started":"2026-01-08T04:02:03.109342Z","shell.execute_reply":"2026-01-08T04:02:03.114247Z"}},"outputs":[{"name":"stdout","text":"\n[DATASET] Loading Phoneme (ID: None)...\n  Error loading data: Neither name nor data_id are provided. Please provide name or data_id.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}