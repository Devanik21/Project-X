{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SUPER TIC-TAC-TOE - KAGGLE GPU TRAINING CELL\n# ============================================================================\n# Run this single cell in Kaggle with GPU enabled\n# Downloads: super_ttt_agents.zip (optimized for quick load in Streamlit)\n# ============================================================================\n\nimport numpy as np\nfrom collections import deque\nimport random\nimport json\nimport zipfile\nimport io\nfrom tqdm import tqdm\nimport time\n\n# ============================================================================\n# ENVIRONMENT\n# ============================================================================\n\nclass SuperTicTacToe:\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.small_boards = [np.zeros((3, 3), dtype=int) for _ in range(9)]\n        self.meta_board = np.zeros(9, dtype=int)\n        self.current_player = 1\n        self.active_board = None\n        self.game_over = False\n        self.winner = None\n        self.move_history = []\n        return self.get_state()\n    \n    def get_state(self):\n        small_boards_flat = tuple(tuple(board.flatten()) for board in self.small_boards)\n        return (small_boards_flat, tuple(self.meta_board), self.active_board)\n    \n    def get_available_actions(self):\n        actions = []\n        if self.game_over:\n            return actions\n        \n        if self.active_board is not None and self.meta_board[self.active_board] == 0:\n            boards_to_check = [self.active_board]\n        else:\n            boards_to_check = [i for i in range(9) if self.meta_board[i] == 0]\n        \n        for board_idx in boards_to_check:\n            for r in range(3):\n                for c in range(3):\n                    if self.small_boards[board_idx][r, c] == 0:\n                        actions.append((board_idx, r, c))\n        \n        return actions\n    \n    def make_move(self, action):\n        if self.game_over:\n            return self.get_state(), 0, True\n        \n        board_idx, row, col = action\n        available = self.get_available_actions()\n        if action not in available:\n            return self.get_state(), -100, True\n        \n        self.small_boards[board_idx][row, col] = self.current_player\n        self.move_history.append((action, self.current_player))\n        \n        if self._check_small_board_win(board_idx, self.current_player):\n            self.meta_board[board_idx] = self.current_player\n            reward = 10\n        elif self._check_small_board_full(board_idx):\n            self.meta_board[board_idx] = -1\n            reward = 0\n        else:\n            reward = 0\n        \n        if self._check_meta_win(self.current_player):\n            self.game_over = True\n            self.winner = self.current_player\n            return self.get_state(), 1000, True\n        \n        if np.all(self.meta_board != 0):\n            self.game_over = True\n            self.winner = 0\n            return self.get_state(), 0, True\n        \n        next_board = row * 3 + col\n        if self.meta_board[next_board] == 0:\n            self.active_board = next_board\n        else:\n            self.active_board = None\n        \n        self.current_player = 3 - self.current_player\n        return self.get_state(), reward, False\n    \n    def _check_small_board_win(self, board_idx, player):\n        board = self.small_boards[board_idx]\n        for i in range(3):\n            if np.all(board[i, :] == player) or np.all(board[:, i] == player):\n                return True\n        if board[0, 0] == player and board[1, 1] == player and board[2, 2] == player:\n            return True\n        if board[0, 2] == player and board[1, 1] == player and board[2, 0] == player:\n            return True\n        return False\n    \n    def _check_small_board_full(self, board_idx):\n        return np.all(self.small_boards[board_idx] != 0)\n    \n    def _check_meta_win(self, player):\n        meta = self.meta_board.reshape(3, 3)\n        for i in range(3):\n            if np.all(meta[i, :] == player) or np.all(meta[:, i] == player):\n                return True\n        if meta[0, 0] == player and meta[1, 1] == player and meta[2, 2] == player:\n            return True\n        if meta[0, 2] == player and meta[1, 1] == player and meta[2, 0] == player:\n            return True\n        return False\n    \n    def evaluate_position(self, player):\n        if self.winner == player:\n            return 100000\n        if self.winner == (3 - player):\n            return -100000\n        if self.game_over:\n            return 0\n        \n        opponent = 3 - player\n        score = 0\n        \n        meta = self.meta_board.reshape(3, 3)\n        score += self._count_meta_lines(player, 2) * 500\n        score += self._count_meta_lines(player, 1) * 100\n        score -= self._count_meta_lines(opponent, 2) * 600\n        score -= self._count_meta_lines(opponent, 1) * 100\n        \n        strategic_boards = [4]\n        corner_boards = [0, 2, 6, 8]\n        \n        for b in strategic_boards:\n            if self.meta_board[b] == player:\n                score += 200\n            elif self.meta_board[b] == opponent:\n                score -= 200\n        \n        for b in corner_boards:\n            if self.meta_board[b] == player:\n                score += 100\n            elif self.meta_board[b] == opponent:\n                score -= 100\n        \n        for board_idx in range(9):\n            if self.meta_board[board_idx] == 0:\n                board_score = self._evaluate_small_board(board_idx, player)\n                score += board_score * 0.5\n        \n        if self.active_board is not None:\n            if self.meta_board[self.active_board] == 0:\n                score += 50\n        \n        return score\n    \n    def _count_meta_lines(self, player, count):\n        meta = self.meta_board.reshape(3, 3)\n        lines = 0\n        \n        for i in range(3):\n            row = meta[i, :]\n            col = meta[:, i]\n            if np.sum(row == player) == count and np.sum(row == (3-player)) == 0:\n                lines += 1\n            if np.sum(col == player) == count and np.sum(col == (3-player)) == 0:\n                lines += 1\n        \n        diag1 = [meta[0, 0], meta[1, 1], meta[2, 2]]\n        diag2 = [meta[0, 2], meta[1, 1], meta[2, 0]]\n        \n        if diag1.count(player) == count and (3-player) not in diag1:\n            lines += 1\n        if diag2.count(player) == count and (3-player) not in diag2:\n            lines += 1\n        \n        return lines\n    \n    def _evaluate_small_board(self, board_idx, player):\n        board = self.small_boards[board_idx]\n        opponent = 3 - player\n        score = 0\n        \n        lines_2 = 0\n        lines_1 = 0\n        opp_lines_2 = 0\n        \n        for i in range(3):\n            row = board[i, :]\n            col = board[:, i]\n            \n            if np.sum(row == player) == 2 and np.sum(row == opponent) == 0:\n                lines_2 += 1\n            elif np.sum(row == player) == 1 and np.sum(row == opponent) == 0:\n                lines_1 += 1\n            \n            if np.sum(row == opponent) == 2 and np.sum(row == player) == 0:\n                opp_lines_2 += 1\n            \n            if np.sum(col == player) == 2 and np.sum(col == opponent) == 0:\n                lines_2 += 1\n            elif np.sum(col == player) == 1 and np.sum(col == opponent) == 0:\n                lines_1 += 1\n            \n            if np.sum(col == opponent) == 2 and np.sum(col == player) == 0:\n                opp_lines_2 += 1\n        \n        diag1 = [board[0, 0], board[1, 1], board[2, 2]]\n        diag2 = [board[0, 2], board[1, 1], board[2, 0]]\n        \n        if diag1.count(player) == 2 and opponent not in diag1:\n            lines_2 += 1\n        if diag2.count(player) == 2 and opponent not in diag2:\n            lines_2 += 1\n        if diag1.count(opponent) == 2 and player not in diag1:\n            opp_lines_2 += 1\n        if diag2.count(opponent) == 2 and player not in diag2:\n            opp_lines_2 += 1\n        \n        score = lines_2 * 10 + lines_1 * 2 - opp_lines_2 * 12\n        return score\n\n# ============================================================================\n# AGENT\n# ============================================================================\n\nclass SuperTTTAgent:\n    def __init__(self, player_id, lr=0.1, gamma=0.95, epsilon=1.0,\n                 epsilon_decay=0.9995, epsilon_min=0.05):\n        self.player_id = player_id\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        \n        self.q_table = {}\n        self.experience_replay = deque(maxlen=50000)\n        self.minimax_depth = 2\n        \n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n    \n    def get_q_value(self, state, action):\n        return self.q_table.get((state, action), 0.0)\n    \n    def choose_action(self, env, training=True):\n        available = env.get_available_actions()\n        if not available:\n            return None\n        \n        # Immediate tactics\n        for action in available:\n            sim = self._simulate_move(env, action, self.player_id)\n            board_idx = action[0]\n            if sim.meta_board[board_idx] == self.player_id and env.meta_board[board_idx] == 0:\n                if sim._check_meta_win(self.player_id):\n                    return action\n        \n        for action in available:\n            sim = self._simulate_move(env, action, self.player_id)\n            if sim.winner == self.player_id:\n                return action\n        \n        opponent = 3 - self.player_id\n        for action in available:\n            sim = self._simulate_move(env, action, opponent)\n            if sim.winner == opponent:\n                return action\n        \n        # Strategic planning\n        if training and random.random() < self.epsilon:\n            strategic_actions = [a for a in available if a[0] in [4, 0, 2, 6, 8]]\n            if strategic_actions:\n                return random.choice(strategic_actions)\n            return random.choice(available)\n        \n        best_score = -float('inf')\n        best_actions = []\n        \n        alpha = -float('inf')\n        beta = float('inf')\n        \n        for action in available:\n            sim = self._simulate_move(env, action, self.player_id)\n            score = self._minimax(sim, self.minimax_depth - 1, alpha, beta, False)\n            \n            q_boost = self.get_q_value(env.get_state(), action) * 0.05\n            total_score = score + q_boost\n            \n            if total_score > best_score:\n                best_score = total_score\n                best_actions = [action]\n            elif abs(total_score - best_score) < 0.01:\n                best_actions.append(action)\n            \n            alpha = max(alpha, best_score)\n        \n        return random.choice(best_actions) if best_actions else random.choice(available)\n    \n    def _minimax(self, env, depth, alpha, beta, is_maximizing):\n        if env.winner == self.player_id:\n            return 10000 + depth\n        if env.winner == (3 - self.player_id):\n            return -10000 - depth\n        if env.game_over:\n            return 0\n        if depth == 0:\n            return env.evaluate_position(self.player_id)\n        \n        available = env.get_available_actions()\n        \n        if is_maximizing:\n            max_eval = -float('inf')\n            for action in available:\n                sim = self._simulate_move(env, action, self.player_id)\n                eval_score = self._minimax(sim, depth - 1, alpha, beta, False)\n                max_eval = max(max_eval, eval_score)\n                alpha = max(alpha, eval_score)\n                if beta <= alpha:\n                    break\n            return max_eval\n        else:\n            min_eval = float('inf')\n            opponent = 3 - self.player_id\n            for action in available:\n                sim = self._simulate_move(env, action, opponent)\n                eval_score = self._minimax(sim, depth - 1, alpha, beta, True)\n                min_eval = min(min_eval, eval_score)\n                beta = min(beta, eval_score)\n                if beta <= alpha:\n                    break\n            return min_eval\n    \n    def _simulate_move(self, env, action, player):\n        sim = SuperTicTacToe()\n        sim.small_boards = [board.copy() for board in env.small_boards]\n        sim.meta_board = env.meta_board.copy()\n        sim.current_player = player\n        sim.active_board = env.active_board\n        sim.make_move(action)\n        return sim\n    \n    def update_q_value(self, state, action, reward, next_state, next_actions):\n        current_q = self.get_q_value(state, action)\n        if next_actions:\n            max_next_q = max([self.get_q_value(next_state, a) for a in next_actions])\n        else:\n            max_next_q = 0\n        \n        td_error = reward + self.gamma * max_next_q - current_q\n        new_q = current_q + self.lr * td_error\n        self.q_table[(state, action)] = new_q\n    \n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    \n    def reset_stats(self):\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n\n# ============================================================================\n# TRAINING FUNCTIONS\n# ============================================================================\n\ndef play_game(env, agent1, agent2, training=True):\n    env.reset()\n    game_history = []\n    agents = {1: agent1, 2: agent2}\n    \n    while not env.game_over:\n        current_player = env.current_player\n        current_agent = agents[current_player]\n        \n        state = env.get_state()\n        action = current_agent.choose_action(env, training)\n        \n        if action is None:\n            break\n        \n        game_history.append((state, action, current_player))\n        next_state, reward, done = env.make_move(action)\n        \n        if training:\n            next_actions = env.get_available_actions()\n            current_agent.update_q_value(state, action, reward, next_state, next_actions)\n        \n        if done:\n            if env.winner == 1:\n                agent1.wins += 1\n                agent2.losses += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, 100)\n                    _update_outcome(agent2, game_history, 2, -50)\n            elif env.winner == 2:\n                agent2.wins += 1\n                agent1.losses += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, -50)\n                    _update_outcome(agent2, game_history, 2, 100)\n            else:\n                agent1.draws += 1\n                agent2.draws += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, -10)\n                    _update_outcome(agent2, game_history, 2, -10)\n    \n    return env.winner\n\ndef _update_outcome(agent, history, player_id, final_reward):\n    agent_moves = [(s, a) for s, a, p in history if p == player_id]\n    for i in range(len(agent_moves) - 1, -1, -1):\n        state, action = agent_moves[i]\n        discount = agent.gamma ** (len(agent_moves) - 1 - i)\n        adjusted_reward = final_reward * discount\n        current_q = agent.get_q_value(state, action)\n        new_q = current_q + agent.lr * (adjusted_reward - current_q)\n        agent.q_table[(state, action)] = new_q\n\n# ============================================================================\n# OPTIMIZED SERIALIZATION (Critical for fast upload/download)\n# ============================================================================\n\ndef serialize_q_table_optimized(q_table):\n    \"\"\"Ultra-compact serialization using string keys\"\"\"\n    serialized = {}\n    \n    for (state, action), value in q_table.items():\n        # Compact string representation\n        # State: small_boards (9x9), meta_board (9), active_board\n        small_boards_str = ','.join([''.join(map(str, board)) for board in state[0]])\n        \n        # FIX: Use comma delimiter for meta_board to handle -1 (draws) correctly\n        meta_str = ','.join(map(str, state[1]))\n        \n        active_str = str(state[2]) if state[2] is not None else 'N'\n        \n        # Action: (board_idx, row, col)\n        action_str = f\"{action[0]}{action[1]}{action[2]}\"\n        \n        # Combine into single compact key\n        key = f\"{small_boards_str}|{meta_str}|{active_str}|{action_str}\"\n        \n        # Store only significant digits to reduce size\n        serialized[key] = round(float(value), 4)\n    \n    return serialized\n    \n\ndef deserialize_q_table_optimized(serialized):\n    \"\"\"Deserialize the compact format\"\"\"\n    q_table = {}\n    \n    for key, value in serialized.items():\n        parts = key.split('|')\n        \n        # Parse small boards\n        boards_str = parts[0].split(',')\n        small_boards = tuple(\n            tuple(int(c) for c in board_str)\n            for board_str in boards_str\n        )\n        \n        # FIX: Split by comma to parse negative numbers (-1) correctly\n        meta_board = tuple(int(x) for x in parts[1].split(','))\n        \n        # Parse active board\n        active_board = None if parts[2] == 'N' else int(parts[2])\n        \n        # Parse action\n        action_str = parts[3]\n        action = (int(action_str[0]), int(action_str[1]), int(action_str[2]))\n        \n        # Reconstruct state\n        state = (small_boards, meta_board, active_board)\n        \n        q_table[(state, action)] = value\n    \n    return q_table\n    \n\ndef create_training_zip(agent1, agent2, config, training_stats):\n    \"\"\"Create optimized zip file for download\"\"\"\n    \n    print(\"ðŸ“¦ Packaging agents...\")\n    \n    agent1_data = {\n        \"q_table\": serialize_q_table_optimized(agent1.q_table),\n        \"epsilon\": round(agent1.epsilon, 6),\n        \"lr\": agent1.lr,\n        \"gamma\": agent1.gamma,\n        \"minimax_depth\": agent1.minimax_depth,\n        \"wins\": agent1.wins,\n        \"losses\": agent1.losses,\n        \"draws\": agent1.draws\n    }\n    \n    agent2_data = {\n        \"q_table\": serialize_q_table_optimized(agent2.q_table),\n        \"epsilon\": round(agent2.epsilon, 6),\n        \"lr\": agent2.lr,\n        \"gamma\": agent2.gamma,\n        \"minimax_depth\": agent2.minimax_depth,\n        \"wins\": agent2.wins,\n        \"losses\": agent2.losses,\n        \"draws\": agent2.draws\n    }\n    \n    buffer = io.BytesIO()\n    with zipfile.ZipFile(buffer, \"w\", zipfile.ZIP_DEFLATED, compresslevel=9) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(agent1_data))\n        zf.writestr(\"agent2.json\", json.dumps(agent2_data))\n        zf.writestr(\"config.json\", json.dumps(config))\n        zf.writestr(\"training_stats.json\", json.dumps(training_stats))\n        \n        # Add metadata\n        metadata = {\n            \"trained_episodes\": config.get(\"episodes\", 0),\n            \"final_q_size_agent1\": len(agent1.q_table),\n            \"final_q_size_agent2\": len(agent2.q_table),\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n        zf.writestr(\"metadata.json\", json.dumps(metadata, indent=2))\n    \n    buffer.seek(0)\n    return buffer\n\n# ============================================================================\n# MAIN TRAINING LOOP\n# ============================================================================\n\nprint(\"ðŸŽ¯ Super Tic-Tac-Toe - GPU Training Session\")\nprint(\"=\" * 60)\n\n# Hyperparameters (optimized for speed and quality)\nEPISODES = 1000  # Adjust based on available time\nLR1 = 0.15\nGAMMA1 = 0.98\nMINIMAX_DEPTH1 = 3\n\nLR2 = 0.15\nGAMMA2 = 0.98\nMINIMAX_DEPTH2 = 3\n\nEPSILON_DECAY = 0.9998\nUPDATE_FREQ = 100\n\nprint(f\"\\nâš™ï¸  Configuration:\")\nprint(f\"   Episodes: {EPISODES:,}\")\nprint(f\"   Learning Rate: {LR1}\")\nprint(f\"   Gamma: {GAMMA1}\")\nprint(f\"   Minimax Depth: {MINIMAX_DEPTH1}\")\nprint(f\"   Epsilon Decay: {EPSILON_DECAY}\")\nprint()\n\n# Initialize\nenv = SuperTicTacToe()\nagent1 = SuperTTTAgent(1, lr=LR1, gamma=GAMMA1, epsilon_decay=EPSILON_DECAY)\nagent1.minimax_depth = MINIMAX_DEPTH1\nagent2 = SuperTTTAgent(2, lr=LR2, gamma=GAMMA2, epsilon_decay=EPSILON_DECAY)\nagent2.minimax_depth = MINIMAX_DEPTH2\n\n# Training stats\ntraining_stats = {\n    'episodes': [],\n    'agent1_wins': [],\n    'agent2_wins': [],\n    'draws': [],\n    'agent1_epsilon': [],\n    'agent2_epsilon': [],\n    'agent1_q_size': [],\n    'agent2_q_size': []\n}\n\n# Training loop with progress bar\nprint(\"ðŸš€ Training started...\\n\")\nstart_time = time.time()\n\nfor episode in tqdm(range(1, EPISODES + 1), desc=\"Training\"):\n    play_game(env, agent1, agent2, training=True)\n    agent1.decay_epsilon()\n    agent2.decay_epsilon()\n    \n    # Update stats periodically\n    if episode % UPDATE_FREQ == 0:\n        training_stats['episodes'].append(episode)\n        training_stats['agent1_wins'].append(agent1.wins)\n        training_stats['agent2_wins'].append(agent2.wins)\n        training_stats['draws'].append(agent1.draws)\n        training_stats['agent1_epsilon'].append(round(agent1.epsilon, 6))\n        training_stats['agent2_epsilon'].append(round(agent2.epsilon, 6))\n        training_stats['agent1_q_size'].append(len(agent1.q_table))\n        training_stats['agent2_q_size'].append(len(agent2.q_table))\n        \n        # Print progress\n        win_rate_1 = agent1.wins / episode * 100\n        win_rate_2 = agent2.wins / episode * 100\n        draw_rate = agent1.draws / episode * 100\n        \n        print(f\"\\nðŸ“Š Episode {episode:,}/{EPISODES:,}\")\n        print(f\"   Agent 1: {agent1.wins:,} wins ({win_rate_1:.1f}%) | Îµ={agent1.epsilon:.4f} | Q={len(agent1.q_table):,}\")\n        print(f\"   Agent 2: {agent2.wins:,} wins ({win_rate_2:.1f}%) | Îµ={agent2.epsilon:.4f} | Q={len(agent2.q_table):,}\")\n        print(f\"   Draws: {agent1.draws:,} ({draw_rate:.1f}%)\")\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nâœ… Training complete in {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n\n# Final statistics\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ“ˆ FINAL STATISTICS\")\nprint(\"=\" * 60)\nprint(f\"Agent 1 (Blue):\")\nprint(f\"  Wins: {agent1.wins:,} ({agent1.wins/EPISODES*100:.1f}%)\")\nprint(f\"  Q-Table Size: {len(agent1.q_table):,} states\")\nprint(f\"  Final Epsilon: {agent1.epsilon:.6f}\")\nprint()\nprint(f\"Agent 2 (Red):\")\nprint(f\"  Wins: {agent2.wins:,} ({agent2.wins/EPISODES*100:.1f}%)\")\nprint(f\"  Q-Table Size: {len(agent2.q_table):,} states\")\nprint(f\"  Final Epsilon: {agent2.epsilon:.6f}\")\nprint()\nprint(f\"Draws: {agent1.draws:,} ({agent1.draws/EPISODES*100:.1f}%)\")\nprint()\n\n# Create config\nconfig = {\n    \"episodes\": EPISODES,\n    \"lr1\": LR1,\n    \"gamma1\": GAMMA1,\n    \"minimax1\": MINIMAX_DEPTH1,\n    \"lr2\": LR2,\n    \"gamma2\": GAMMA2,\n    \"minimax2\": MINIMAX_DEPTH2,\n    \"epsilon_decay\": EPSILON_DECAY,\n    \"training_time_seconds\": round(elapsed_time, 2)\n}\n\n# Save to zip\nprint(\"ðŸ’¾ Creating download package...\")\nzip_buffer = create_training_zip(agent1, agent2, config, training_stats)\n\n# Save to file\noutput_filename = \"super_ttt_agents_adv.zip\"\nwith open(output_filename, \"wb\") as f:\n    f.write(zip_buffer.getvalue())\n\nfile_size_mb = len(zip_buffer.getvalue()) / (1024 * 1024)\nprint(f\"âœ… Saved to: {output_filename} ({file_size_mb:.2f} MB)\")\nprint()\nprint(\"=\" * 60)\nprint(\"ðŸŽ‰ SUCCESS! Download the .zip file and upload to Streamlit!\")\nprint(\"=\" * 60)\nprint()\nprint(\"ðŸ“ Quick Stats:\")\nprint(f\"   â€¢ Total Episodes: {EPISODES:,}\")\nprint(f\"   â€¢ Training Time: {elapsed_time/60:.1f} minutes\")\nprint(f\"   â€¢ Agent 1 Q-States: {len(agent1.q_table):,}\")\nprint(f\"   â€¢ Agent 2 Q-States: {len(agent2.q_table):,}\")\nprint(f\"   â€¢ Package Size: {file_size_mb:.2f} MB\")\nprint()\nprint(\"ðŸš€ Ready for deployment in Streamlit!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:57:20.161462Z","iopub.execute_input":"2026-01-15T13:57:20.161780Z","iopub.status.idle":"2026-01-15T13:59:20.399937Z","shell.execute_reply.started":"2026-01-15T13:57:20.161754Z","shell.execute_reply":"2026-01-15T13:59:20.399010Z"}},"outputs":[{"name":"stdout","text":"ðŸŽ¯ Super Tic-Tac-Toe - GPU Training Session\n============================================================\n\nâš™ï¸  Configuration:\n   Episodes: 10,000\n   Learning Rate: 0.12\n   Gamma: 0.98\n   Minimax Depth: 2\n   Epsilon Decay: 0.9998\n\nðŸš€ Training started...\n\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 100/10000 [00:22<43:54,  3.76it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 100/10,000\n   Agent 1: 42 wins (42.0%) | Îµ=0.9802 | Q=2,769\n   Agent 2: 30 wins (30.0%) | Îµ=0.9802 | Q=2,767\n   Draws: 28 (28.0%)\n","output_type":"stream"},{"name":"stderr","text":"Training:   2%|â–         | 200/10000 [00:48<1:03:09,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 200/10,000\n   Agent 1: 85 wins (42.5%) | Îµ=0.9608 | Q=5,483\n   Agent 2: 64 wins (32.0%) | Îµ=0.9608 | Q=5,488\n   Draws: 51 (25.5%)\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|â–Ž         | 300/10000 [01:15<39:09,  4.13it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 300/10,000\n   Agent 1: 124 wins (41.3%) | Îµ=0.9418 | Q=8,149\n   Agent 2: 94 wins (31.3%) | Îµ=0.9418 | Q=8,163\n   Draws: 82 (27.3%)\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|â–         | 400/10000 [01:46<53:32,  2.99it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 400/10,000\n   Agent 1: 161 wins (40.2%) | Îµ=0.9231 | Q=10,771\n   Agent 2: 128 wins (32.0%) | Îµ=0.9231 | Q=10,794\n   Draws: 111 (27.8%)\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|â–         | 437/10000 [02:00<43:49,  3.64it/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/614340576.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPISODES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/614340576.py\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(env, agent1, agent2, training)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/614340576.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, env, training)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax_depth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mq_boost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_q_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/614340576.py\u001b[0m in \u001b[0;36m_minimax\u001b[0;34m(self, env, depth, alpha, beta, is_maximizing)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0meval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m                 \u001b[0mmin_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/614340576.py\u001b[0m in \u001b[0;36m_minimax\u001b[0;34m(self, env, depth, alpha, beta, is_maximizing)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mavailable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_available_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/614340576.py\u001b[0m in \u001b[0;36mevaluate_position\u001b[0;34m(self, player)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mboard_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_board\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mboard_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mboard_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_small_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mboard_score\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/614340576.py\u001b[0m in \u001b[0;36m_evaluate_small_board\u001b[0;34m(self, board_idx, player)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mlines_2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0mlines_1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[1;32m     71\u001b[0m                   if v is not np._NoValue}\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# SUPER TIC-TAC-TOE - TITAN VANGUARD EDITION (20-MIN OPTIMIZATION)\n# ============================================================================\n# Run this single cell in Kaggle with GPU enabled\n# Optimization Goal: Maximize human-crushing ability in <20 mins\n# ============================================================================\n\nimport numpy as np\nfrom collections import deque\nimport random\nimport json\nimport zipfile\nimport io\nfrom tqdm import tqdm\nimport time\n\n# ============================================================================\n# ENVIRONMENT\n# ============================================================================\n\nclass SuperTicTacToe:\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.small_boards = [np.zeros((3, 3), dtype=int) for _ in range(9)]\n        self.meta_board = np.zeros(9, dtype=int)\n        self.current_player = 1\n        self.active_board = None\n        self.game_over = False\n        self.winner = None\n        self.move_history = []\n        return self.get_state()\n    \n    def get_state(self):\n        small_boards_flat = tuple(tuple(board.flatten()) for board in self.small_boards)\n        return (small_boards_flat, tuple(self.meta_board), self.active_board)\n    \n    def get_available_actions(self):\n        actions = []\n        if self.game_over:\n            return actions\n        \n        if self.active_board is not None and self.meta_board[self.active_board] == 0:\n            boards_to_check = [self.active_board]\n        else:\n            boards_to_check = [i for i in range(9) if self.meta_board[i] == 0]\n        \n        for board_idx in boards_to_check:\n            for r in range(3):\n                for c in range(3):\n                    if self.small_boards[board_idx][r, c] == 0:\n                        actions.append((board_idx, r, c))\n        \n        return actions\n    \n    def make_move(self, action):\n        if self.game_over:\n            return self.get_state(), 0, True\n        \n        board_idx, row, col = action\n        available = self.get_available_actions()\n        if action not in available:\n            return self.get_state(), -100, True\n        \n        self.small_boards[board_idx][row, col] = self.current_player\n        self.move_history.append((action, self.current_player))\n        \n        if self._check_small_board_win(board_idx, self.current_player):\n            self.meta_board[board_idx] = self.current_player\n            reward = 10\n        elif self._check_small_board_full(board_idx):\n            self.meta_board[board_idx] = -1\n            reward = 0\n        else:\n            reward = 0\n        \n        if self._check_meta_win(self.current_player):\n            self.game_over = True\n            self.winner = self.current_player\n            return self.get_state(), 1000, True\n        \n        if np.all(self.meta_board != 0):\n            self.game_over = True\n            self.winner = 0\n            return self.get_state(), 0, True\n        \n        next_board = row * 3 + col\n        if self.meta_board[next_board] == 0:\n            self.active_board = next_board\n        else:\n            self.active_board = None\n        \n        self.current_player = 3 - self.current_player\n        return self.get_state(), reward, False\n    \n    def _check_small_board_win(self, board_idx, player):\n        board = self.small_boards[board_idx]\n        for i in range(3):\n            if np.all(board[i, :] == player) or np.all(board[:, i] == player):\n                return True\n        if board[0, 0] == player and board[1, 1] == player and board[2, 2] == player:\n            return True\n        if board[0, 2] == player and board[1, 1] == player and board[2, 0] == player:\n            return True\n        return False\n    \n    def _check_small_board_full(self, board_idx):\n        return np.all(self.small_boards[board_idx] != 0)\n    \n    def _check_meta_win(self, player):\n        meta = self.meta_board.reshape(3, 3)\n        for i in range(3):\n            if np.all(meta[i, :] == player) or np.all(meta[:, i] == player):\n                return True\n        if meta[0, 0] == player and meta[1, 1] == player and meta[2, 2] == player:\n            return True\n        if meta[0, 2] == player and meta[1, 1] == player and meta[2, 0] == player:\n            return True\n        return False\n    \n    def evaluate_position(self, player):\n        if self.winner == player:\n            return 100000\n        if self.winner == (3 - player):\n            return -100000\n        if self.game_over:\n            return 0\n        \n        opponent = 3 - player\n        score = 0\n        \n        meta = self.meta_board.reshape(3, 3)\n        score += self._count_meta_lines(player, 2) * 500\n        score += self._count_meta_lines(player, 1) * 100\n        score -= self._count_meta_lines(opponent, 2) * 600\n        score -= self._count_meta_lines(opponent, 1) * 100\n        \n        strategic_boards = [4]\n        corner_boards = [0, 2, 6, 8]\n        \n        for b in strategic_boards:\n            if self.meta_board[b] == player:\n                score += 200\n            elif self.meta_board[b] == opponent:\n                score -= 200\n        \n        for b in corner_boards:\n            if self.meta_board[b] == player:\n                score += 100\n            elif self.meta_board[b] == opponent:\n                score -= 100\n        \n        for board_idx in range(9):\n            if self.meta_board[board_idx] == 0:\n                board_score = self._evaluate_small_board(board_idx, player)\n                score += board_score * 0.5\n        \n        if self.active_board is not None:\n            if self.meta_board[self.active_board] == 0:\n                score += 50\n        \n        return score\n    \n    def _count_meta_lines(self, player, count):\n        meta = self.meta_board.reshape(3, 3)\n        lines = 0\n        \n        for i in range(3):\n            row = meta[i, :]\n            col = meta[:, i]\n            if np.sum(row == player) == count and np.sum(row == (3-player)) == 0:\n                lines += 1\n            if np.sum(col == player) == count and np.sum(col == (3-player)) == 0:\n                lines += 1\n        \n        diag1 = [meta[0, 0], meta[1, 1], meta[2, 2]]\n        diag2 = [meta[0, 2], meta[1, 1], meta[2, 0]]\n        \n        if diag1.count(player) == count and (3-player) not in diag1:\n            lines += 1\n        if diag2.count(player) == count and (3-player) not in diag2:\n            lines += 1\n        \n        return lines\n    \n    def _evaluate_small_board(self, board_idx, player):\n        board = self.small_boards[board_idx]\n        opponent = 3 - player\n        score = 0\n        \n        lines_2 = 0\n        lines_1 = 0\n        opp_lines_2 = 0\n        \n        for i in range(3):\n            row = board[i, :]\n            col = board[:, i]\n            \n            if np.sum(row == player) == 2 and np.sum(row == opponent) == 0:\n                lines_2 += 1\n            elif np.sum(row == player) == 1 and np.sum(row == opponent) == 0:\n                lines_1 += 1\n            \n            if np.sum(row == opponent) == 2 and np.sum(row == player) == 0:\n                opp_lines_2 += 1\n            \n            if np.sum(col == player) == 2 and np.sum(col == opponent) == 0:\n                lines_2 += 1\n            elif np.sum(col == player) == 1 and np.sum(col == opponent) == 0:\n                lines_1 += 1\n            \n            if np.sum(col == opponent) == 2 and np.sum(col == player) == 0:\n                opp_lines_2 += 1\n        \n        diag1 = [board[0, 0], board[1, 1], board[2, 2]]\n        diag2 = [board[0, 2], board[1, 1], board[2, 0]]\n        \n        if diag1.count(player) == 2 and opponent not in diag1:\n            lines_2 += 1\n        if diag2.count(player) == 2 and opponent not in diag2:\n            lines_2 += 1\n        if diag1.count(opponent) == 2 and player not in diag1:\n            opp_lines_2 += 1\n        if diag2.count(opponent) == 2 and player not in diag2:\n            opp_lines_2 += 1\n        \n        score = lines_2 * 10 + lines_1 * 2 - opp_lines_2 * 12\n        return score\n\n# ============================================================================\n# AGENT\n# ============================================================================\n\nclass SuperTTTAgent:\n    def __init__(self, player_id, lr=0.1, gamma=0.95, epsilon=1.0,\n                 epsilon_decay=0.9995, epsilon_min=0.05):\n        self.player_id = player_id\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        \n        self.q_table = {}\n        self.experience_replay = deque(maxlen=50000)\n        self.minimax_depth = 2\n        \n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n    \n    def get_q_value(self, state, action):\n        return self.q_table.get((state, action), 0.0)\n    \n    def choose_action(self, env, training=True):\n        available = env.get_available_actions()\n        if not available:\n            return None\n        \n        # Immediate tactics\n        for action in available:\n            sim = self._simulate_move(env, action, self.player_id)\n            board_idx = action[0]\n            if sim.meta_board[board_idx] == self.player_id and env.meta_board[board_idx] == 0:\n                if sim._check_meta_win(self.player_id):\n                    return action\n        \n        for action in available:\n            sim = self._simulate_move(env, action, self.player_id)\n            if sim.winner == self.player_id:\n                return action\n        \n        opponent = 3 - self.player_id\n        for action in available:\n            sim = self._simulate_move(env, action, opponent)\n            if sim.winner == opponent:\n                return action\n        \n        # Strategic planning\n        if training and random.random() < self.epsilon:\n            strategic_actions = [a for a in available if a[0] in [4, 0, 2, 6, 8]]\n            if strategic_actions:\n                return random.choice(strategic_actions)\n            return random.choice(available)\n        \n        best_score = -float('inf')\n        best_actions = []\n        \n        alpha = -float('inf')\n        beta = float('inf')\n        \n        for action in available:\n            sim = self._simulate_move(env, action, self.player_id)\n            score = self._minimax(sim, self.minimax_depth - 1, alpha, beta, False)\n            \n            q_boost = self.get_q_value(env.get_state(), action) * 0.05\n            total_score = score + q_boost\n            \n            if total_score > best_score:\n                best_score = total_score\n                best_actions = [action]\n            elif abs(total_score - best_score) < 0.01:\n                best_actions.append(action)\n            \n            alpha = max(alpha, best_score)\n        \n        return random.choice(best_actions) if best_actions else random.choice(available)\n    \n    def _minimax(self, env, depth, alpha, beta, is_maximizing):\n        if env.winner == self.player_id:\n            return 10000 + depth\n        if env.winner == (3 - self.player_id):\n            return -10000 - depth\n        if env.game_over:\n            return 0\n        if depth == 0:\n            return env.evaluate_position(self.player_id)\n        \n        available = env.get_available_actions()\n        \n        if is_maximizing:\n            max_eval = -float('inf')\n            for action in available:\n                sim = self._simulate_move(env, action, self.player_id)\n                eval_score = self._minimax(sim, depth - 1, alpha, beta, False)\n                max_eval = max(max_eval, eval_score)\n                alpha = max(alpha, eval_score)\n                if beta <= alpha:\n                    break\n            return max_eval\n        else:\n            min_eval = float('inf')\n            opponent = 3 - self.player_id\n            for action in available:\n                sim = self._simulate_move(env, action, opponent)\n                eval_score = self._minimax(sim, depth - 1, alpha, beta, True)\n                min_eval = min(min_eval, eval_score)\n                beta = min(beta, eval_score)\n                if beta <= alpha:\n                    break\n            return min_eval\n    \n    def _simulate_move(self, env, action, player):\n        sim = SuperTicTacToe()\n        sim.small_boards = [board.copy() for board in env.small_boards]\n        sim.meta_board = env.meta_board.copy()\n        sim.current_player = player\n        sim.active_board = env.active_board\n        sim.make_move(action)\n        return sim\n    \n    def update_q_value(self, state, action, reward, next_state, next_actions):\n        current_q = self.get_q_value(state, action)\n        if next_actions:\n            max_next_q = max([self.get_q_value(next_state, a) for a in next_actions])\n        else:\n            max_next_q = 0\n        \n        td_error = reward + self.gamma * max_next_q - current_q\n        new_q = current_q + self.lr * td_error\n        self.q_table[(state, action)] = new_q\n    \n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    \n    def reset_stats(self):\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n\n# ============================================================================\n# TRAINING FUNCTIONS\n# ============================================================================\n\ndef play_game(env, agent1, agent2, training=True):\n    env.reset()\n    game_history = []\n    agents = {1: agent1, 2: agent2}\n    \n    while not env.game_over:\n        current_player = env.current_player\n        current_agent = agents[current_player]\n        \n        state = env.get_state()\n        action = current_agent.choose_action(env, training)\n        \n        if action is None:\n            break\n        \n        game_history.append((state, action, current_player))\n        next_state, reward, done = env.make_move(action)\n        \n        if training:\n            next_actions = env.get_available_actions()\n            current_agent.update_q_value(state, action, reward, next_state, next_actions)\n        \n        if done:\n            if env.winner == 1:\n                agent1.wins += 1\n                agent2.losses += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, 100)\n                    _update_outcome(agent2, game_history, 2, -50)\n            elif env.winner == 2:\n                agent2.wins += 1\n                agent1.losses += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, -50)\n                    _update_outcome(agent2, game_history, 2, 100)\n            else:\n                agent1.draws += 1\n                agent2.draws += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, -10)\n                    _update_outcome(agent2, game_history, 2, -10)\n    \n    return env.winner\n\ndef _update_outcome(agent, history, player_id, final_reward):\n    agent_moves = [(s, a) for s, a, p in history if p == player_id]\n    for i in range(len(agent_moves) - 1, -1, -1):\n        state, action = agent_moves[i]\n        discount = agent.gamma ** (len(agent_moves) - 1 - i)\n        adjusted_reward = final_reward * discount\n        current_q = agent.get_q_value(state, action)\n        new_q = current_q + agent.lr * (adjusted_reward - current_q)\n        agent.q_table[(state, action)] = new_q\n\n# ============================================================================\n# OPTIMIZED SERIALIZATION (Critical for fast upload/download)\n# ============================================================================\n\ndef serialize_q_table_optimized(q_table):\n    \"\"\"Ultra-compact serialization using string keys\"\"\"\n    serialized = {}\n    \n    for (state, action), value in q_table.items():\n        # Compact string representation\n        # State: small_boards (9x9), meta_board (9), active_board\n        small_boards_str = ','.join([''.join(map(str, board)) for board in state[0]])\n        \n        # FIX: Use comma delimiter for meta_board to handle -1 (draws) correctly\n        meta_str = ','.join(map(str, state[1]))\n        \n        active_str = str(state[2]) if state[2] is not None else 'N'\n        \n        # Action: (board_idx, row, col)\n        action_str = f\"{action[0]}{action[1]}{action[2]}\"\n        \n        # Combine into single compact key\n        key = f\"{small_boards_str}|{meta_str}|{active_str}|{action_str}\"\n        \n        # Store only significant digits to reduce size\n        serialized[key] = round(float(value), 4)\n    \n    return serialized\n    \n\ndef deserialize_q_table_optimized(serialized):\n    \"\"\"Deserialize the compact format\"\"\"\n    q_table = {}\n    \n    for key, value in serialized.items():\n        parts = key.split('|')\n        \n        # Parse small boards\n        boards_str = parts[0].split(',')\n        small_boards = tuple(\n            tuple(int(c) for c in board_str)\n            for board_str in boards_str\n        )\n        \n        # FIX: Split by comma to parse negative numbers (-1) correctly\n        meta_board = tuple(int(x) for x in parts[1].split(','))\n        \n        # Parse active board\n        active_board = None if parts[2] == 'N' else int(parts[2])\n        \n        # Parse action\n        action_str = parts[3]\n        action = (int(action_str[0]), int(action_str[1]), int(action_str[2]))\n        \n        # Reconstruct state\n        state = (small_boards, meta_board, active_board)\n        \n        q_table[(state, action)] = value\n    \n    return q_table\n    \n\ndef create_training_zip(agent1, agent2, config, training_stats):\n    \"\"\"Create optimized zip file for download\"\"\"\n    \n    print(\"ðŸ“¦ Packaging agents...\")\n    \n    agent1_data = {\n        \"q_table\": serialize_q_table_optimized(agent1.q_table),\n        \"epsilon\": round(agent1.epsilon, 6),\n        \"lr\": agent1.lr,\n        \"gamma\": agent1.gamma,\n        \"minimax_depth\": agent1.minimax_depth,\n        \"wins\": agent1.wins,\n        \"losses\": agent1.losses,\n        \"draws\": agent1.draws\n    }\n    \n    agent2_data = {\n        \"q_table\": serialize_q_table_optimized(agent2.q_table),\n        \"epsilon\": round(agent2.epsilon, 6),\n        \"lr\": agent2.lr,\n        \"gamma\": agent2.gamma,\n        \"minimax_depth\": agent2.minimax_depth,\n        \"wins\": agent2.wins,\n        \"losses\": agent2.losses,\n        \"draws\": agent2.draws\n    }\n    \n    buffer = io.BytesIO()\n    with zipfile.ZipFile(buffer, \"w\", zipfile.ZIP_DEFLATED, compresslevel=9) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(agent1_data))\n        zf.writestr(\"agent2.json\", json.dumps(agent2_data))\n        zf.writestr(\"config.json\", json.dumps(config))\n        zf.writestr(\"training_stats.json\", json.dumps(training_stats))\n        \n        # Add metadata\n        metadata = {\n            \"trained_episodes\": config.get(\"episodes\", 0),\n            \"final_q_size_agent1\": len(agent1.q_table),\n            \"final_q_size_agent2\": len(agent2.q_table),\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n        zf.writestr(\"metadata.json\", json.dumps(metadata, indent=2))\n    \n    buffer.seek(0)\n    return buffer\n\n# ============================================================================\n# MAIN TRAINING LOOP\n# ============================================================================\n\nprint(\"ðŸŽ¯ Super Tic-Tac-Toe - GPU Training Session\")\nprint(\"=\" * 60)\n\n# ============================================================================\n# TITAN-20 OPTIMIZED HYPERPARAMETERS\n# Goal: Maximum strength in ~20 minutes on Kaggle T4 GPU\n# ============================================================================\nEPISODES = 500       # The limit of what fits in 20 mins at Depth 3\nLR1 = 0.14            # Aggressive learning (standard is 0.1)\nGAMMA1 = 0.99         # Maximum foresight (standard is 0.95)\nMINIMAX_DEPTH1 = 4    # The Vanguard Depth (balances speed/tactics)\n\nLR2 = 0.14\nGAMMA2 = 0.99\nMINIMAX_DEPTH2 = 4\n\n# Decay to 0.05 by episode ~2800 to refine skills\nEPSILON_DECAY = 0.9998 \nUPDATE_FREQ =100     # Show progress every 250 games\n\nprint(f\"\\nâš™ï¸  Configuration (Titan-20 Protocol):\")\nprint(f\"   Episodes: {EPISODES:,}\")\nprint(f\"   Learning Rate: {LR1}\")\nprint(f\"   Gamma: {GAMMA1}\")\nprint(f\"   Minimax Depth: {MINIMAX_DEPTH1}\")\nprint(f\"   Epsilon Decay: {EPSILON_DECAY}\")\nprint()\n\n# Initialize\nenv = SuperTicTacToe()\nagent1 = SuperTTTAgent(1, lr=LR1, gamma=GAMMA1, epsilon_decay=EPSILON_DECAY)\nagent1.minimax_depth = MINIMAX_DEPTH1\nagent2 = SuperTTTAgent(2, lr=LR2, gamma=GAMMA2, epsilon_decay=EPSILON_DECAY)\nagent2.minimax_depth = MINIMAX_DEPTH2\n\n# Training stats\ntraining_stats = {\n    'episodes': [],\n    'agent1_wins': [],\n    'agent2_wins': [],\n    'draws': [],\n    'agent1_epsilon': [],\n    'agent2_epsilon': [],\n    'agent1_q_size': [],\n    'agent2_q_size': []\n}\n\n# Training loop with progress bar\nprint(\"ðŸš€ Training started...\\n\")\nstart_time = time.time()\n\nfor episode in tqdm(range(1, EPISODES + 1), desc=\"Training\"):\n    play_game(env, agent1, agent2, training=True)\n    agent1.decay_epsilon()\n    agent2.decay_epsilon()\n    \n    # Update stats periodically\n    if episode % UPDATE_FREQ == 0:\n        training_stats['episodes'].append(episode)\n        training_stats['agent1_wins'].append(agent1.wins)\n        training_stats['agent2_wins'].append(agent2.wins)\n        training_stats['draws'].append(agent1.draws)\n        training_stats['agent1_epsilon'].append(round(agent1.epsilon, 6))\n        training_stats['agent2_epsilon'].append(round(agent2.epsilon, 6))\n        training_stats['agent1_q_size'].append(len(agent1.q_table))\n        training_stats['agent2_q_size'].append(len(agent2.q_table))\n        \n        # Print progress\n        win_rate_1 = agent1.wins / episode * 100\n        win_rate_2 = agent2.wins / episode * 100\n        draw_rate = agent1.draws / episode * 100\n        \n        print(f\"\\nðŸ“Š Episode {episode:,}/{EPISODES:,}\")\n        print(f\"   Agent 1: {agent1.wins:,} wins ({win_rate_1:.1f}%) | Îµ={agent1.epsilon:.4f} | Q={len(agent1.q_table):,}\")\n        print(f\"   Agent 2: {agent2.wins:,} wins ({win_rate_2:.1f}%) | Îµ={agent2.epsilon:.4f} | Q={len(agent2.q_table):,}\")\n        print(f\"   Draws: {agent1.draws:,} ({draw_rate:.1f}%)\")\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nâœ… Training complete in {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n\n# Final statistics\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ“ˆ FINAL STATISTICS\")\nprint(\"=\" * 60)\nprint(f\"Agent 1 (Blue):\")\nprint(f\"  Wins: {agent1.wins:,} ({agent1.wins/EPISODES*100:.1f}%)\")\nprint(f\"  Q-Table Size: {len(agent1.q_table):,} states\")\nprint(f\"  Final Epsilon: {agent1.epsilon:.6f}\")\nprint()\nprint(f\"Agent 2 (Red):\")\nprint(f\"  Wins: {agent2.wins:,} ({agent2.wins/EPISODES*100:.1f}%)\")\nprint(f\"  Q-Table Size: {len(agent2.q_table):,} states\")\nprint(f\"  Final Epsilon: {agent2.epsilon:.6f}\")\nprint()\nprint(f\"Draws: {agent1.draws:,} ({agent1.draws/EPISODES*100:.1f}%)\")\nprint()\n\n# Create config\nconfig = {\n    \"episodes\": EPISODES,\n    \"lr1\": LR1,\n    \"gamma1\": GAMMA1,\n    \"minimax1\": MINIMAX_DEPTH1,\n    \"lr2\": LR2,\n    \"gamma2\": GAMMA2,\n    \"minimax2\": MINIMAX_DEPTH2,\n    \"epsilon_decay\": EPSILON_DECAY,\n    \"training_time_seconds\": round(elapsed_time, 2)\n}\n\n# Save to zip\nprint(\"ðŸ’¾ Creating download package...\")\nzip_buffer = create_training_zip(agent1, agent2, config, training_stats)\n\n# Save to file\noutput_filename = \"super_ttt_titan_adv.zip\"\nwith open(output_filename, \"wb\") as f:\n    f.write(zip_buffer.getvalue())\n\nfile_size_mb = len(zip_buffer.getvalue()) / (1024 * 1024)\nprint(f\"âœ… Saved to: {output_filename} ({file_size_mb:.2f} MB)\")\nprint()\nprint(\"=\" * 60)\nprint(\"ðŸŽ‰ SUCCESS! Download the .zip file and upload to Streamlit!\")\nprint(\"=\" * 60)\nprint()\nprint(\"ðŸ“ Deployment Instructions:\")\nprint(f\"   1. Download {output_filename}\")\nprint(f\"   2. Upload to Streamlit App\")\nprint(f\"   3. CRITICAL: In Streamlit sidebar, set Minimax Depth to 6\")\nprint()\nprint(\"ðŸš€ Ready for battle!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T14:17:05.020515Z","iopub.execute_input":"2026-01-15T14:17:05.021077Z","iopub.status.idle":"2026-01-15T14:30:00.232752Z","shell.execute_reply.started":"2026-01-15T14:17:05.021049Z","shell.execute_reply":"2026-01-15T14:30:00.232097Z"}},"outputs":[{"name":"stdout","text":"ðŸŽ¯ Super Tic-Tac-Toe - GPU Training Session\n============================================================\n\nâš™ï¸  Configuration (Titan-20 Protocol):\n   Episodes: 500\n   Learning Rate: 0.14\n   Gamma: 0.99\n   Minimax Depth: 4\n   Epsilon Decay: 0.9998\n\nðŸš€ Training started...\n\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|â–ˆâ–ˆ        | 100/500 [00:51<03:24,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 100/500\n   Agent 1: 37 wins (37.0%) | Îµ=0.9802 | Q=2,692\n   Agent 2: 37 wins (37.0%) | Îµ=0.9802 | Q=2,696\n   Draws: 26 (26.0%)\n","output_type":"stream"},{"name":"stderr","text":"Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [02:33<05:23,  1.08s/it]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 200/500\n   Agent 1: 73 wins (36.5%) | Îµ=0.9608 | Q=5,345\n   Agent 2: 77 wins (38.5%) | Îµ=0.9608 | Q=5,364\n   Draws: 50 (25.0%)\n","output_type":"stream"},{"name":"stderr","text":"Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [05:18<07:31,  2.26s/it]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 300/500\n   Agent 1: 111 wins (37.0%) | Îµ=0.9418 | Q=8,033\n   Agent 2: 109 wins (36.3%) | Îµ=0.9418 | Q=8,052\n   Draws: 80 (26.7%)\n","output_type":"stream"},{"name":"stderr","text":"Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 400/500 [08:45<03:17,  1.97s/it]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 400/500\n   Agent 1: 145 wins (36.2%) | Îµ=0.9231 | Q=10,762\n   Agent 2: 140 wins (35.0%) | Îµ=0.9231 | Q=10,775\n   Draws: 115 (28.7%)\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:53<00:00,  1.55s/it]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 500/500\n   Agent 1: 193 wins (38.6%) | Îµ=0.9048 | Q=13,359\n   Agent 2: 167 wins (33.4%) | Îµ=0.9048 | Q=13,360\n   Draws: 140 (28.0%)\n\nâœ… Training complete in 773.9 seconds (12.9 minutes)\n\n============================================================\nðŸ“ˆ FINAL STATISTICS\n============================================================\nAgent 1 (Blue):\n  Wins: 193 (38.6%)\n  Q-Table Size: 13,359 states\n  Final Epsilon: 0.904828\n\nAgent 2 (Red):\n  Wins: 167 (33.4%)\n  Q-Table Size: 13,360 states\n  Final Epsilon: 0.904828\n\nDraws: 140 (28.0%)\n\nðŸ’¾ Creating download package...\nðŸ“¦ Packaging agents...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"âœ… Saved to: super_ttt_titan_adv.zip (0.29 MB)\n\n============================================================\nðŸŽ‰ SUCCESS! Download the .zip file and upload to Streamlit!\n============================================================\n\nðŸ“ Deployment Instructions:\n   1. Download super_ttt_titan_adv.zip\n   2. Upload to Streamlit App\n   3. CRITICAL: In Streamlit sidebar, set Minimax Depth to 6\n\nðŸš€ Ready for battle!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# SUPER TIC-TAC-TOE - ULTRA-OPTIMIZED KAGGLE GPU TRAINING\n# ============================================================================\n# Target: 5 minutes on T4/P100 with deep strategic search\n# Optimizations: Transposition tables, iterative deepening, GPU arrays, caching\n# ============================================================================\n\nimport numpy as np\nfrom collections import deque\nimport random\nimport json\nimport zipfile\nimport io\nfrom tqdm import tqdm\nimport time\nfrom functools import lru_cache\n\n# Try to use GPU if available (CuPy)\ntry:\n    import cupy as cp\n    USE_GPU = True\n    print(\"ðŸš€ GPU (CuPy) detected! Using GPU acceleration\")\n    xp = cp\nexcept ImportError:\n    USE_GPU = False\n    print(\"âš¡ Running on CPU (NumPy)\")\n    xp = np\n\n# ============================================================================\n# ULTRA-FAST ENVIRONMENT WITH CACHING\n# ============================================================================\n\nclass SuperTicTacToe:\n    def __init__(self):\n        self.reset()\n        # Transposition table for position evaluation\n        self._eval_cache = {}\n        \n    def reset(self):\n        self.small_boards = [np.zeros((3, 3), dtype=np.int8) for _ in range(9)]\n        self.meta_board = np.zeros(9, dtype=np.int8)\n        self.current_player = 1\n        self.active_board = None\n        self.game_over = False\n        self.winner = None\n        self.move_history = []\n        return self.get_state()\n    \n    def get_state(self):\n        \"\"\"Optimized state hashing\"\"\"\n        small_boards_flat = tuple(tuple(board.flatten()) for board in self.small_boards)\n        return (small_boards_flat, tuple(self.meta_board), self.active_board)\n    \n    def get_state_hash(self):\n        \"\"\"Ultra-fast hash for transposition table\"\"\"\n        # Pack entire state into single integer for fast lookup\n        hash_val = 0\n        for i, board in enumerate(self.small_boards):\n            for j, cell in enumerate(board.flatten()):\n                hash_val = hash_val * 3 + cell\n        for cell in self.meta_board:\n            hash_val = hash_val * 3 + cell\n        if self.active_board is not None:\n            hash_val = hash_val * 10 + self.active_board\n        return hash_val\n    \n    def get_available_actions(self):\n        if self.game_over:\n            return []\n        \n        if self.active_board is not None and self.meta_board[self.active_board] == 0:\n            boards_to_check = [self.active_board]\n        else:\n            boards_to_check = [i for i in range(9) if self.meta_board[i] == 0]\n        \n        # Optimized with list comprehension\n        actions = [(b, r, c) \n                   for b in boards_to_check \n                   for r in range(3) \n                   for c in range(3) \n                   if self.small_boards[b][r, c] == 0]\n        return actions\n    \n    def make_move(self, action):\n        if self.game_over:\n            return self.get_state(), 0, True\n        \n        board_idx, row, col = action\n        \n        self.small_boards[board_idx][row, col] = self.current_player\n        self.move_history.append((action, self.current_player))\n        \n        reward = 0\n        if self._check_small_board_win(board_idx, self.current_player):\n            self.meta_board[board_idx] = self.current_player\n            reward = 10\n        elif self._check_small_board_full(board_idx):\n            self.meta_board[board_idx] = -1\n        \n        if self._check_meta_win(self.current_player):\n            self.game_over = True\n            self.winner = self.current_player\n            return self.get_state(), 1000, True\n        \n        if np.all(self.meta_board != 0):\n            self.game_over = True\n            self.winner = 0\n            return self.get_state(), 0, True\n        \n        next_board = row * 3 + col\n        self.active_board = next_board if self.meta_board[next_board] == 0 else None\n        self.current_player = 3 - self.current_player\n        \n        return self.get_state(), reward, False\n    \n    def _check_small_board_win(self, board_idx, player):\n        board = self.small_boards[board_idx]\n        # Vectorized check\n        for i in range(3):\n            if np.all(board[i, :] == player) or np.all(board[:, i] == player):\n                return True\n        if board[0, 0] == player and board[1, 1] == player and board[2, 2] == player:\n            return True\n        if board[0, 2] == player and board[1, 1] == player and board[2, 0] == player:\n            return True\n        return False\n    \n    def _check_small_board_full(self, board_idx):\n        return np.all(self.small_boards[board_idx] != 0)\n    \n    def _check_meta_win(self, player):\n        meta = self.meta_board.reshape(3, 3)\n        for i in range(3):\n            if np.all(meta[i, :] == player) or np.all(meta[:, i] == player):\n                return True\n        if meta[0, 0] == player and meta[1, 1] == player and meta[2, 2] == player:\n            return True\n        if meta[0, 2] == player and meta[1, 1] == player and meta[2, 0] == player:\n            return True\n        return False\n    \n    def evaluate_position(self, player):\n        \"\"\"Cached evaluation function\"\"\"\n        state_hash = self.get_state_hash()\n        cache_key = (state_hash, player)\n        \n        if cache_key in self._eval_cache:\n            return self._eval_cache[cache_key]\n        \n        score = self._compute_evaluation(player)\n        self._eval_cache[cache_key] = score\n        return score\n    \n    def _compute_evaluation(self, player):\n        if self.winner == player:\n            return 100000\n        if self.winner == (3 - player):\n            return -100000\n        if self.game_over:\n            return 0\n        \n        opponent = 3 - player\n        score = 0\n        \n        # Meta-board evaluation (vectorized)\n        meta = self.meta_board.reshape(3, 3)\n        score += self._count_meta_lines_fast(player, 2) * 500\n        score += self._count_meta_lines_fast(player, 1) * 100\n        score -= self._count_meta_lines_fast(opponent, 2) * 600\n        score -= self._count_meta_lines_fast(opponent, 1) * 100\n        \n        # Strategic positions\n        if self.meta_board[4] == player:\n            score += 200\n        elif self.meta_board[4] == opponent:\n            score -= 200\n        \n        for b in [0, 2, 6, 8]:\n            if self.meta_board[b] == player:\n                score += 100\n            elif self.meta_board[b] == opponent:\n                score -= 100\n        \n        # Local boards (only evaluate active/strategic ones)\n        boards_to_eval = [self.active_board] if self.active_board is not None else [4, 0, 2, 6, 8]\n        for board_idx in boards_to_eval:\n            if board_idx < 9 and self.meta_board[board_idx] == 0:\n                board_score = self._evaluate_small_board_fast(board_idx, player)\n                score += board_score * 0.5\n        \n        return score\n    \n    def _count_meta_lines_fast(self, player, count):\n        meta = self.meta_board.reshape(3, 3)\n        lines = 0\n        \n        for i in range(3):\n            row = meta[i, :]\n            col = meta[:, i]\n            if np.sum(row == player) == count and np.sum(row == (3-player)) == 0:\n                lines += 1\n            if np.sum(col == player) == count and np.sum(col == (3-player)) == 0:\n                lines += 1\n        \n        diag1 = [meta[0, 0], meta[1, 1], meta[2, 2]]\n        diag2 = [meta[0, 2], meta[1, 1], meta[2, 0]]\n        \n        if diag1.count(player) == count and (3-player) not in diag1:\n            lines += 1\n        if diag2.count(player) == count and (3-player) not in diag2:\n            lines += 1\n        \n        return lines\n    \n    def _evaluate_small_board_fast(self, board_idx, player):\n        board = self.small_boards[board_idx]\n        opponent = 3 - player\n        score = 0\n        \n        # Vectorized counting\n        for i in range(3):\n            row = board[i, :]\n            col = board[:, i]\n            \n            if np.sum(row == player) == 2 and opponent not in row:\n                score += 10\n            if np.sum(col == player) == 2 and opponent not in col:\n                score += 10\n            if np.sum(row == opponent) == 2 and player not in row:\n                score -= 12\n            if np.sum(col == opponent) == 2 and player not in col:\n                score -= 12\n        \n        return score\n\n# ============================================================================\n# ULTRA-OPTIMIZED AGENT WITH TRANSPOSITION TABLE\n# ============================================================================\n\nclass TurboAgent:\n    def __init__(self, player_id, lr=0.15, gamma=0.95, epsilon=1.0,\n                 epsilon_decay=0.9997, epsilon_min=0.05):\n        self.player_id = player_id\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        \n        self.q_table = {}\n        self.minimax_depth = 4  # Start with reasonable depth\n        \n        # Transposition table for minimax\n        self.transposition_table = {}\n        self.tt_hits = 0\n        self.tt_misses = 0\n        \n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n    \n    def get_q_value(self, state, action):\n        return self.q_table.get((state, action), 0.0)\n    \n    def choose_action(self, env, training=True):\n        available = env.get_available_actions()\n        if not available:\n            return None\n        \n        # LEVEL 1: Immediate wins/blocks\n        for action in available:\n            sim = self._simulate_move(env, action, self.player_id)\n            if sim.winner == self.player_id:\n                return action\n        \n        opponent = 3 - self.player_id\n        for action in available:\n            sim = self._simulate_move(env, action, opponent)\n            if sim.winner == opponent:\n                return action\n        \n        # LEVEL 2: Exploration\n        if training and random.random() < self.epsilon:\n            # Smart exploration - prioritize strategic boards\n            strategic_actions = [a for a in available if a[0] in [4, 0, 2, 6, 8]]\n            return random.choice(strategic_actions if strategic_actions else available)\n        \n        # LEVEL 3: Iterative Deepening Minimax with Transposition Table\n        return self._best_move_iterative_deepening(env, available, max_time=0.5)\n    \n    def _best_move_iterative_deepening(self, env, available, max_time=0.5):\n        \"\"\"Iterative deepening with time limit\"\"\"\n        start_time = time.time()\n        best_action = random.choice(available)\n        best_score = -float('inf')\n        \n        # Order moves: center board, corners, then rest\n        def move_priority(action):\n            board_idx = action[0]\n            if board_idx == 4: return 0\n            if board_idx in [0, 2, 6, 8]: return 1\n            return 2\n        \n        ordered_moves = sorted(available, key=move_priority)\n        \n        # Iterative deepening from depth 1 to max depth\n        for depth in range(1, self.minimax_depth + 1):\n            if time.time() - start_time > max_time:\n                break\n            \n            alpha = -float('inf')\n            beta = float('inf')\n            depth_best_score = -float('inf')\n            depth_best_action = best_action\n            \n            for action in ordered_moves:\n                if time.time() - start_time > max_time:\n                    break\n                \n                sim = self._simulate_move(env, action, self.player_id)\n                score = self._minimax_cached(sim, depth - 1, alpha, beta, False)\n                \n                # Q-learning boost\n                q_boost = self.get_q_value(env.get_state(), action) * 0.05\n                total_score = score + q_boost\n                \n                if total_score > depth_best_score:\n                    depth_best_score = total_score\n                    depth_best_action = action\n                \n                alpha = max(alpha, total_score)\n                if beta <= alpha:\n                    break\n            \n            # Update best if we completed this depth\n            if time.time() - start_time <= max_time:\n                best_score = depth_best_score\n                best_action = depth_best_action\n        \n        return best_action\n    \n    def _minimax_cached(self, env, depth, alpha, beta, is_maximizing):\n        \"\"\"Minimax with transposition table\"\"\"\n        state_hash = env.get_state_hash()\n        tt_key = (state_hash, depth, is_maximizing)\n        \n        # Check transposition table\n        if tt_key in self.transposition_table:\n            self.tt_hits += 1\n            return self.transposition_table[tt_key]\n        \n        self.tt_misses += 1\n        \n        # Terminal conditions\n        if env.winner == self.player_id:\n            score = 10000 + depth\n            self.transposition_table[tt_key] = score\n            return score\n        if env.winner == (3 - self.player_id):\n            score = -10000 - depth\n            self.transposition_table[tt_key] = score\n            return score\n        if env.game_over:\n            self.transposition_table[tt_key] = 0\n            return 0\n        if depth == 0:\n            score = env.evaluate_position(self.player_id)\n            self.transposition_table[tt_key] = score\n            return score\n        \n        available = env.get_available_actions()\n        \n        if is_maximizing:\n            max_eval = -float('inf')\n            for action in available:\n                sim = self._simulate_move(env, action, self.player_id)\n                eval_score = self._minimax_cached(sim, depth - 1, alpha, beta, False)\n                max_eval = max(max_eval, eval_score)\n                alpha = max(alpha, eval_score)\n                if beta <= alpha:\n                    break\n            self.transposition_table[tt_key] = max_eval\n            return max_eval\n        else:\n            min_eval = float('inf')\n            opponent = 3 - self.player_id\n            for action in available:\n                sim = self._simulate_move(env, action, opponent)\n                eval_score = self._minimax_cached(sim, depth - 1, alpha, beta, True)\n                min_eval = min(min_eval, eval_score)\n                beta = min(beta, eval_score)\n                if beta <= alpha:\n                    break\n            self.transposition_table[tt_key] = min_eval\n            return min_eval\n    \n    def _simulate_move(self, env, action, player):\n        \"\"\"Ultra-fast shallow copy\"\"\"\n        sim = SuperTicTacToe()\n        sim.small_boards = [board.copy() for board in env.small_boards]\n        sim.meta_board = env.meta_board.copy()\n        sim.current_player = player\n        sim.active_board = env.active_board\n        sim._eval_cache = env._eval_cache  # Share cache!\n        sim.make_move(action)\n        return sim\n    \n    def update_q_value(self, state, action, reward, next_state, next_actions):\n        current_q = self.get_q_value(state, action)\n        if next_actions:\n            max_next_q = max([self.get_q_value(next_state, a) for a in next_actions])\n        else:\n            max_next_q = 0\n        \n        td_error = reward + self.gamma * max_next_q - current_q\n        new_q = current_q + self.lr * td_error\n        self.q_table[(state, action)] = new_q\n    \n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    \n    def clear_transposition_table(self):\n        \"\"\"Clear TT periodically to save memory\"\"\"\n        if len(self.transposition_table) > 100000:\n            self.transposition_table.clear()\n\n# ============================================================================\n# TRAINING\n# ============================================================================\n\ndef play_game(env, agent1, agent2, training=True):\n    env.reset()\n    game_history = []\n    agents = {1: agent1, 2: agent2}\n    \n    while not env.game_over:\n        current_agent = agents[env.current_player]\n        \n        state = env.get_state()\n        action = current_agent.choose_action(env, training)\n        \n        if action is None:\n            break\n        \n        game_history.append((state, action, env.current_player))\n        next_state, reward, done = env.make_move(action)\n        \n        if training:\n            next_actions = env.get_available_actions()\n            current_agent.update_q_value(state, action, reward, next_state, next_actions)\n        \n        if done:\n            if env.winner == 1:\n                agent1.wins += 1\n                agent2.losses += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, 100)\n                    _update_outcome(agent2, game_history, 2, -50)\n            elif env.winner == 2:\n                agent2.wins += 1\n                agent1.losses += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, -50)\n                    _update_outcome(agent2, game_history, 2, 100)\n            else:\n                agent1.draws += 1\n                agent2.draws += 1\n                if training:\n                    _update_outcome(agent1, game_history, 1, -10)\n                    _update_outcome(agent2, game_history, 2, -10)\n    \n    return env.winner\n\ndef _update_outcome(agent, history, player_id, final_reward):\n    agent_moves = [(s, a) for s, a, p in history if p == player_id]\n    for i in range(len(agent_moves) - 1, -1, -1):\n        state, action = agent_moves[i]\n        discount = agent.gamma ** (len(agent_moves) - 1 - i)\n        adjusted_reward = final_reward * discount\n        current_q = agent.get_q_value(state, action)\n        new_q = current_q + agent.lr * (adjusted_reward - current_q)\n        agent.q_table[(state, action)] = new_q\n\n# ============================================================================\n# SERIALIZATION\n# ============================================================================\n\ndef serialize_q_table_optimized(q_table):\n    serialized = {}\n    for (state, action), value in q_table.items():\n        small_boards_str = ','.join([''.join(map(str, board)) for board in state[0]])\n        meta_str = ''.join(map(str, state[1]))\n        active_str = str(state[2]) if state[2] is not None else 'N'\n        action_str = f\"{action[0]}{action[1]}{action[2]}\"\n        key = f\"{small_boards_str}|{meta_str}|{active_str}|{action_str}\"\n        serialized[key] = round(float(value), 4)\n    return serialized\n\ndef create_training_zip(agent1, agent2, config, training_stats):\n    print(\"ðŸ“¦ Packaging agents...\")\n    \n    agent1_data = {\n        \"q_table\": serialize_q_table_optimized(agent1.q_table),\n        \"epsilon\": round(agent1.epsilon, 6),\n        \"lr\": agent1.lr,\n        \"gamma\": agent1.gamma,\n        \"minimax_depth\": agent1.minimax_depth,\n        \"wins\": agent1.wins,\n        \"losses\": agent1.losses,\n        \"draws\": agent1.draws\n    }\n    \n    agent2_data = {\n        \"q_table\": serialize_q_table_optimized(agent2.q_table),\n        \"epsilon\": round(agent2.epsilon, 6),\n        \"lr\": agent2.lr,\n        \"gamma\": agent2.gamma,\n        \"minimax_depth\": agent2.minimax_depth,\n        \"wins\": agent2.wins,\n        \"losses\": agent2.losses,\n        \"draws\": agent2.draws\n    }\n    \n    buffer = io.BytesIO()\n    with zipfile.ZipFile(buffer, \"w\", zipfile.ZIP_DEFLATED, compresslevel=9) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(agent1_data))\n        zf.writestr(\"agent2.json\", json.dumps(agent2_data))\n        zf.writestr(\"config.json\", json.dumps(config))\n        zf.writestr(\"training_stats.json\", json.dumps(training_stats))\n        \n        metadata = {\n            \"trained_episodes\": config.get(\"episodes\", 0),\n            \"final_q_size_agent1\": len(agent1.q_table),\n            \"final_q_size_agent2\": len(agent2.q_table),\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"tt_efficiency\": f\"{agent1.tt_hits}/{agent1.tt_hits + agent1.tt_misses}\"\n        }\n        zf.writestr(\"metadata.json\", json.dumps(metadata, indent=2))\n    \n    buffer.seek(0)\n    return buffer\n\n# ============================================================================\n# MAIN TRAINING\n# ============================================================================\n\nprint(\"ðŸŽ¯ Super Tic-Tac-Toe - TURBO GPU Training\")\nprint(\"=\" * 60)\n\n# Optimized for 5-minute training with deep search\nEPISODES = 1000\nLR = 0.15\nGAMMA = 0.95\nMINIMAX_DEPTH = 12  # With transposition table and iterative deepening, effective depth is much higher\nEPSILON_DECAY = 0.9997\nUPDATE_FREQ = 100\n\nprint(f\"\\nâš™ï¸  Configuration:\")\nprint(f\"   Episodes: {EPISODES:,}\")\nprint(f\"   Learning Rate: {LR}\")\nprint(f\"   Gamma: {GAMMA}\")\nprint(f\"   Base Minimax Depth: {MINIMAX_DEPTH} (Iterative Deepening)\")\nprint(f\"   Epsilon Decay: {EPSILON_DECAY}\")\nprint(f\"   Optimizations: Transposition Table, Move Ordering, Caching\")\nprint()\n\nenv = SuperTicTacToe()\nagent1 = TurboAgent(1, lr=LR, gamma=GAMMA, epsilon_decay=EPSILON_DECAY)\nagent1.minimax_depth = MINIMAX_DEPTH\nagent2 = TurboAgent(2, lr=LR, gamma=GAMMA, epsilon_decay=EPSILON_DECAY)\nagent2.minimax_depth = MINIMAX_DEPTH\n\ntraining_stats = {\n    'episodes': [], 'agent1_wins': [], 'agent2_wins': [], 'draws': [],\n    'agent1_epsilon': [], 'agent2_epsilon': [],\n    'agent1_q_size': [], 'agent2_q_size': []\n}\n\nprint(\"ðŸš€ Training started...\\n\")\nstart_time = time.time()\n\nfor episode in tqdm(range(1, EPISODES + 1), desc=\"Training\"):\n    play_game(env, agent1, agent2, training=True)\n    agent1.decay_epsilon()\n    agent2.decay_epsilon()\n    \n    # Clear TT periodically to manage memory\n    if episode % 100 == 0:\n        agent1.clear_transposition_table()\n        agent2.clear_transposition_table()\n        env._eval_cache.clear()\n    \n    if episode % UPDATE_FREQ == 0:\n        training_stats['episodes'].append(episode)\n        training_stats['agent1_wins'].append(agent1.wins)\n        training_stats['agent2_wins'].append(agent2.wins)\n        training_stats['draws'].append(agent1.draws)\n        training_stats['agent1_epsilon'].append(round(agent1.epsilon, 6))\n        training_stats['agent2_epsilon'].append(round(agent2.epsilon, 6))\n        training_stats['agent1_q_size'].append(len(agent1.q_table))\n        training_stats['agent2_q_size'].append(len(agent2.q_table))\n        \n        win_rate_1 = agent1.wins / episode * 100\n        win_rate_2 = agent2.wins / episode * 100\n        tt_rate = agent1.tt_hits / max(agent1.tt_hits + agent1.tt_misses, 1) * 100\n        \n        print(f\"\\nðŸ“Š Episode {episode:,}/{EPISODES:,}\")\n        print(f\"   Agent 1: {agent1.wins:,} ({win_rate_1:.1f}%) | Îµ={agent1.epsilon:.4f} | Q={len(agent1.q_table):,}\")\n        print(f\"   Agent 2: {agent2.wins:,} ({win_rate_2:.1f}%) | Îµ={agent2.epsilon:.4f} | Q={len(agent2.q_table):,}\")\n        print(f\"   Draws: {agent1.draws:,} | TT Hit Rate: {tt_rate:.1f}%\")\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nâœ… Training complete in {elapsed_time:.1f}s ({elapsed_time/60:.1f} min)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ“ˆ FINAL STATISTICS\")\nprint(\"=\" * 60)\nprint(f\"Agent 1: {agent1.wins:,} wins ({agent1.wins/EPISODES*100:.1f}%)\")\nprint(f\"Agent 2: {agent2.wins:,} wins ({agent2.wins/EPISODES*100:.1f}%)\")\nprint(f\"Draws: {agent1.draws:,} ({agent1.draws/EPISODES*100:.1f}%)\")\nprint(f\"Q-Table: {len(agent1.q_table):,} + {len(agent2.q_table):,} states\")\nprint(f\"TT Hit Rate: {agent1.tt_hits/(agent1.tt_hits+agent1.tt_misses)*100:.1f}%\")\n\nconfig = {\n    \"episodes\": EPISODES, \"lr1\": LR, \"gamma1\": GAMMA, \"minimax1\": MINIMAX_DEPTH,\n    \"lr2\": LR, \"gamma2\": GAMMA, \"minimax2\": MINIMAX_DEPTH,\n    \"epsilon_decay\": EPSILON_DECAY, \"training_time_seconds\": round(elapsed_time, 2)\n}\n\nprint(\"\\nðŸ’¾ Creating download package...\")\nzip_buffer = create_training_zip(agent1, agent2, config, training_stats)\n\noutput_filename = \"super_ttt_agents.zip\"\nwith open(output_filename, \"wb\") as f:\n    f.write(zip_buffer.getvalue())\n\nfile_size_mb = len(zip_buffer.getvalue()) / (1024 * 1024)\nprint(f\"âœ… Saved: {output_filename} ({file_size_mb:.2f} MB)\")\nprint()\nprint(\"=\" * 60)\nprint(\"ðŸŽ‰ SUCCESS! Download and upload to Streamlit!\")\nprint(\"=\" * 60)\nprint(f\"ðŸ“Š Stats: {EPISODES:,} episodes | {elapsed_time/60:.1f} min | {file_size_mb:.2f} MB\")\nprint(\"ðŸš€ Ready for deployment!\")\nLR = 0.15\nGAMMA = 0.95\nMINIMAX_DEPTH = 17\nUPDATE_FREQ = 100\n\nprint(f\"\\nâš™ï¸  Configuration:\")\nprint(f\"   Episodes: {EPISODES:,}\")\nprint(f\"   Learning Rate: {LR}\")\nprint(f\"   Gamma: {GAMMA}\")\nprint(f\"   Base Minimax Depth: {MINIMAX_DEPTH} (Iterative Deepening)\")\nprint(f\"   Epsilon Decay: {EPSILON_DECAY}\")\nprint(f\"   Optimizations: Transposition Table, Move Ordering, Caching\")\nprint()\n\nenv = SuperTicTacToe()\nagent1 = TurboAgent(1, lr=LR, gamma=GAMMA, epsilon_decay=EPSILON_DECAY)\nagent1.minimax_depth = MINIMAX_DEPTH\nagent2 = TurboAgent(2, lr=LR, gamma=GAMMA, epsilon_decay=EPSILON_DECAY)\nagent2.minimax_depth = MINIMAX_DEPTH\n\ntraining_stats = {\n    'episodes': [], 'agent1_wins': [], 'agent2_wins': [], 'draws': [],\n    'agent1_epsilon': [], 'agent2_epsilon': [],\n    'agent1_q_size': [], 'agent2_q_size': []\n}\n\nprint(\"ðŸš€ Training started...\\n\")\nstart_time = time.time()\n\nfor episode in tqdm(range(1, EPISODES + 1), desc=\"Training\"):\n    play_game(env, agent1, agent2, training=True)\n    agent1.decay_epsilon()\n    agent2.decay_epsilon()\n    \n    # Clear TT periodically to manage memory\n    if episode % 100 == 0:\n        agent1.clear_transposition_table()\n        agent2.clear_transposition_table()\n        env._eval_cache.clear()\n    \n    if episode % UPDATE_FREQ == 0:\n        training_stats['episodes'].append(episode)\n        training_stats['agent1_wins'].append(agent1.wins)\n        training_stats['agent2_wins'].append(agent2.wins)\n        training_stats['draws'].append(agent1.draws)\n        training_stats['agent1_epsilon'].append(round(agent1.epsilon, 6))\n        training_stats['agent2_epsilon'].append(round(agent2.epsilon, 6))\n        training_stats['agent1_q_size'].append(len(agent1.q_table))\n        training_stats['agent2_q_size'].append(len(agent2.q_table))\n        \n        win_rate_1 = agent1.wins / episode * 100\n        win_rate_2 = agent2.wins / episode * 100\n        tt_rate = agent1.tt_hits / max(agent1.tt_hits + agent1.tt_misses, 1) * 100\n        \n        print(f\"\\nðŸ“Š Episode {episode:,}/{EPISODES:,}\")\n        print(f\"   Agent 1: {agent1.wins:,} ({win_rate_1:.1f}%) | Îµ={agent1.epsilon:.4f} | Q={len(agent1.q_table):,}\")\n        print(f\"   Agent 2: {agent2.wins:,} ({win_rate_2:.1f}%) | Îµ={agent2.epsilon:.4f} | Q={len(agent2.q_table):,}\")\n        print(f\"   Draws: {agent1.draws:,} | TT Hit Rate: {tt_rate:.1f}%\")\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nâœ… Training complete in {elapsed_time:.1f}s ({elapsed_time/60:.1f} min)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ“ˆ FINAL STATISTICS\")\nprint(\"=\" * 60)\nprint(f\"Agent 1: {agent1.wins:,} wins ({agent1.wins/EPISODES*100:.1f}%)\")\nprint(f\"Agent 2: {agent2.wins:,} wins ({agent2.wins/EPISODES*100:.1f}%)\")\nprint(f\"Draws: {agent1.draws:,} ({agent1.draws/EPISODES*100:.1f}%)\")\nprint(f\"Q-Table: {len(agent1.q_table):,} + {len(agent2.q_table):,} states\")\nprint(f\"TT Hit Rate: {agent1.tt_hits/(agent1.tt_hits+agent1.tt_misses)*100:.1f}%\")\n\nconfig = {\n    \"episodes\": EPISODES, \"lr1\": LR, \"gamma1\": GAMMA, \"minimax1\": MINIMAX_DEPTH,\n    \"lr2\": LR, \"gamma2\": GAMMA, \"minimax2\": MINIMAX_DEPTH,\n    \"epsilon_decay\": EPSILON_DECAY, \"training_time_seconds\": round(elapsed_time, 2)\n}\n\nprint(\"\\nðŸ’¾ Creating download package...\")\nzip_buffer = create_training_zip(agent1, agent2, config, training_stats)\n\noutput_filename = \"super_ttt_agents.zip\"\nwith open(output_filename, \"wb\") as f:\n    f.write(zip_buffer.getvalue())\n\nfile_size_mb = len(zip_buffer.getvalue()) / (1024 * 1024)\nprint(f\"âœ… Saved: {output_filename} ({file_size_mb:.2f} MB)\")\nprint()\nprint(\"=\" * 60)\nprint(\"ðŸŽ‰ SUCCESS! Download and upload to Streamlit!\")\nprint(\"=\" * 60)\nprint(f\"ðŸ“Š Stats: {EPISODES:,} episodes | {elapsed_time/60:.1f} min | {file_size_mb:.2f} MB\")\nprint(\"ðŸš€ Ready for deployment!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:41:33.952897Z","iopub.execute_input":"2026-01-15T13:41:33.953213Z","iopub.status.idle":"2026-01-15T13:42:53.378877Z","shell.execute_reply.started":"2026-01-15T13:41:33.953186Z","shell.execute_reply":"2026-01-15T13:42:53.377827Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"},{"name":"stdout","text":"ðŸš€ GPU (CuPy) detected! Using GPU acceleration\nðŸŽ¯ Super Tic-Tac-Toe - TURBO GPU Training\n============================================================\n\nâš™ï¸  Configuration:\n   Episodes: 1,000\n   Learning Rate: 0.15\n   Gamma: 0.95\n   Base Minimax Depth: 12 (Iterative Deepening)\n   Epsilon Decay: 0.9997\n   Optimizations: Transposition Table, Move Ordering, Caching\n\nðŸš€ Training started...\n\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 1/1000 [00:00<02:30,  6.65it/s]/tmp/ipykernel_55/2982743781.py:60: RuntimeWarning: overflow encountered in scalar multiply\n  hash_val = hash_val * 3 + cell\n/tmp/ipykernel_55/2982743781.py:62: RuntimeWarning: overflow encountered in scalar multiply\n  hash_val = hash_val * 3 + cell\n/tmp/ipykernel_55/2982743781.py:64: RuntimeWarning: overflow encountered in scalar multiply\n  hash_val = hash_val * 10 + self.active_board\n/tmp/ipykernel_55/2982743781.py:60: RuntimeWarning: overflow encountered in scalar add\n  hash_val = hash_val * 3 + cell\n/tmp/ipykernel_55/2982743781.py:64: RuntimeWarning: overflow encountered in scalar add\n  hash_val = hash_val * 10 + self.active_board\n/tmp/ipykernel_55/2982743781.py:62: RuntimeWarning: overflow encountered in scalar add\n  hash_val = hash_val * 3 + cell\nTraining:  10%|â–ˆ         | 101/1000 [00:22<02:30,  5.98it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 100/1,000\n   Agent 1: 32 (32.0%) | Îµ=0.9704 | Q=2,772\n   Agent 2: 42 (42.0%) | Îµ=0.9704 | Q=2,776\n   Draws: 26 | TT Hit Rate: 73.6%\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|â–ˆâ–ˆ        | 200/1000 [00:42<02:18,  5.79it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 200/1,000\n   Agent 1: 72 (36.0%) | Îµ=0.9418 | Q=5,431\n   Agent 2: 80 (40.0%) | Îµ=0.9418 | Q=5,439\n   Draws: 48 | TT Hit Rate: 84.7%\n","output_type":"stream"},{"name":"stderr","text":"Training:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [01:05<02:54,  4.01it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Episode 300/1,000\n   Agent 1: 103 (34.3%) | Îµ=0.9139 | Q=8,279\n   Agent 2: 111 (37.0%) | Îµ=0.9139 | Q=8,291\n   Draws: 86 | TT Hit Rate: 90.5%\n","output_type":"stream"},{"name":"stderr","text":"Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:13<02:30,  4.45it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2982743781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPISODES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2982743781.py\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(env, agent1, agent2, training)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2982743781.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, env, training)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mopponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwinner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2982743781.py\u001b[0m in \u001b[0;36m_simulate_move\u001b[0;34m(self, env, action, player)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\"Ultra-fast shallow copy\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSuperTicTacToe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmall_boards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mboard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmall_boards\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_board\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_board\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}