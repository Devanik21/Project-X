{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n#  THE IMMORTALITY PROTOCOL: HRF TITAN-26 (KAGGLE P100 EDITION)\n#  SYSTEM: NVIDIA TESLA P100 (16GB HBM2) | RAM: 30GB\n#  TARGET: MAE < 1.0 YEAR\n# ==============================================================================\n\nimport sys\nimport subprocess\nimport gc\nimport os\nimport numpy as np\nimport pandas as pd\n\n# --- 1. RAPIDS & ENVIRONMENT CHECK ---\nprint(\"‚ö° SYSTEM DIAGNOSTICS:\")\ntry:\n    import cuml\n    import cupy as cp\n    import cudf\n    gpu_name = subprocess.check_output([\"nvidia-smi\", \"-L\"]).decode(\"utf-8\").strip()\n    print(f\"   ‚úÖ GPU DETECTED: {gpu_name}\")\n    print(f\"   ‚úÖ RAPIDS VERSION: {cuml.__version__}\")\n    \n    # Check if we are truly on P100\n    if \"P100\" in gpu_name:\n        print(\"   üöÄ PERFORMANCE MODE: P100 HBM2 BANDWIDTH ACTIVE.\")\n    else:\n        print(\"   ‚ö†Ô∏è WARNING: You are running on T4. Switch to P100 in Settings for 2x speed.\")\nexcept ImportError:\n    print(\"   ‚ùå CRITICAL: RAPIDS not found. Ensure Accelerator is set to GPU P100.\")\n\n# --- 2. INSTALL BIOLOGY STACK ---\n# Kaggle has RAPIDS, but needs GEOparse\nprint(\"\\nüì¶ INSTALLING BIO-INFORMATICS LAYER...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"GEOparse\", \"fastparquet\"])\n\nimport GEOparse\n\n# --- 3. MEMORY-SAFE DATA LOADER (THE \"PIVOT KILLER\") ---\ndef load_hannum_optimized():\n    \"\"\"\n    Loads GSE40279 with RAM Safety Protocols.\n    Drops the massive GEO object immediately after extraction to prevent OOM.\n    \"\"\"\n    print(\"\\nüß¨ DOWNLOADING GSE40279 (HANNUM DATASET)...\")\n    # Download to local directory\n    try:\n        gse = GEOparse.get_GEO(geo=\"GSE40279\", destdir=\"./\", silent=True)\n    except Exception as e:\n        print(f\"   ‚ùå DOWNLOAD ERROR: {e}\")\n        return None, None\n\n    print(\"   ‚úÖ Download Complete. Extracting Metadata...\")\n    \n    # 1. Extract Targets (Age)\n    meta = gse.phenotype_data\n    # Search for age column safely\n    age_col = next((c for c in meta.columns if 'age' in c.lower()), None)\n    if age_col:\n        y = meta[age_col].astype('float32').values # float32 saves 50% RAM\n        print(f\"   -> Target Extracted: {len(y)} Samples (Age {y.min()}-{y.max()})\")\n    else:\n        raise ValueError(\"Age column not found!\")\n\n    # 2. Extract Data & PURGE RAM\n    print(\"   -> EXTRACTING METHYLATION MATRIX (May spike RAM to 12GB+)...\")\n    # We extract the table first\n    df_temp = gse.pivot_samples('VALUE')\n    \n    # 3. CRITICAL: KILL THE GSE OBJECT\n    print(\"   -> üßπ PURGING RAW FILES FROM RAM...\")\n    del gse\n    gc.collect() # Force Python to release memory instantly\n    \n    # 4. Transpose & Optimize\n    print(\"   -> Transposing & Casting to Float32...\")\n    X = df_temp.T.astype('float32') # Crucial for P100 speed\n    \n    # Cleanup temp dataframe\n    del df_temp\n    gc.collect()\n    \n    print(f\"\\nüìä FINAL DATA SHAPE: {X.shape}\")\n    print(f\"   [RAM USAGE OPTIMIZED] - Ready for Titan-26\")\n    \n    return X, y\n\nif __name__ == \"__main__\":\n    # Execute the loader\n    X, y = load_hannum_optimized()\n    \n    # Preview\n    print(\"\\nüîç SAMPLE BETA VALUES (CpG Sites):\")\n    print(X.iloc[:3, :5])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-14T06:57:19.314744Z","iopub.execute_input":"2026-01-14T06:57:19.315029Z","execution_failed":"2026-01-14T07:09:34.730Z"}},"outputs":[{"name":"stdout","text":"‚ö° SYSTEM DIAGNOSTICS:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ GPU DETECTED: GPU 0: Tesla T4 (UUID: GPU-5225ca79-692c-9b7f-a438-75a28ab48138)\nGPU 1: Tesla T4 (UUID: GPU-e609f12d-2655-8b3a-ea38-6ebf6dabb3ed)\n   ‚úÖ RAPIDS VERSION: 25.06.00\n   ‚ö†Ô∏è WARNING: You are running on T4. Switch to P100 in Settings for 2x speed.\n\nüì¶ INSTALLING BIO-INFORMATICS LAYER...\nCollecting GEOparse\n  Downloading GEOparse-2.0.4-py3-none-any.whl.metadata (6.5 kB)\nCollecting fastparquet\n  Downloading fastparquet-2025.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.0.2)\nRequirement already satisfied: pandas>=0.17 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.2.2)\nRequirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.32.5)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (4.67.1)\nRequirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.11.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2025.10.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastparquet) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2025.11.12)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.17->GEOparse) (1.17.0)\nDownloading GEOparse-2.0.4-py3-none-any.whl (29 kB)\nDownloading fastparquet-2025.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.8/1.8 MB 24.2 MB/s eta 0:00:00\nInstalling collected packages: GEOparse, fastparquet\nSuccessfully installed GEOparse-2.0.4 fastparquet-2025.12.0\n\nüß¨ DOWNLOADING GSE40279 (HANNUM DATASET)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/GEOparse/GEOparse.py:401: DtypeWarning: Columns (11,14,15,36) have mixed types. Specify dtype option on import or set low_memory=False.\n  return read_csv(StringIO(data), index_col=None, sep=\"\\t\")\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Download Complete. Extracting Metadata...\n   -> Target Extracted: 656 Samples (Age 19.0-101.0)\n   -> EXTRACTING METHYLATION MATRIX (May spike RAM to 12GB+)...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n#  THE IMMORTALITY PROTOCOL: TITAN-26 [CORE-5K INITIALIZATION]\n#  TARGET: MAE < 1.0 YEAR | R > 0.99 (DETERMINISTIC WAVE DECODING)\n#  HARDWARE: NVIDIA T4 (KAGGLE/COLAB 2026)\n# ==============================================================================\n\nimport sys, os, subprocess, gc\nprint(\"‚ö° INITIATING TITAN-26 HYPER-SPEED ENVIRONMENT...\")\n\n# --- 1. NVIDIA RAPIDS AUTO-INSTALL (T4 OPTIMIZED) ---\ntry:\n    import cuml, cudf, cupy as cp\n    print(\"   ‚úÖ RAPIDS ENGINE DETECTED.\")\nexcept ImportError:\n    print(\"   ‚ö†Ô∏è RAPIDS ENGINE MISSING. INSTALLING (90s)...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cudf-cu12\", \"cuml-cu12\", \n                           \"--extra-index-url=https://pypi.nvidia.com\", \"--no-cache-dir\", \"-q\"])\n    import cuml, cudf, cupy as cp\n    print(\"   ‚úÖ INSTALLATION SUCCESSFUL.\")\n\n# --- 2. GPU MEMORY SWAP PROTECTION ---\n# This ensures that even if we hit VRAM limits, the P100/T4 uses managed memory.\ncp.cuda.set_allocator(cp.cuda.MemoryPool(cp.cuda.malloc_managed).malloc)\n\n# --- 3. IMPORT SCIENTIFIC STACK ---\nfrom cuml.linear_model import ElasticNet, Ridge\nfrom cuml.metrics import mean_absolute_error, r2_score\nfrom cuml.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\n# --- 4. CONFIGURATION FOR NOBEL-TIER ACCURACY ---\n# Using the Top 5k features eliminates 'epigenetic noise' and prevents RAM crashes.\nPROTOCAL_CONFIG = {\n    \"target_features\": 5000,\n    \"precision\": \"float32\",\n    \"gpu_managed\": True,\n    \"seed\": 26  # Harmonic Constant\n}\n\nprint(f\"\\nüåü SYSTEM READY:\")\nprint(f\"   [ACCELERATOR]: {subprocess.check_output(['nvidia-smi', '-L']).decode('utf-8').strip()}\")\nprint(f\"   [PROTOCOL]: CORE-5K (Deterministic Filter)\")\nprint(f\"   [MEMORY]: MANAGED POOL ACTIVE\")\nprint(f\"\\n‚úÖ PROCEED TO CELL 2: LOAD SLIM PARQUET (GSE40279_Core_5k.parquet)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:56:23.247950Z","iopub.execute_input":"2026-01-14T07:56:23.248691Z","iopub.status.idle":"2026-01-14T07:56:23.285717Z","shell.execute_reply.started":"2026-01-14T07:56:23.248660Z","shell.execute_reply":"2026-01-14T07:56:23.284918Z"}},"outputs":[{"name":"stdout","text":"‚ö° INITIATING TITAN-26 HYPER-SPEED ENVIRONMENT...\n   ‚úÖ RAPIDS ENGINE DETECTED.\n\nüåü SYSTEM READY:\n   [ACCELERATOR]: GPU 0: Tesla T4 (UUID: GPU-bf0ddeb4-4fd8-9c5b-c358-f22fb5428750)\nGPU 1: Tesla T4 (UUID: GPU-25e3ef52-22f2-ef08-0ff8-940338480edc)\n   [PROTOCOL]: CORE-5K (Deterministic Filter)\n   [MEMORY]: MANAGED POOL ACTIVE\n\n‚úÖ PROCEED TO CELL 2: LOAD SLIM PARQUET (GSE40279_Core_5k.parquet)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n#  THE IMMORTALITY PROTOCOL: BRAIN CLOCK FIX (GSE74193)\n#  STATUS: DEBUGGING & REPAIRING\n# ==============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport requests\nimport gzip\nimport re\nimport os\nimport gc\nimport cupy as cp\n\ndef load_brain_clock_robust(target_k=3000):\n    print(\"‚ö° CONNECTING TO GSE74193 (FIXED MODE)...\")\n    \n    # 1. DOWNLOAD\n    url = \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE74nnn/GSE74193/matrix/GSE74193_series_matrix.txt.gz\"\n    local_filename = \"GSE74193_Brain.txt.gz\"\n    \n    if not os.path.exists(local_filename):\n        print(\"   -> Downloading...\")\n        r = requests.get(url, stream=True)\n        with open(local_filename, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n    else:\n        print(\"   -> Found Local File.\")\n\n    # 2. EXTRACT AGES (Handling Quotes)\n    print(\"   -> Parsing Header for Brain Ages...\")\n    y_ages = []\n    \n    with gzip.open(local_filename, 'rt', encoding='latin-1') as f:\n        for line in f:\n            # Look for line with \"age\" or \"Age\" inside !Sample_characteristics\n            if \"!Sample_characteristics_ch1\" in line and (\"age\" in line.lower() or \"Age\" in line):\n                parts = line.strip().split('\\t')[1:]\n                for p in parts:\n                    # Remove quotes first: \"Age: 45\" -> Age: 45\n                    clean_p = p.replace('\"', '').strip()\n                    # Find number: matches 45, 45.0, etc.\n                    match = re.search(r\"(\\d+\\.?\\d*)\", clean_p)\n                    if match:\n                        y_ages.append(float(match.group(1)))\n                    else:\n                        y_ages.append(np.nan)\n                \n                # If we found enough numbers, we assume this is the age line and stop\n                if len(y_ages) > 10: \n                    break \n                else:\n                    y_ages = [] # Reset if this was just a junk line\n\n    y = np.array(y_ages, dtype=np.float32)\n    \n    # Fill NaNs with mean age to prevent crash\n    if np.isnan(y).any():\n        print(f\"   ‚ö†Ô∏è Found {np.isnan(y).sum()} missing ages. Filling with mean.\")\n        y = np.nan_to_num(y, nan=np.nanmean(y))\n        \n    print(f\"   -> ‚úÖ Target Acquired: {len(y)} Brain Samples. (Age {y.min():.1f} - {y.max():.1f})\")\n\n    # 3. LOAD MATRIX (Manual Skip)\n    print(\"   -> locating Table Start...\")\n    \n    # Find the line number where the data starts\n    skip_count = 0\n    with gzip.open(local_filename, 'rt', encoding='latin-1') as f:\n        for i, line in enumerate(f):\n            if \"!series_matrix_table_begin\" in line:\n                skip_count = i + 1 # Data starts after this line\n                break\n    \n    print(f\"   -> Loading Methylation Matrix (Skipping {skip_count} lines)...\")\n    # Read CSV skipping the metadata manually. We do NOT use comment='!' here to avoid skipping data.\n    X_raw = pd.read_csv(local_filename, sep='\\t', index_col=0, header=0, skiprows=skip_count, compression='gzip', engine='c')\n    \n    # Drop the LAST row if it is \"!series_matrix_table_end\"\n    if X_raw.index[-1].startswith(\"!\"):\n        X_raw = X_raw.iloc[:-1]\n\n    print(\"   -> Transposing...\")\n    X = X_raw.T.values.astype('float32')\n    \n    del X_raw\n    gc.collect()\n\n    # 4. DEATH RAY (GPU Filter)\n    print(f\"   -> üéØ FIRING DEATH RAY SNIPER (Filtering {X.shape[1]} sites)...\")\n    \n    X = np.nan_to_num(X, nan=0.5)\n    X_gpu = cp.asarray(X)\n    y_gpu = cp.asarray(y)\n    \n    # Correlation\n    X_mean = cp.mean(X_gpu, axis=0)\n    y_mean = cp.mean(y_gpu)\n    numerator = cp.sum((X_gpu - X_mean) * (y_gpu - y_mean)[:, None], axis=0)\n    denominator = cp.sqrt(cp.sum((X_gpu - X_mean)**2, axis=0) * cp.sum((y_gpu - y_mean)**2))\n    correlations = cp.abs(numerator / (denominator + 1e-9))\n    \n    top_indices = cp.argsort(correlations)[-target_k:]\n    top_indices = cp.sort(top_indices)\n    \n    X_core = X_gpu[:, top_indices]\n    \n    print(f\"   -> ‚úÖ READY. Shape: {X_core.shape}\")\n    return X_core, y_gpu\n\nif __name__ == \"__main__\":\n    X, y = load_brain_clock_robust(target_k=3000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:28:30.040609Z","iopub.execute_input":"2026-01-14T08:28:30.041294Z","iopub.status.idle":"2026-01-14T08:28:30.189103Z","shell.execute_reply.started":"2026-01-14T08:28:30.041264Z","shell.execute_reply":"2026-01-14T08:28:30.188496Z"}},"outputs":[{"name":"stdout","text":"‚ö° CONNECTING TO GSE74193 (FIXED MODE)...\n   -> Found Local File.\n   -> Parsing Header for Brain Ages...\n   -> ‚úÖ Target Acquired: 675 Brain Samples. (Age 0.0 - 97.0)\n   -> locating Table Start...\n   -> Loading Methylation Matrix (Skipping 91 lines)...\n   -> Transposing...\n   -> üéØ FIRING DEATH RAY SNIPER (Filtering 0 sites)...\n   -> ‚úÖ READY. Shape: (675, 0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install GEOparse\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:35:44.965319Z","iopub.execute_input":"2026-01-14T08:35:44.965612Z","iopub.status.idle":"2026-01-14T08:35:49.267173Z","shell.execute_reply.started":"2026-01-14T08:35:44.965587Z","shell.execute_reply":"2026-01-14T08:35:49.266271Z"}},"outputs":[{"name":"stdout","text":"Collecting GEOparse\n  Downloading GEOparse-2.0.4-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.0.2)\nRequirement already satisfied: pandas>=0.17 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.2.2)\nRequirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (2.32.5)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from GEOparse) (4.67.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17->GEOparse) (2025.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->GEOparse) (2025.11.12)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.17->GEOparse) (1.17.0)\nDownloading GEOparse-2.0.4-py3-none-any.whl (29 kB)\nInstalling collected packages: GEOparse\nSuccessfully installed GEOparse-2.0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ==============================================================================\n#  THE IMMORTALITY PROTOCOL: BRUTE FORCE ALIGNMENT\n#  STRATEGY: Manual ID-to-Age Mapping (Bypasses all library errors)\n#  HARDWARE: T4 GPU\n# ==============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport requests\nimport gzip\nimport os\nimport gc\nimport re\nimport cupy as cp\nfrom cuml.neighbors import NearestNeighbors as cuNN\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.model_selection import train_test_split\n\n# ------------------------------------------------------------------------------\n# 1. THE BRUTE FORCE LOADER (MANUAL MAPPING)\n# ------------------------------------------------------------------------------\ndef load_brain_data_brute_force(target_k=3000):\n    print(\"‚ö° STEP 1: DOWNLOADING & MAPPING DATA MANUALLY...\")\n    \n    url = \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE74nnn/GSE74193/matrix/GSE74193_series_matrix.txt.gz\"\n    local_filename = \"GSE74193_Brain.txt.gz\"\n    \n    if not os.path.exists(local_filename):\n        r = requests.get(url, stream=True)\n        with open(local_filename, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n    # --- PHASE A: BUILD THE MAP (Header Scan) ---\n    print(\"   -> Scanning Header for Sample IDs and Ages...\")\n    sample_ids = []\n    ages = []\n    \n    with gzip.open(local_filename, 'rt', encoding='latin-1') as f:\n        for line in f:\n            # 1. Capture Sample IDs (GSMxxxx)\n            if \"!Sample_geo_accession\" in line:\n                parts = line.strip().replace('\"', '').split('\\t')[1:]\n                sample_ids = parts\n                \n            # 2. Capture Ages\n            if \"!Sample_characteristics_ch1\" in line and (\"age\" in line.lower() or \"Age\" in line):\n                parts = line.strip().replace('\"', '').split('\\t')[1:]\n                # Extract numbers from strings like \"Age: 45\"\n                line_ages = []\n                for p in parts:\n                    match = re.search(r\"(\\d+\\.?\\d*)\", p)\n                    if match: line_ages.append(float(match.group(1)))\n                    else: line_ages.append(np.nan)\n                \n                # Only keep this line if it looks like the Age line\n                if len(line_ages) > 0 and np.nanmean(line_ages) > 0:\n                    ages = line_ages\n\n            if \"!series_matrix_table_begin\" in line:\n                break\n    \n    # Create the Master Map\n    if len(sample_ids) != len(ages):\n        print(f\"   ‚ö†Ô∏è Mismatch: {len(sample_ids)} IDs vs {len(ages)} Ages. Truncating to minimum.\")\n        min_len = min(len(sample_ids), len(ages))\n        sample_ids = sample_ids[:min_len]\n        ages = ages[:min_len]\n\n    id_to_age = dict(zip(sample_ids, ages))\n    print(f\"   -> Map Created: {len(id_to_age)} Samples Mapped.\")\n\n    # --- PHASE B: LOAD MATRIX & ALIGN ---\n    print(\"   -> Loading Data Table (Skipping Metadata)...\")\n    \n    # Find start line\n    skip_rows = 0\n    with gzip.open(local_filename, 'rt', encoding='latin-1') as f:\n        for i, line in enumerate(f):\n            if \"!series_matrix_table_begin\" in line:\n                skip_rows = i + 1\n                break\n                \n    # Load Data\n    df = pd.read_csv(local_filename, sep='\\t', index_col=0, header=0, \n                     skiprows=skip_rows, compression='gzip', engine='c')\n    \n    # Drop last row if garbage\n    if isinstance(df.index[-1], str) and df.index[-1].startswith(\"!\"):\n        df = df.iloc[:-1]\n        \n    print(f\"   -> Raw Matrix Shape: {df.shape}\")\n\n    # --- PHASE C: ALIGNMENT ---\n    print(\"   -> Aligning Data columns to Age Map...\")\n    # Only keep columns that are in our map\n    valid_cols = [c for c in df.columns if c in id_to_age]\n    df = df[valid_cols]\n    \n    # Create Target Vector y based on column order\n    y = np.array([id_to_age[c] for c in df.columns], dtype=np.float32)\n    \n    # Fill NaNs in Age (Fetal data usually -1 or Nan)\n    y = np.nan_to_num(y, nan=0.0)\n    \n    # Transpose to (Samples x Features)\n    X = df.T.values.astype('float32')\n    \n    # Clean RAM\n    del df\n    gc.collect()\n\n    # --- PHASE D: DEATH RAY (GPU) ---\n    print(f\"   -> üéØ FIRING DEATH RAY on {X.shape} Matrix...\")\n    \n    # Fill Data NaNs\n    X = np.nan_to_num(X, nan=0.5)\n    \n    X_gpu = cp.asarray(X)\n    y_gpu = cp.asarray(y)\n    \n    # Variance Filter (Faster/Safer than Correlation for first pass)\n    # We select features that actually CHANGE across samples\n    print(\"   -> Selecting High-Variance Features...\")\n    variances = cp.var(X_gpu, axis=0)\n    top_indices = cp.argsort(variances)[-target_k:]\n    top_indices = cp.sort(top_indices)\n    \n    X_core = cp.asnumpy(X_gpu[:, top_indices])\n    \n    print(f\"   -> ‚úÖ SUCCESS. Ready for Titan. Shape: {X_core.shape}\")\n    return X_core, y\n\n# ------------------------------------------------------------------------------\n# 2. THE TITAN-26 MODEL (GPU KERNELS)\n# ------------------------------------------------------------------------------\nclass HarmonicResonanceRegressor_v15(BaseEstimator, RegressorMixin):\n    def __init__(self):\n        self.base_freq = 10.0\n        self.gamma = 0.5\n        self.n_neighbors = 5\n        self.scaler_ = RobustScaler()\n\n    def fit(self, X, y):\n        # CPU Pre-processing\n        X_scaled = self.scaler_.fit_transform(X)\n        X_clip = np.clip(X_scaled, 0, 1)\n        diffs = np.diff(X_clip, axis=1)\n        coherence = np.var(X_clip, axis=1).reshape(-1, 1)\n        self.X_train_ = np.hstack([X_clip, diffs, coherence])\n        self.y_train_ = y\n        return self\n\n    def predict(self, X):\n        X_scaled = self.scaler_.transform(X)\n        X_clip = np.clip(X_scaled, 0, 1)\n        diffs = np.diff(X_clip, axis=1)\n        coherence = np.var(X_clip, axis=1).reshape(-1, 1)\n        X_holo = np.hstack([X_clip, diffs, coherence])\n        \n        # Handshake: CPU -> GPU\n        return self._predict_gpu(self.X_train_, self.y_train_, X_holo)\n\n    def _predict_gpu(self, X_tr, y_tr, X_q):\n        # Move to T4\n        X_tr_g = cp.asarray(X_tr)\n        y_tr_g = cp.asarray(y_tr)\n        X_q_g = cp.asarray(X_q)\n        \n        knn = cuNN(n_neighbors=self.n_neighbors)\n        knn.fit(X_tr_g)\n        dists, indices = knn.kneighbors(X_q_g)\n        \n        # Harmonic Kernel\n        w = cp.exp(-self.gamma * dists**2.5) * (1.0 + cp.cos(self.base_freq * dists))\n        \n        neighbors_y = y_tr_g[indices]\n        preds = cp.sum(w * neighbors_y, axis=1) / (cp.sum(w, axis=1) + 1e-9)\n        return cp.asnumpy(preds)\n\n# ------------------------------------------------------------------------------\n# 3. EXECUTION\n# ------------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    # Load\n    X, y = load_brain_data_brute_force(target_k=3000)\n    \n    # Split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n    \n    print(\"\\n‚öîÔ∏è  TRAINING TITAN-26 FOREST...\")\n    model = BaggingRegressor(\n        estimator=HarmonicResonanceRegressor_v15(),\n        n_estimators=40, # High precision\n        max_samples=0.7,\n        n_jobs=1,\n        random_state=42\n    )\n    \n    model.fit(X_train, y_train)\n    \n    print(\"üîÆ PREDICTING...\")\n    preds = model.predict(X_test)\n    \n    mae = mean_absolute_error(y_test, preds)\n    r2 = r2_score(y_test, preds)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(f\"FINAL RESULT (BRAIN CLOCK)\")\n    print(\"=\"*50)\n    print(f\"MAE (Error): {mae:.4f} Years\")\n    print(f\"R2 Score:    {r2:.4f}\")\n    print(\"=\"*50)\n    \n    if mae < 2.0:\n        print(\"üèÜ STATUS: NOBEL-TIER BREAKTHROUGH.\")\n    else:\n        print(\"‚úÖ STATUS: SUCCESSFUL RUN.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:37:55.755141Z","iopub.execute_input":"2026-01-14T08:37:55.755961Z","iopub.status.idle":"2026-01-14T08:37:56.014538Z","shell.execute_reply.started":"2026-01-14T08:37:55.755923Z","shell.execute_reply":"2026-01-14T08:37:56.013616Z"}},"outputs":[{"name":"stdout","text":"‚ö° STEP 1: DOWNLOADING & MAPPING DATA MANUALLY...\n   -> Scanning Header for Sample IDs and Ages...\n   -> Map Created: 675 Samples Mapped.\n   -> Loading Data Table (Skipping Metadata)...\n   -> Raw Matrix Shape: (0, 675)\n   -> Aligning Data columns to Age Map...\n   -> üéØ FIRING DEATH RAY on (675, 0) Matrix...\n   -> Selecting High-Variance Features...\n   -> ‚úÖ SUCCESS. Ready for Titan. Shape: (675, 0)\n\n‚öîÔ∏è  TRAINING TITAN-26 FOREST...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/4169924067.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîÆ PREDICTING...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;31m# Convert data (X is required to be 2d and indexable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0;34m\"Found array with %d feature(s) (shape=%s) while\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(573, 0)) while a minimum of 1 is required by BaggingRegressor."],"ename":"ValueError","evalue":"Found array with 0 feature(s) (shape=(573, 0)) while a minimum of 1 is required by BaggingRegressor.","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}