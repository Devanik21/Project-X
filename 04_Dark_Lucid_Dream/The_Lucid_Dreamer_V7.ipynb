{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Version 1"
      ],
      "metadata": {
        "id": "BNSfvMV_GnXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7886Jp2iaeKM",
        "outputId": "bbc97bf3-3594-4104-973e-dbfed2b5e765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° HARDWARE: Tesla T4\n",
            "\n",
            "üöÄ STARTING TURBO LIQUID DARK DUEL üöÄ\n",
            "Ep 1: Dark -4.6 (üíé0) | Std -5.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3275301907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mrun_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3275301907.py\u001b[0m in \u001b[0;36mrun_benchmark\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHYPER_PARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MAX_STEPS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplan_and_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3275301907.py\u001b[0m in \u001b[0;36mplan_and_act\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mhorizon\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0msim_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mcurr_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdream_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3275301907.py\u001b[0m in \u001b[0;36mdream_step\u001b[0;34m(self, h, action)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mact_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mnext_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliquid_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_onehot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muncertainty_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_z_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3275301907.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msensory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_new\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msensory\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mh_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_new\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "import os\n",
        "import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚ö° HARDWARE: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "HYPER_PARAMS = {\n",
        "    \"GRID_W\": 10,\n",
        "    \"MAX_STEPS\": 100,\n",
        "    \"EPISODES\": 100, # More episodes for genius emergence\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"VECTOR_DIM\": 32, # State vector size\n",
        "}\n",
        "\n",
        "# --- CELL 1: THE TITAN CRAFT ENVIRONMENT (TURBO) ---\n",
        "class TitanCraftEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(TitanCraftEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(6)\n",
        "        # Vector: [AgentPos(2), Inv(4), Tools(2), NearTree(2), NearRock(2), NearIron(2), NearDiamond(2), StepCount(1)]\n",
        "        self.observation_space = spaces.Box(low=0, high=10, shape=(HYPER_PARAMS[\"VECTOR_DIM\"],), dtype=np.float32)\n",
        "        self.grid_size = HYPER_PARAMS[\"GRID_W\"]\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = np.array([5, 5])\n",
        "        self.inventory = {\"wood\": 0, \"stone\": 0, \"iron\": 0, \"diamond\": 0}\n",
        "        self.tools = {\"table\": False, \"pick_level\": 0}\n",
        "        self.steps = 0\n",
        "        self.map_objects = []\n",
        "        for _ in range(3): self.map_objects.append({\"type\": \"tree\", \"pos\": self._rand_pos()})\n",
        "        for _ in range(3): self.map_objects.append({\"type\": \"rock\", \"pos\": self._rand_pos()})\n",
        "        for _ in range(2): self.map_objects.append({\"type\": \"iron\", \"pos\": self._rand_pos()})\n",
        "        self.map_objects.append({\"type\": \"diamond\", \"pos\": self._rand_pos()})\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _rand_pos(self):\n",
        "        return np.random.randint(0, self.grid_size, size=2)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # Flattened state for fast learning\n",
        "        obs = [self.agent_pos[0]/10.0, self.agent_pos[1]/10.0]\n",
        "        obs += [self.inventory[k]/5.0 for k in [\"wood\", \"stone\", \"iron\", \"diamond\"]]\n",
        "        obs += [1.0 if self.tools[\"table\"] else 0.0, self.tools[\"pick_level\"]/3.0]\n",
        "\n",
        "        # Add relative pos of nearest objects of each type\n",
        "        for t in [\"tree\", \"rock\", \"iron\", \"diamond\"]:\n",
        "            nearest_dist = 100\n",
        "            rel_pos = np.array([0, 0])\n",
        "            for obj in self.map_objects:\n",
        "                if obj[\"type\"] == t and obj[\"pos\"][0] != -1:\n",
        "                    d = np.sum(np.abs(self.agent_pos - obj[\"pos\"]))\n",
        "                    if d < nearest_dist:\n",
        "                        nearest_dist = d\n",
        "                        rel_pos = obj[\"pos\"] - self.agent_pos\n",
        "            obs += [rel_pos[0]/10.0, rel_pos[1]/10.0]\n",
        "\n",
        "        # Pad to VECTOR_DIM\n",
        "        obs += [self.steps / 250.0]\n",
        "        while len(obs) < HYPER_PARAMS[\"VECTOR_DIM\"]: obs.append(0.0)\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        reward = -0.05 # Existential pressure\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        move = {0: [0,-1], 1: [0,1], 2: [-1,0], 3: [1,0]}\n",
        "        if action < 4:\n",
        "            old_dist = self._min_dist_to_any()\n",
        "            new_pos = self.agent_pos + np.array(move[action])\n",
        "            self.agent_pos = np.clip(new_pos, 0, self.grid_size-1)\n",
        "            new_dist = self._min_dist_to_any()\n",
        "            # Reward Shaping: Better to move toward objects\n",
        "            if new_dist < old_dist: reward += 0.05\n",
        "\n",
        "        elif action == 4: # Interact\n",
        "            for obj in self.map_objects:\n",
        "                if np.array_equal(self.agent_pos, obj[\"pos\"]):\n",
        "                    if obj[\"type\"] == \"tree\":\n",
        "                        self.inventory[\"wood\"] += 1\n",
        "                        reward += 2.0 # High value for foundation\n",
        "                        obj[\"pos\"] = np.array([-1, -1])\n",
        "                        info[\"log\"] = \"ü™ì Chopped Wood\"\n",
        "                    elif obj[\"type\"] == \"rock\" and self.tools[\"pick_level\"] >= 1:\n",
        "                        self.inventory[\"stone\"] += 1\n",
        "                        reward += 3.0\n",
        "                        obj[\"pos\"] = np.array([-1, -1])\n",
        "                        info[\"log\"] = \"‚õèÔ∏è Mined Stone\"\n",
        "                    elif obj[\"type\"] == \"iron\" and self.tools[\"pick_level\"] >= 2:\n",
        "                        self.inventory[\"iron\"] += 1\n",
        "                        reward += 5.0\n",
        "                        obj[\"pos\"] = np.array([-1, -1])\n",
        "                        info[\"log\"] = \"‚õìÔ∏è Mined Iron\"\n",
        "                    elif obj[\"type\"] == \"diamond\" and self.tools[\"pick_level\"] >= 3:\n",
        "                        self.inventory[\"diamond\"] += 1\n",
        "                        reward += 100.0\n",
        "                        done = True\n",
        "                        info[\"log\"] = \"üíé FOUND DIAMOND!\"\n",
        "\n",
        "        elif action == 5: # Craft\n",
        "            if self.inventory[\"wood\"] >= 2 and not self.tools[\"table\"]:\n",
        "                self.inventory[\"wood\"] -= 2\n",
        "                self.tools[\"table\"] = True\n",
        "                reward += 5.0\n",
        "                info[\"log\"] = \"üî® Crafted Table\"\n",
        "            elif self.tools[\"table\"] and self.inventory[\"wood\"] >= 2 and self.tools[\"pick_level\"] == 0:\n",
        "                self.inventory[\"wood\"] -= 2\n",
        "                self.tools[\"pick_level\"] = 1\n",
        "                reward += 5.0\n",
        "                info[\"log\"] = \"üó°Ô∏è Crafted Wood Pick\"\n",
        "            elif self.tools[\"table\"] and self.inventory[\"stone\"] >= 3 and self.tools[\"pick_level\"] == 1:\n",
        "                self.inventory[\"stone\"] -= 3\n",
        "                self.tools[\"pick_level\"] = 2\n",
        "                reward += 10.0\n",
        "                info[\"log\"] = \"‚öíÔ∏è Crafted Stone Pick\"\n",
        "            elif self.tools[\"table\"] and self.inventory[\"iron\"] >= 3 and self.tools[\"pick_level\"] == 2:\n",
        "                self.inventory[\"iron\"] -= 3\n",
        "                self.tools[\"pick_level\"] = 3\n",
        "                reward += 20.0\n",
        "                info[\"log\"] = \"‚öîÔ∏è Crafted Iron Pick\"\n",
        "\n",
        "        if self.steps >= HYPER_PARAMS[\"MAX_STEPS\"]: done = True\n",
        "        return self._get_obs(), reward, done, info\n",
        "\n",
        "    def _min_dist_to_any(self):\n",
        "        dists = [np.sum(np.abs(self.agent_pos - o[\"pos\"])) for o in self.map_objects if o[\"pos\"][0] != -1]\n",
        "        return min(dists) if dists else 0\n",
        "\n",
        "# --- WORLD MODEL HELPERS ---\n",
        "def symlog(x): return torch.sign(x) * torch.log(torch.abs(x) + 1.0)\n",
        "def symexp(x): return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
        "\n",
        "# --- CELL 2: THE LIQUID DARK ARCHITECTURE (TURBO) ---\n",
        "class LiquidCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, delta_t=0.5):\n",
        "        super().__init__()\n",
        "        self.w_input = nn.Linear(input_size, hidden_size)\n",
        "        self.w_state = nn.Linear(hidden_size, hidden_size)\n",
        "        self.tau = nn.Parameter(torch.ones(1, hidden_size))\n",
        "        self.A = nn.Parameter(torch.ones(1, hidden_size))\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        sensory = torch.sigmoid(self.w_input(x) + self.w_state(h))\n",
        "        h_new = h\n",
        "        for _ in range(3):\n",
        "            dh = (-(1.0 / torch.clamp(torch.exp(self.tau), min=1e-3) + sensory) * h_new + sensory * self.A)\n",
        "            h_new = h_new + 0.5 * dh\n",
        "        return h_new\n",
        "\n",
        "class DarkHippocampus:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.data = []\n",
        "        self.priorities = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def push(self, exp, p):\n",
        "        if len(self.data) >= self.capacity:\n",
        "            self.data.pop(0)\n",
        "            self.priorities.pop(0)\n",
        "        self.data.append(exp)\n",
        "        self.priorities.append(p + 1e-5)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        p = np.array(self.priorities)\n",
        "        p = p / p.sum()\n",
        "        indices = np.random.choice(len(self.data), batch_size, p=p)\n",
        "        return [self.data[i] for i in indices], indices\n",
        "\n",
        "    def update_priorities(self, idxs, ps):\n",
        "        for i, p in zip(idxs, ps):\n",
        "            self.priorities[i] = p\n",
        "\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Vector Projector instead of CNN\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ELU(),\n",
        "            nn.Linear(128, 256), nn.ELU()\n",
        "        )\n",
        "        self.liquid_core = LiquidCell(256 + 6, 256)\n",
        "        self.reward_head = nn.Linear(256, 1)\n",
        "        self.value_head = nn.Linear(256, 1)\n",
        "        self.uncertainty_head = nn.Linear(256, 1)\n",
        "        self.next_z_head = nn.Linear(256, 256)\n",
        "        self.opt = optim.Adam(self.parameters(), lr=1e-3) # Faster LR\n",
        "\n",
        "    def encode(self, obs):\n",
        "        return self.encoder(obs)\n",
        "\n",
        "    def dream_step(self, h, action):\n",
        "        if isinstance(action, int) or len(action.shape) == 0:\n",
        "            act_onehot = F.one_hot(torch.tensor(action, device=device).long(), 6).float().unsqueeze(0)\n",
        "        else:\n",
        "            act_onehot = F.one_hot(action.long(), 6).float()\n",
        "        next_h = self.liquid_core(torch.cat([h, act_onehot], dim=1), h)\n",
        "        return next_h, self.reward_head(next_h), self.value_head(next_h), torch.sigmoid(self.uncertainty_head(next_h)), self.next_z_head(next_h)\n",
        "\n",
        "class LiquidDarkAgent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wm = WorldModel().to(device)\n",
        "        self.actor = nn.Sequential(nn.Linear(256, 128), nn.ELU(), nn.Linear(128, 6)).to(device)\n",
        "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=5e-4)\n",
        "        self.hippocampus = DarkHippocampus()\n",
        "        self.lambda_surprise = 2.0 # EXTREME CURIOSITY\n",
        "\n",
        "    def plan_and_act(self, obs):\n",
        "        h = self.wm.encode(torch.FloatTensor(obs).unsqueeze(0).to(device))\n",
        "        best_a, max_score = 0, -1e9\n",
        "        for a in range(6):\n",
        "            curr_h, r_pred, v_pred, unc, _ = self.wm.dream_step(h, a)\n",
        "            score = r_pred.item() + 0.95 * v_pred.item()\n",
        "            horizon = 0\n",
        "            while horizon < 20 and unc.item() < 0.3: # Turbo depth\n",
        "                horizon += 1\n",
        "                sim_a = torch.argmax(self.actor(curr_h), dim=1)\n",
        "                curr_h, r_pred, v_pred, unc, _ = self.wm.dream_step(curr_h, sim_a)\n",
        "                score += (0.95**horizon) * r_pred.item()\n",
        "            if score > max_score: max_score, best_a = score, a\n",
        "        return best_a\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.hippocampus.data) < 64: return\n",
        "        batch, idxs = self.hippocampus.sample(64)\n",
        "        o, a, r, no, d = zip(*batch)\n",
        "        o, no = torch.FloatTensor(np.array(o)).to(device), torch.FloatTensor(np.array(no)).to(device)\n",
        "        a, r = torch.tensor(a).to(device), torch.FloatTensor(r).unsqueeze(1).to(device)\n",
        "\n",
        "        z = self.wm.encode(o)\n",
        "        with torch.no_grad(): target_z = self.wm.encode(no)\n",
        "        h_next, r_p, v_p, u_p, nz_p = self.wm.dream_step(z, a)\n",
        "\n",
        "        surprise = F.mse_loss(nz_p, target_z, reduction='none').mean(dim=1, keepdim=True)\n",
        "        self.wm.opt.zero_grad()\n",
        "        (F.mse_loss(nz_p, target_z) + F.mse_loss(r_p, symlog(r)) + F.mse_loss(u_p, surprise.detach())).backward()\n",
        "        self.wm.opt.step()\n",
        "\n",
        "        dist = torch.distributions.Categorical(logits=self.actor(h_next.detach()))\n",
        "        act = dist.sample()\n",
        "        adv = (symlog(r) + self.lambda_surprise * surprise.detach() - v_p.detach())\n",
        "        loss_actor = -(dist.log_prob(act) * adv).mean()\n",
        "        self.opt_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        self.opt_actor.step()\n",
        "        self.hippocampus.update_priorities(idxs, surprise.flatten().detach().cpu().numpy())\n",
        "\n",
        "# --- BENCHMARK ---\n",
        "class StandardDQRN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ReLU(), nn.Linear(128, 6)).to(device)\n",
        "    def act(self, obs):\n",
        "        q = self.net(torch.FloatTensor(obs).to(device))\n",
        "        return torch.argmax(q).item()\n",
        "\n",
        "def run_benchmark():\n",
        "    print(\"\\nüöÄ STARTING TURBO LIQUID DARK DUEL üöÄ\")\n",
        "    env = TitanCraftEnv()\n",
        "    dark, std = LiquidDarkAgent(), StandardDQRN()\n",
        "    res = {\"dark\": [], \"std\": []}\n",
        "\n",
        "    for ep in range(1, HYPER_PARAMS[\"EPISODES\"] + 1):\n",
        "        # DARK RUN\n",
        "        o, total_r, d_found = env.reset(), 0, 0\n",
        "        for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "            a = dark.plan_and_act(o)\n",
        "            if random.random() < 0.1: a = random.randint(0,5)\n",
        "            no, r, done, info = env.step(a)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                z, nz_t = dark.wm.encode(torch.FloatTensor(o).to(device)), dark.wm.encode(torch.FloatTensor(no).to(device))\n",
        "                _, _, _, _, nz_p = dark.wm.dream_step(z.unsqueeze(0), a)\n",
        "                s = F.mse_loss(nz_p.squeeze(), nz_t).item()\n",
        "\n",
        "            dark.hippocampus.push((o, a, r, no, done), s)\n",
        "            if t % 5 == 0: dark.train_step()\n",
        "            total_r, o = total_r + r, no\n",
        "            if env.inventory[\"diamond\"] > 0: d_found = 1\n",
        "            if done: break\n",
        "        res[\"dark\"].append(total_r)\n",
        "\n",
        "        # STD RUN (Minimalist Baseline)\n",
        "        o, r_std = env.reset(), 0\n",
        "        for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "            a = std.act(o)\n",
        "            o, r, d, _ = env.step(a)\n",
        "            r_std += r\n",
        "            if d: break\n",
        "        res[\"std\"].append(r_std)\n",
        "\n",
        "        if ep % 1 == 0 or d_found:\n",
        "            print(f\"Ep {ep}: Dark {total_r:.1f} (üíé{d_found}) | Std {r_std:.1f}\")\n",
        "\n",
        "    print(f\"\\nüèÜ FINAL: Std {np.mean(res['std']):.2f} | Dark {np.mean(res['dark']):.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2"
      ],
      "metadata": {
        "id": "dDvW55orGq3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "import os\n",
        "import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚ö° HARDWARE: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "HYPER_PARAMS = {\n",
        "    \"MAX_STEPS\": 200,\n",
        "    \"EPISODES\": 50,\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"VECTOR_DIM\": 9, # [x, y, vx, vy, sin_th, cos_th, dist, cur_x, cur_y]\n",
        "}\n",
        "\n",
        "# --- CELL 1: THE TITAN CRAFT ENVIRONMENT (TURBO) ---\n",
        "class LiquidAbyssEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A continuous control environment representing navigation in a chaotic vortex.\n",
        "    Goal: Reach (0, 0).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LiquidAbyssEnv, self).__init__()\n",
        "        # Actions: [Thrust (0 to 1), Torque (-1 to 1)]\n",
        "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=-20, high=20, shape=(HYPER_PARAMS[\"VECTOR_DIM\"],), dtype=np.float32)\n",
        "        self.dt = 0.1\n",
        "        self.intensity = 2.0\n",
        "\n",
        "    def reset(self):\n",
        "        # Start at a random position on a circle of radius 8\n",
        "        angle = random.uniform(0, 2 * np.pi)\n",
        "        self.pos = np.array([8.0 * np.cos(angle), 8.0 * np.sin(angle)])\n",
        "        self.vel = np.array([0.0, 0.0])\n",
        "        self.theta = random.uniform(0, 2 * np.pi)\n",
        "        self.steps = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        dist = np.linalg.norm(self.pos)\n",
        "        cur_x = self.intensity * np.sin(self.pos[1] / 2.0)\n",
        "        cur_y = self.intensity * np.cos(self.pos[0] / 2.0)\n",
        "        obs = [\n",
        "            self.pos[0] / 10.0, self.pos[1] / 10.0,\n",
        "            self.vel[0] / 5.0, self.vel[1] / 5.0,\n",
        "            np.sin(self.theta), np.cos(self.theta),\n",
        "            dist / 10.0,\n",
        "            cur_x / self.intensity, cur_y / self.intensity\n",
        "        ]\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        thrust = np.clip((action[0] + 1.0) / 2.0, 0.0, 1.0) # Map -1,1 to 0,1\n",
        "        torque = action[1]\n",
        "\n",
        "        # Current forces\n",
        "        cur_x = self.intensity * np.sin(self.pos[1] / 2.0)\n",
        "        cur_y = self.intensity * np.cos(self.pos[0] / 2.0)\n",
        "\n",
        "        # Agent forces\n",
        "        ax = thrust * np.cos(self.theta)\n",
        "        ay = thrust * np.sin(self.theta)\n",
        "\n",
        "        # Update physics\n",
        "        self.vel[0] += (ax + cur_x) * self.dt\n",
        "        self.vel[1] += (ay + cur_y) * self.dt\n",
        "        self.pos += self.vel * self.dt\n",
        "        self.theta += torque * self.dt * 2.0 # More rotation power\n",
        "\n",
        "        dist = np.linalg.norm(self.pos)\n",
        "        reward = -dist * 0.01 # Distance penalty\n",
        "        reward -= 0.01 # Existential penalty\n",
        "\n",
        "        # Reward for getting closer\n",
        "        if dist < 0.5:\n",
        "            reward += 10.0\n",
        "            done = True\n",
        "        elif self.steps >= HYPER_PARAMS[\"MAX_STEPS\"]:\n",
        "            done = True\n",
        "        elif dist > 15.0: # Swept away\n",
        "            reward -= 5.0\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        return self._get_obs(), reward, done, {}\n",
        "# --- WORLD MODEL HELPERS ---\n",
        "def symlog(x): return torch.sign(x) * torch.log(torch.abs(x) + 1.0)\n",
        "def symexp(x): return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
        "\n",
        "# --- CELL 2: THE LIQUID DARK ARCHITECTURE (TURBO) ---\n",
        "class LiquidCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, delta_t=0.5):\n",
        "        super().__init__()\n",
        "        self.w_input = nn.Linear(input_size, hidden_size)\n",
        "        self.w_state = nn.Linear(hidden_size, hidden_size)\n",
        "        self.tau = nn.Parameter(torch.ones(1, hidden_size))\n",
        "        self.A = nn.Parameter(torch.ones(1, hidden_size))\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        sensory = torch.sigmoid(self.w_input(x) + self.w_state(h))\n",
        "        h_new = h\n",
        "        for _ in range(3):\n",
        "            dh = (-(1.0 / torch.clamp(torch.exp(self.tau), min=1e-3) + sensory) * h_new + sensory * self.A)\n",
        "            h_new = h_new + 0.5 * dh\n",
        "        return h_new\n",
        "\n",
        "class DarkHippocampus:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.data = []\n",
        "        self.priorities = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def push(self, exp, p):\n",
        "        if len(self.data) >= self.capacity:\n",
        "            self.data.pop(0)\n",
        "            self.priorities.pop(0)\n",
        "        self.data.append(exp)\n",
        "        self.priorities.append(p + 1e-5)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        p = np.array(self.priorities)\n",
        "        p = p / p.sum()\n",
        "        indices = np.random.choice(len(self.data), batch_size, p=p)\n",
        "        return [self.data[i] for i in indices], indices\n",
        "\n",
        "    def update_priorities(self, idxs, ps):\n",
        "        for i, p in zip(idxs, ps):\n",
        "            self.priorities[i] = p\n",
        "\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Vector Projector instead of CNN\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ELU(),\n",
        "            nn.Linear(128, 256), nn.ELU()\n",
        "        )\n",
        "        self.liquid_core = LiquidCell(256 + 2, 256) # 256 state + 2 action\n",
        "        self.reward_head = nn.Linear(256, 1)\n",
        "        self.value_head = nn.Linear(256, 1)\n",
        "        self.uncertainty_head = nn.Linear(256, 1)\n",
        "        self.next_z_head = nn.Linear(256, 256)\n",
        "        self.opt = optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "    def encode(self, obs):\n",
        "        return self.encoder(obs)\n",
        "\n",
        "    def dream_step(self, h, action):\n",
        "        # Action is [batch, 2]\n",
        "        if len(action.shape) == 1:\n",
        "            action = action.unsqueeze(0)\n",
        "        next_h = self.liquid_core(torch.cat([h, action], dim=1), h)\n",
        "        return next_h, self.reward_head(next_h), self.value_head(next_h), torch.sigmoid(self.uncertainty_head(next_h)), self.next_z_head(next_h)\n",
        "\n",
        "class LiquidDarkAgent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wm = WorldModel().to(device)\n",
        "        # Actor outputs [mu_thrust, mu_torque, log_std_thrust, log_std_torque]\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(256, 128), nn.ELU(),\n",
        "            nn.Linear(128, 4)\n",
        "        ).to(device)\n",
        "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=5e-4)\n",
        "        self.hippocampus = DarkHippocampus()\n",
        "        self.lambda_surprise = 2.0\n",
        "        self.num_candidates = 8 # Sample 8 trajectories\n",
        "\n",
        "    def get_action_dist(self, h):\n",
        "        out = self.actor(h)\n",
        "        mu, log_std = out[:, :2], out[:, 2:]\n",
        "        std = torch.exp(torch.clamp(log_std, -5, 2))\n",
        "        return torch.distributions.Normal(mu, std)\n",
        "\n",
        "    def plan_and_act(self, obs):\n",
        "        h = self.wm.encode(torch.FloatTensor(obs).unsqueeze(0).to(device))\n",
        "\n",
        "        # CEM-like selection in imagination\n",
        "        best_a, max_score = None, -1e9\n",
        "\n",
        "        # Try a few random actions + the actor's mu\n",
        "        candidates = []\n",
        "        with torch.no_grad():\n",
        "            dist = self.get_action_dist(h)\n",
        "            candidates.append(dist.mean) # The \"Expert\" suggestion\n",
        "            for _ in range(self.num_candidates - 1):\n",
        "                candidates.append(dist.sample())\n",
        "\n",
        "        for a_cand in candidates:\n",
        "            curr_h, r_pred, v_pred, unc, _ = self.wm.dream_step(h, a_cand)\n",
        "            score = r_pred.item() + 0.95 * v_pred.item()\n",
        "\n",
        "            # Deeper dreaming if confident\n",
        "            horizon = 0\n",
        "            while horizon < 10 and unc.item() < 0.3:\n",
        "                horizon += 1\n",
        "                sim_a = self.get_action_dist(curr_h).sample()\n",
        "                curr_h, r_pred, v_pred, unc, _ = self.wm.dream_step(curr_h, sim_a)\n",
        "                score += (0.95**horizon) * r_pred.item()\n",
        "\n",
        "            if score > max_score:\n",
        "                max_score = score\n",
        "                best_a = a_cand\n",
        "\n",
        "        return best_a.squeeze().cpu().numpy()\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.hippocampus.data) < 64: return\n",
        "        batch, idxs = self.hippocampus.sample(64)\n",
        "        o, a, r, no, d = zip(*batch)\n",
        "        o, no = torch.FloatTensor(np.array(o)).to(device), torch.FloatTensor(np.array(no)).to(device)\n",
        "        a, r = torch.FloatTensor(np.array(a)).to(device), torch.FloatTensor(r).unsqueeze(1).to(device)\n",
        "\n",
        "        z = self.wm.encode(o)\n",
        "        with torch.no_grad(): target_z = self.wm.encode(no)\n",
        "        h_next, r_p, v_p, u_p, nz_p = self.wm.dream_step(z, a)\n",
        "\n",
        "        surprise = F.mse_loss(nz_p, target_z, reduction='none').mean(dim=1, keepdim=True)\n",
        "        self.wm.opt.zero_grad()\n",
        "        (F.mse_loss(nz_p, target_z) + F.mse_loss(r_p, symlog(r)) + F.mse_loss(u_p, surprise.detach())).backward()\n",
        "        self.wm.opt.step()\n",
        "\n",
        "        dist = self.get_action_dist(h_next.detach())\n",
        "        act = dist.sample()\n",
        "        # Intrinsic Reward: Surprise!\n",
        "        adv = (symlog(r) + self.lambda_surprise * surprise.detach() - v_p.detach())\n",
        "        loss_actor = -(dist.log_prob(act).sum(dim=1, keepdim=True) * adv).mean()\n",
        "\n",
        "        self.opt_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        self.opt_actor.step()\n",
        "        self.hippocampus.update_priorities(idxs, surprise.flatten().detach().cpu().numpy())\n",
        "\n",
        "# --- BENCHMARK ---\n",
        "class StandardMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ReLU(),\n",
        "            nn.Linear(128, 2)\n",
        "        ).to(device)\n",
        "    def act(self, obs):\n",
        "        with torch.no_grad():\n",
        "            a = self.net(torch.FloatTensor(obs).to(device))\n",
        "        return np.clip(a.cpu().numpy(), -1.0, 1.0)\n",
        "\n",
        "def run_benchmark():\n",
        "    print(\"\\nüöÄ STARTING LIQUID ABYSS CHALLENGE üöÄ\")\n",
        "    env = LiquidAbyssEnv()\n",
        "    dark, std = LiquidDarkAgent(), StandardMLP()\n",
        "    res = {\"dark\": [], \"std\": []}\n",
        "\n",
        "    for ep in range(1, HYPER_PARAMS[\"EPISODES\"] + 1):\n",
        "        # DARK RUN\n",
        "        o, total_r, success = env.reset(), 0, 0\n",
        "        for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "            a = dark.plan_and_act(o)\n",
        "            # Add some exploration noise to data collection\n",
        "            noise = np.random.normal(0, 0.1, size=2)\n",
        "            no, r, done, info = env.step(a + noise)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                z = dark.wm.encode(torch.FloatTensor(o).to(device)).unsqueeze(0)\n",
        "                nz_t = dark.wm.encode(torch.FloatTensor(no).to(device)).unsqueeze(0)\n",
        "                _, _, _, _, nz_p = dark.wm.dream_step(z, torch.FloatTensor(a).to(device).unsqueeze(0))\n",
        "                s = F.mse_loss(nz_p, nz_t).item()\n",
        "\n",
        "            dark.hippocampus.push((o, a, r, no, done), s)\n",
        "            if t % 5 == 0: dark.train_step()\n",
        "            total_r, o = total_r + r, no\n",
        "            if done:\n",
        "                if np.linalg.norm(env.pos) < 0.5: success = 1\n",
        "                break\n",
        "        res[\"dark\"].append(total_r)\n",
        "\n",
        "        # STD RUN (Random/Simple Baseline)\n",
        "        o, r_std = env.reset(), 0\n",
        "        for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "            a = std.act(o)\n",
        "            o, r, d, _ = env.step(a)\n",
        "            r_std += r\n",
        "            if d: break\n",
        "        res[\"std\"].append(r_std)\n",
        "\n",
        "        if ep % 5 == 0 or success:\n",
        "            print(f\"Ep {ep}: Dark {total_r:.1f} ({'üéØ' if success else 'üåä'}) | Std {r_std:.1f}\")\n",
        "\n",
        "    print(f\"\\nüèÜ FINAL AVG: Std {np.mean(res['std']):.2f} | Dark {np.mean(res['dark']):.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "id": "AnChckHekM4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PE1DskYI4du_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ne0HuSfC4dp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FDFCXiZ54diI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 3"
      ],
      "metadata": {
        "id": "Ms2rHM6-GtNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "import os\n",
        "import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚ö° HARDWARE: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "SEED = 42\n",
        "HYPER_PARAMS = {\n",
        "    \"MAX_STEPS\": 200,\n",
        "    \"EPISODES_PER_LEVEL\": 10,\n",
        "    \"BATCH_SIZE\": 16, # Sequence batch size\n",
        "    \"SEQ_LEN\": 16,    # Length of imagination/training sequence\n",
        "    \"HORIZON\": 15,    # Planning horizon\n",
        "    \"VECTOR_DIM\": 10,\n",
        "    \"DIAMOND_REWARD\": 50.0,\n",
        "    \"MINE_DISTANCE\": 0.8,\n",
        "}\n",
        "\n",
        "LEVEL_DEFS = {\n",
        "    1: {\"name\": \"Abyssal Grass\", \"intensity\": 0.5, \"diamonds\": 15},\n",
        "    2: {\"name\": \"Vortex Woods\", \"intensity\": 1.2, \"diamonds\": 7},\n",
        "    3: {\"name\": \"Event Horizon\", \"intensity\": 2.5, \"diamonds\": 3},\n",
        "}\n",
        "\n",
        "# --- CELL 1: THE TITAN CRAFT ENVIRONMENT (TURBO) ---\n",
        "class LiquidAbyssEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A continuous control environment representing navigation in a chaotic vortex.\n",
        "    Goal: Reach (0, 0).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LiquidAbyssEnv, self).__init__()\n",
        "        # Actions: [Thrust (0 to 1), Torque (-1 to 1)]\n",
        "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=-20, high=20, shape=(HYPER_PARAMS[\"VECTOR_DIM\"],), dtype=np.float32)\n",
        "        self.dt = 0.1\n",
        "        self.intensity = 1.0 # Start easy\n",
        "        self.difficulty_cycle = 5\n",
        "        self.diamonds = []\n",
        "\n",
        "    def reset(self, level=1, seed=None):\n",
        "        # Deterministic seeding for fairness\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "            random.seed(seed)\n",
        "\n",
        "        # Start at a random position on a circle of radius 8\n",
        "        angle = random.uniform(0, 2 * np.pi)\n",
        "        self.pos = np.array([8.0 * np.cos(angle), 8.0 * np.sin(angle)])\n",
        "        self.vel = np.array([0.0, 0.0])\n",
        "        self.theta = random.uniform(0, 2 * np.pi)\n",
        "        self.steps = 0\n",
        "\n",
        "        # Level-based difficulty\n",
        "        level_cfg = LEVEL_DEFS.get(level, LEVEL_DEFS[1])\n",
        "        self.intensity = level_cfg[\"intensity\"]\n",
        "        self.diamonds = [np.array([random.uniform(-12, 12), random.uniform(-12, 12)]) for _ in range(level_cfg[\"diamonds\"])]\n",
        "\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        dist = np.linalg.norm(self.pos)\n",
        "        cur_x = self.intensity * np.sin(self.pos[1] / 2.0)\n",
        "        cur_y = self.intensity * np.cos(self.pos[0] / 2.0)\n",
        "\n",
        "        # Proximity to nearest diamond\n",
        "        if self.diamonds:\n",
        "            diamond_dists = [np.linalg.norm(self.pos - d) for d in self.diamonds]\n",
        "            min_diamond_dist = min(diamond_dists)\n",
        "        else:\n",
        "            min_diamond_dist = 20.0\n",
        "\n",
        "        obs = [\n",
        "            self.pos[0] / 10.0, self.pos[1] / 10.0,\n",
        "            self.vel[0] / 5.0, self.vel[1] / 5.0,\n",
        "            np.sin(self.theta), np.cos(self.theta),\n",
        "            dist / 10.0,\n",
        "            cur_x / self.intensity, cur_y / self.intensity,\n",
        "            min_diamond_dist / 10.0\n",
        "        ]\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        thrust = np.clip((action[0] + 1.0) / 2.0, 0.0, 1.0) # Map -1,1 to 0,1\n",
        "        torque = action[1]\n",
        "\n",
        "        # Current forces\n",
        "        cur_x = self.intensity * np.sin(self.pos[1] / 2.0)\n",
        "        cur_y = self.intensity * np.cos(self.pos[0] / 2.0)\n",
        "\n",
        "        # Agent forces\n",
        "        ax = thrust * np.cos(self.theta)\n",
        "        ay = thrust * np.sin(self.theta)\n",
        "\n",
        "        # Update physics\n",
        "        self.vel[0] += (ax + cur_x) * self.dt\n",
        "        self.vel[1] += (ay + cur_y) * self.dt\n",
        "        self.pos += self.vel * self.dt\n",
        "        self.theta += torque * self.dt * 2.0 # More rotation power\n",
        "\n",
        "        dist = np.linalg.norm(self.pos)\n",
        "        reward = -dist * 0.01 # Distance penalty\n",
        "        reward -= 0.01 # Existential penalty\n",
        "\n",
        "        # Diamond Mining Logic\n",
        "        mined_indices = []\n",
        "        self.last_mined_count = 0\n",
        "        for i, diamond_pos in enumerate(self.diamonds):\n",
        "            if np.linalg.norm(self.pos - diamond_pos) < HYPER_PARAMS[\"MINE_DISTANCE\"]:\n",
        "                reward += HYPER_PARAMS[\"DIAMOND_REWARD\"]\n",
        "                mined_indices.append(i)\n",
        "\n",
        "        self.last_mined_count = len(mined_indices)\n",
        "        # Remove mined diamonds\n",
        "        for i in sorted(mined_indices, reverse=True):\n",
        "            self.diamonds.pop(i)\n",
        "\n",
        "        # Reward for getting closer to origin (optional goal)\n",
        "        if dist < 0.5:\n",
        "            reward += 10.0\n",
        "            done = True\n",
        "        elif self.steps >= HYPER_PARAMS[\"MAX_STEPS\"]:\n",
        "            done = True\n",
        "        elif dist > 20.0: # Swept away (increased range for easier mode)\n",
        "            reward -= 5.0\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        return self._get_obs(), reward, done, {}\n",
        "# --- WORLD MODEL HELPERS ---\n",
        "def symlog(x): return torch.sign(x) * torch.log(torch.abs(x) + 1.0)\n",
        "def symexp(x): return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
        "\n",
        "# --- CELL 2: THE LIQUID DARK ARCHITECTURE (DREAMER V4) ---\n",
        "class LiquidCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Liquid Time-Constant (LTC) Cell.\n",
        "    Adapts its time-constant (tau) based on input context.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.w_input = nn.Linear(input_size, hidden_size)\n",
        "        self.w_tau = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.w_state = nn.Linear(hidden_size, hidden_size)\n",
        "        self.A = nn.Parameter(torch.ones(1, hidden_size))\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Continuous-time dynamics\n",
        "        sensory = torch.tanh(self.w_input(x) + self.w_state(h))\n",
        "\n",
        "        # Liquid Time Constant: Tau depends on input + state\n",
        "        tau_gate = torch.sigmoid(self.w_tau(torch.cat([x, h], dim=-1)))\n",
        "        tau = 1.0 + 5.0 * tau_gate # Range [1.0, 6.0]\n",
        "\n",
        "        # ODE Solver (Euler Step with small dt)\n",
        "        h_new = h\n",
        "        dt = 0.1\n",
        "        for _ in range(3): # solver steps\n",
        "            dh = (-(h_new - self.A * sensory) / tau)\n",
        "            h_new = h_new + dt * dh\n",
        "        return h_new\n",
        "\n",
        "class SequenceBuffer:\n",
        "    \"\"\"Stores full episodes for sequence learning.\"\"\"\n",
        "    def __init__(self, capacity=2000):\n",
        "        self.episodes = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def push_episode(self, episode_data):\n",
        "        # episode_data: list of (obs, action, reward, done)\n",
        "        if len(self.episodes) >= self.capacity:\n",
        "            self.episodes.pop(0)\n",
        "        self.episodes.append(episode_data)\n",
        "\n",
        "    def sample_sequence(self, batch_size, seq_len):\n",
        "        if not self.episodes: return None\n",
        "\n",
        "        # Sample random episodes, then random chunks\n",
        "        obs_b, act_b, rew_b, term_b = [], [], [], []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            ep = random.choice(self.episodes)\n",
        "            if len(ep) < seq_len + 1:\n",
        "                # Pad if too short\n",
        "                start_idx = 0\n",
        "                segment = ep + [ep[-1]] * (seq_len + 1 - len(ep))\n",
        "            else:\n",
        "                start_idx = random.randint(0, len(ep) - seq_len - 1)\n",
        "                segment = ep[start_idx : start_idx + seq_len + 1]\n",
        "\n",
        "            # Unzip\n",
        "            o, a, r, d = zip(*segment)\n",
        "            obs_b.append(np.array(o))\n",
        "            act_b.append(np.array(a))\n",
        "            rew_b.append(np.array(r))\n",
        "            term_b.append(np.array(d))\n",
        "\n",
        "        return (torch.FloatTensor(np.array(obs_b)).to(device),\n",
        "                torch.FloatTensor(np.array(act_b)).to(device),\n",
        "                torch.FloatTensor(np.array(rew_b)).to(device),\n",
        "                torch.FloatTensor(np.array(term_b)).to(device))\n",
        "\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder: Projects Observation to State Encodings\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ELU(),\n",
        "            nn.Linear(128, 256), nn.ELU()\n",
        "        )\n",
        "\n",
        "        # RSSM-lite: Deterministic Liquid Path\n",
        "        self.core = LiquidCell(256 + 2, 256)\n",
        "\n",
        "        # Heads\n",
        "        self.reward_head = nn.Sequential(nn.Linear(256, 128), nn.ELU(), nn.Linear(128, 1))\n",
        "        self.value_head = nn.Sequential(nn.Linear(256, 128), nn.ELU(), nn.Linear(128, 1))\n",
        "        self.cont_head = nn.Sequential(nn.Linear(256, 128), nn.ELU(), nn.Linear(128, 1)) # Continuation (Success/Survivor)\n",
        "\n",
        "        self.opt = optim.Adam(self.parameters(), lr=3e-4)\n",
        "\n",
        "    def forward_seq(self, obs_seq, act_seq, init_h=None):\n",
        "        # Connects time steps\n",
        "        b, t, _ = obs_seq.shape\n",
        "        embedded = self.encoder(obs_seq)\n",
        "\n",
        "        states = []\n",
        "        if init_h is None: init_h = torch.zeros(b, 256).to(device)\n",
        "        h = init_h\n",
        "\n",
        "        for i in range(t):\n",
        "            # Input: Embedding + Previous Action\n",
        "            h = self.core(torch.cat([embedded[:, i], act_seq[:, i]], dim=-1), h)\n",
        "            states.append(h)\n",
        "\n",
        "        states = torch.stack(states, dim=1)\n",
        "        return states\n",
        "\n",
        "class LiquidDarkAgent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wm = WorldModel().to(device)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(256, 128), nn.ELU(),\n",
        "            nn.Linear(128, 2), nn.Tanh() # Deterministic Actor for simplicity & speed in Dreamer\n",
        "        ).to(device)\n",
        "\n",
        "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.buffer = SequenceBuffer()\n",
        "        self.h_state = None\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h_state = torch.zeros(1, 256).to(device)\n",
        "\n",
        "    def plan_and_act(self, obs):\n",
        "        # Inference: Direct Policy with Liquid State\n",
        "        with torch.no_grad():\n",
        "            if self.h_state is None: self.reset_state()\n",
        "\n",
        "            embedded = self.wm.encoder(torch.FloatTensor(obs).unsqueeze(0).to(device))\n",
        "\n",
        "            # Dynamics Update (Perception)\n",
        "            # We assume 'zero' previous action for perception update in this simplified loop\n",
        "            dummy_action = torch.zeros(1, 2).to(device)\n",
        "            self.h_state = self.wm.core(torch.cat([embedded, dummy_action], dim=-1), self.h_state)\n",
        "\n",
        "            # Action\n",
        "            action = self.actor(self.h_state)\n",
        "\n",
        "        return action.squeeze().cpu().numpy()\n",
        "\n",
        "    def imagine(self, start_states, horizon=15):\n",
        "        # Dreamer Rollout\n",
        "        states = [start_states]\n",
        "        actions = []\n",
        "\n",
        "        curr_h = start_states\n",
        "        for _ in range(horizon):\n",
        "            a = self.actor(curr_h.detach())\n",
        "            a_noisy = a + torch.randn_like(a) * 0.1\n",
        "\n",
        "            # Imagine dynamics (Thinking)\n",
        "            dummy_embed = torch.zeros(curr_h.shape[0], 256).to(device)\n",
        "            curr_h = self.wm.core(torch.cat([dummy_embed, a_noisy], dim=-1), curr_h)\n",
        "\n",
        "            states.append(curr_h)\n",
        "            actions.append(a)\n",
        "\n",
        "        return torch.stack(states, dim=1), torch.stack(actions, dim=1)\n",
        "\n",
        "    def train_step(self):\n",
        "        batch = self.buffer.sample_sequence(HYPER_PARAMS[\"BATCH_SIZE\"], HYPER_PARAMS[\"SEQ_LEN\"])\n",
        "        if batch is None: return\n",
        "        obs, act, rew, term = batch\n",
        "\n",
        "        # 1. Train World Model\n",
        "        model_states = self.wm.forward_seq(obs, act) # (B, T, 256)\n",
        "\n",
        "        pred_rews = self.wm.reward_head(model_states).squeeze(-1)\n",
        "        pred_vals = self.wm.value_head(model_states).squeeze(-1)\n",
        "\n",
        "        target_rews = symlog(rew)\n",
        "        loss_reward = F.mse_loss(pred_rews, target_rews)\n",
        "\n",
        "        self.wm.opt.zero_grad()\n",
        "        loss_reward.backward()\n",
        "        self.wm.opt.step()\n",
        "\n",
        "        # 2. Train Actor (Deep Planning)\n",
        "        start_states = model_states[:, -1].detach()\n",
        "        dream_states, dream_acts = self.imagine(start_states, horizon=HYPER_PARAMS[\"HORIZON\"])\n",
        "\n",
        "        dream_rews = self.wm.reward_head(dream_states).squeeze(-1)\n",
        "        dream_vals = self.wm.value_head(dream_states).squeeze(-1)\n",
        "\n",
        "        # Maximize Dreamed Value + Reward\n",
        "        loss_actor = -(dream_vals.mean() + dream_rews.mean())\n",
        "\n",
        "        self.opt_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        self.opt_actor.step()\n",
        "\n",
        "# --- BENCHMARK ---\n",
        "class StandardMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ReLU(),\n",
        "            nn.Linear(128, 2)\n",
        "        ).to(device)\n",
        "    def act(self, obs):\n",
        "        with torch.no_grad():\n",
        "            a = self.net(torch.FloatTensor(obs).to(device))\n",
        "        return np.clip(a.cpu().numpy(), -1.0, 1.0)\n",
        "\n",
        "def run_benchmark():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ STARTING UPGRADED MULTI-LEVEL CHALLENGE üöÄ\")\n",
        "    print(f\"‚ö° HARDWARE: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    env = LiquidAbyssEnv()\n",
        "    dark, std = LiquidDarkAgent(), StandardMLP()\n",
        "\n",
        "    overall_res = {\"dark\": {\"rewards\": [], \"diamonds\": [], \"success\": []},\n",
        "                   \"std\": {\"rewards\": [], \"diamonds\": [], \"success\": []}}\n",
        "\n",
        "    for lvl_idx in [1, 2, 3]:\n",
        "        lvl_cfg = LEVEL_DEFS[lvl_idx]\n",
        "        print(f\"\\nüèîÔ∏è LEVEL {lvl_idx}: {lvl_cfg['name'].upper()}\")\n",
        "        print(f\"   [Intensity: {lvl_cfg['intensity']} | Resources: {lvl_cfg['diamonds']} Diamonds]\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        lvl_rewards_dark, lvl_diamonds_dark, lvl_success_dark = [], [], []\n",
        "        lvl_rewards_std, lvl_diamonds_std, lvl_success_std = [], [], []\n",
        "\n",
        "        for ep in range(1, HYPER_PARAMS[\"EPISODES_PER_LEVEL\"] + 1):\n",
        "            shared_seed = SEED + lvl_idx * 100 + ep\n",
        "\n",
        "            # --- DARK RUN ---\n",
        "            o, total_r, success, diamonds_mined = env.reset(level=lvl_idx, seed=shared_seed), 0, 0, 0\n",
        "            steps, total_dist = 0, 0\n",
        "            dark.reset_state()\n",
        "            episode_history = [] # Buffer for sequence learning\n",
        "\n",
        "            for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "                a = dark.plan_and_act(o)\n",
        "                no, r, done, _ = env.step(a)\n",
        "\n",
        "                # Store transition\n",
        "                episode_history.append((o, a, r, done))\n",
        "\n",
        "                # Train frequently! (Dreamer style: 1 step = 1 train)\n",
        "                if t > HYPER_PARAMS[\"SEQ_LEN\"]:\n",
        "                    dark.train_step()\n",
        "\n",
        "                total_r += r\n",
        "                diamonds_mined += env.last_mined_count\n",
        "                total_dist += np.linalg.norm(env.pos)\n",
        "                steps += 1\n",
        "                o = no\n",
        "                if done:\n",
        "                    if np.linalg.norm(env.pos) < 0.5: success = 1\n",
        "                    break\n",
        "\n",
        "            # Push episode to replay buffer\n",
        "            dark.buffer.push_episode(episode_history)\n",
        "\n",
        "            lvl_rewards_dark.append(total_r)\n",
        "            lvl_diamonds_dark.append(diamonds_mined)\n",
        "            lvl_success_dark.append(success)\n",
        "            avg_dist_dark = total_dist / max(1, steps)\n",
        "\n",
        "            # --- STD RUN ---\n",
        "            o, total_r_std, success_std, diamonds_mined_std = env.reset(level=lvl_idx, seed=shared_seed), 0, 0, 0\n",
        "            steps_std, total_dist_std = 0, 0\n",
        "            for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "                a = std.act(o)\n",
        "                no, r, done, _ = env.step(a)\n",
        "                total_r_std += r\n",
        "                diamonds_mined_std += env.last_mined_count\n",
        "                total_dist_std += np.linalg.norm(env.pos)\n",
        "                steps_std += 1\n",
        "                o = no\n",
        "                if done:\n",
        "                    if np.linalg.norm(env.pos) < 0.5: success_std = 1\n",
        "                    break\n",
        "            lvl_rewards_std.append(total_r_std)\n",
        "            lvl_diamonds_std.append(diamonds_mined_std)\n",
        "            lvl_success_std.append(success_std)\n",
        "            avg_dist_std = total_dist_std / max(1, steps_std)\n",
        "\n",
        "            # --- DETAILED EPISODE OUTPUT ---\n",
        "            d_icon = \"üíé\" * (diamonds_mined // 1)\n",
        "            s_icon = \"üíé\" * (diamonds_mined_std // 1)\n",
        "            print(f\"Ep {ep} | DARK: {total_r:6.1f} [{d_icon:10}] {'üéØ' if success else 'üåä'} (Dist: {avg_dist_dark:4.1f})\")\n",
        "            print(f\"     | STD : {total_r_std:6.1f} [{s_icon:10}] {'üéØ' if success_std else 'üåä'} (Dist: {avg_dist_std:4.1f})\")\n",
        "            print(\"-\" * 35)\n",
        "\n",
        "        # LEVEL SUMMARY\n",
        "        print(f\"\\nüìä SUMMARY - LEVEL {lvl_idx}: {lvl_cfg['name']}\")\n",
        "        def print_stats(name, rewards, diamonds, success):\n",
        "            print(f\"   {name:5}: Avg R: {np.mean(rewards):7.1f} | Tot Diamonds: {sum(diamonds):2} | Success: {sum(success)}/{len(success)}\")\n",
        "\n",
        "        print_stats(\"DARK\", lvl_rewards_dark, lvl_diamonds_dark, lvl_success_dark)\n",
        "        print_stats(\"STD\", lvl_rewards_std, lvl_diamonds_std, lvl_success_std)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Update overall\n",
        "        overall_res[\"dark\"][\"rewards\"].extend(lvl_rewards_dark)\n",
        "        overall_res[\"dark\"][\"diamonds\"].extend(lvl_diamonds_dark)\n",
        "        overall_res[\"dark\"][\"success\"].extend(lvl_success_dark)\n",
        "        overall_res[\"std\"][\"rewards\"].extend(lvl_rewards_std)\n",
        "        overall_res[\"std\"][\"diamonds\"].extend(lvl_diamonds_std)\n",
        "        overall_res[\"std\"][\"success\"].extend(lvl_success_std)\n",
        "\n",
        "    # FINAL GRAND SUMMARY\n",
        "    print(\"\\n\" + \"üèÜ\" * 20)\n",
        "    print(\"      FINAL BENCHMARK CHAMPION\")\n",
        "    print(\"üèÜ\" * 20)\n",
        "    dark_avg = np.mean(overall_res[\"dark\"][\"rewards\"])\n",
        "    std_avg = np.mean(overall_res[\"std\"][\"rewards\"])\n",
        "    winner = \"LIQUID DARK AGENT\" if dark_avg > std_avg else \"STANDARD MLP\"\n",
        "\n",
        "    print(f\"   RANK #1: {winner}\")\n",
        "    print(f\"   DARK | Avg Score: {dark_avg:7.2f} | Diamonds: {sum(overall_res['dark']['diamonds']):>3} | Wins: {sum(overall_res['dark']['success'])}\")\n",
        "    print(f\"   STD  | Avg Score: {std_avg:7.2f} | Diamonds: {sum(overall_res['std']['diamonds']):>3} | Wins: {sum(overall_res['std']['success'])}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNbY7Kql4eGQ",
        "outputId": "493c8306-5319-4c9a-d8ac-2077a1169f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° HARDWARE: Tesla T4\n",
            "\n",
            "============================================================\n",
            "üöÄ STARTING UPGRADED MULTI-LEVEL CHALLENGE üöÄ\n",
            "‚ö° HARDWARE: Tesla T4\n",
            "============================================================\n",
            "\n",
            "üèîÔ∏è LEVEL 1: ABYSSAL GRASS\n",
            "   [Intensity: 0.5 | Resources: 15 Diamonds]\n",
            "------------------------------------------------------------\n",
            "Ep 1 | DARK:   37.6 [üíé         ] üåä (Dist: 12.0)\n",
            "     | STD :  -12.5 [          ] üåä (Dist: 11.9)\n",
            "-----------------------------------\n",
            "Ep 2 | DARK:   35.5 [üíé         ] üåä (Dist: 10.7)\n",
            "     | STD :   77.7 [üíéüíé        ] üåä (Dist: 10.2)\n",
            "-----------------------------------\n",
            "Ep 3 | DARK:  -11.8 [          ] üåä (Dist: 12.3)\n",
            "     | STD :  -12.0 [          ] üåä (Dist: 12.2)\n",
            "-----------------------------------\n",
            "Ep 4 | DARK:  -12.5 [          ] üåä (Dist:  7.3)\n",
            "     | STD :   27.5 [üíé         ] üåä (Dist: 10.3)\n",
            "-----------------------------------\n",
            "Ep 5 | DARK:   22.1 [üíé         ] üåä (Dist: 13.0)\n",
            "     | STD :   32.2 [üíé         ] üåä (Dist: 10.7)\n",
            "-----------------------------------\n",
            "Ep 6 | DARK:   80.4 [üíéüíé        ] üåä (Dist:  8.7)\n",
            "     | STD :  -14.7 [          ] üåä (Dist: 10.1)\n",
            "-----------------------------------\n",
            "Ep 7 | DARK:  -13.7 [          ] üåä (Dist: 12.7)\n",
            "     | STD :  -12.3 [          ] üåä (Dist: 11.7)\n",
            "-----------------------------------\n",
            "Ep 8 | DARK:  -13.7 [          ] üåä (Dist: 12.3)\n",
            "     | STD :  -12.1 [          ] üåä (Dist: 12.1)\n",
            "-----------------------------------\n",
            "Ep 9 | DARK:   34.1 [üíé         ] üåä (Dist:  7.6)\n",
            "     | STD :  -13.7 [          ] üåä (Dist: 10.5)\n",
            "-----------------------------------\n",
            "Ep 10 | DARK:  -20.4 [          ] üåä (Dist:  9.2)\n",
            "     | STD :   82.4 [üíéüíé        ] üåä (Dist:  7.8)\n",
            "-----------------------------------\n",
            "\n",
            "üìä SUMMARY - LEVEL 1: Abyssal Grass\n",
            "   DARK : Avg R:    13.8 | Tot Diamonds:  6 | Success: 0/10\n",
            "   STD  : Avg R:    14.2 | Tot Diamonds:  6 | Success: 0/10\n",
            "------------------------------------------------------------\n",
            "\n",
            "üèîÔ∏è LEVEL 2: VORTEX WOODS\n",
            "   [Intensity: 1.2 | Resources: 7 Diamonds]\n",
            "------------------------------------------------------------\n",
            "Ep 1 | DARK:  -24.0 [          ] üåä (Dist:  9.5)\n",
            "     | STD :  -14.6 [          ] üåä (Dist: 10.7)\n",
            "-----------------------------------\n",
            "Ep 2 | DARK:  -22.1 [          ] üåä (Dist:  9.3)\n",
            "     | STD :  -11.3 [          ] üåä (Dist: 11.1)\n",
            "-----------------------------------\n",
            "Ep 3 | DARK:  -15.9 [          ] üåä (Dist: 11.9)\n",
            "     | STD :   32.3 [üíé         ] üåä (Dist: 11.8)\n",
            "-----------------------------------\n",
            "Ep 4 | DARK:  -22.2 [          ] üåä (Dist: 10.0)\n",
            "     | STD :  -25.1 [          ] üåä (Dist: 10.3)\n",
            "-----------------------------------\n",
            "Ep 5 | DARK:  -21.3 [          ] üåä (Dist: 12.1)\n",
            "     | STD :   32.8 [üíé         ] üåä (Dist:  7.6)\n",
            "-----------------------------------\n",
            "Ep 6 | DARK:  -11.3 [          ] üåä (Dist: 12.3)\n",
            "     | STD :  -10.5 [          ] üåä (Dist: 12.1)\n",
            "-----------------------------------\n",
            "Ep 7 | DARK:  -12.7 [          ] üåä (Dist: 12.6)\n",
            "     | STD :  -14.1 [          ] üåä (Dist: 12.4)\n",
            "-----------------------------------\n",
            "Ep 8 | DARK:    8.0 [          ] üéØ (Dist:  5.0)\n",
            "     | STD :   58.1 [üíé         ] üéØ (Dist:  5.2)\n",
            "-----------------------------------\n",
            "Ep 9 | DARK:  -18.2 [          ] üåä (Dist: 10.5)\n",
            "     | STD :  -18.8 [          ] üåä (Dist:  9.4)\n",
            "-----------------------------------\n",
            "Ep 10 | DARK:  -14.4 [          ] üåä (Dist: 10.8)\n",
            "     | STD :  -14.3 [          ] üåä (Dist: 11.3)\n",
            "-----------------------------------\n",
            "\n",
            "üìä SUMMARY - LEVEL 2: Vortex Woods\n",
            "   DARK : Avg R:   -15.4 | Tot Diamonds:  0 | Success: 1/10\n",
            "   STD  : Avg R:     1.4 | Tot Diamonds:  3 | Success: 1/10\n",
            "------------------------------------------------------------\n",
            "\n",
            "üèîÔ∏è LEVEL 3: EVENT HORIZON\n",
            "   [Intensity: 2.5 | Resources: 3 Diamonds]\n",
            "------------------------------------------------------------\n",
            "Ep 1 | DARK:  -11.4 [          ] üåä (Dist: 11.7)\n",
            "     | STD :  -11.6 [          ] üåä (Dist: 11.5)\n",
            "-----------------------------------\n",
            "Ep 2 | DARK:    8.4 [          ] üéØ (Dist:  5.1)\n",
            "     | STD :  -10.4 [          ] üåä (Dist:  8.4)\n",
            "-----------------------------------\n",
            "Ep 3 | DARK:    7.4 [          ] üéØ (Dist:  5.1)\n",
            "     | STD :  -13.0 [          ] üåä (Dist:  7.8)\n",
            "-----------------------------------\n",
            "Ep 4 | DARK:  -10.3 [          ] üåä (Dist: 13.4)\n",
            "     | STD :   -9.7 [          ] üåä (Dist: 13.2)\n",
            "-----------------------------------\n",
            "Ep 5 | DARK:  -12.2 [          ] üåä (Dist:  7.3)\n",
            "     | STD :  -12.5 [          ] üåä (Dist:  8.0)\n",
            "-----------------------------------\n",
            "Ep 6 | DARK:   -9.3 [          ] üåä (Dist: 12.3)\n",
            "     | STD :   -8.9 [          ] üåä (Dist: 12.1)\n",
            "-----------------------------------\n",
            "Ep 7 | DARK:  -14.6 [          ] üåä (Dist:  8.0)\n",
            "     | STD :  -11.2 [          ] üåä (Dist:  6.5)\n",
            "-----------------------------------\n",
            "Ep 8 | DARK:   40.7 [üíé         ] üåä (Dist: 11.9)\n",
            "     | STD :   41.0 [üíé         ] üåä (Dist: 11.9)\n",
            "-----------------------------------\n",
            "Ep 9 | DARK:   -9.4 [          ] üåä (Dist: 12.4)\n",
            "     | STD :   -9.3 [          ] üåä (Dist: 12.1)\n",
            "-----------------------------------\n",
            "Ep 10 | DARK:   -9.4 [          ] üåä (Dist: 11.7)\n",
            "     | STD :   -9.5 [          ] üåä (Dist: 11.9)\n",
            "-----------------------------------\n",
            "\n",
            "üìä SUMMARY - LEVEL 3: Event Horizon\n",
            "   DARK : Avg R:    -2.0 | Tot Diamonds:  1 | Success: 2/10\n",
            "   STD  : Avg R:    -5.5 | Tot Diamonds:  1 | Success: 0/10\n",
            "------------------------------------------------------------\n",
            "\n",
            "üèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜ\n",
            "      FINAL BENCHMARK CHAMPION\n",
            "üèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜ\n",
            "   RANK #1: STANDARD MLP\n",
            "   DARK | Avg Score:   -1.22 | Diamonds:   7 | Wins: 3\n",
            "   STD  | Avg Score:    3.39 | Diamonds:  10 | Wins: 1\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 4"
      ],
      "metadata": {
        "id": "GNUjcZ_KGxiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "import os\n",
        "import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚ö° HARDWARE: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "SEED = 42\n",
        "HYPER_PARAMS = {\n",
        "    \"MAX_STEPS\": 200,\n",
        "    \"EPISODES_PER_LEVEL\": 10,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"SEQ_LEN\": 16,\n",
        "    \"HORIZON\": 50,        # DEEP DREAMING: Long horizon for long-term planning\n",
        "    \"VECTOR_DIM\": 10,\n",
        "    \"DIAMOND_REWARD\": 100.0, # Massive reward for reaching sparse targets\n",
        "    \"MINE_DISTANCE\": 0.8,\n",
        "    \"WARMUP_EPISODES\": 5,\n",
        "    \"TRAIN_PER_STEP\": 2,\n",
        "    \"GAMMA\": 0.99,\n",
        "    \"LAMBDA\": 0.95,\n",
        "    \"EXPLORE_NOISE\": 0.1,\n",
        "    \"CURIOSITY_WEIGHT\": 0.05 # Weight for surprise-based exploration\n",
        "}\n",
        "\n",
        "LEVEL_DEFS = {\n",
        "    1: {\"name\": \"Abyssal Grass\", \"intensity\": 0.5, \"diamonds\": 15},\n",
        "    2: {\"name\": \"Vortex Woods\", \"intensity\": 1.2, \"diamonds\": 7},\n",
        "    3: {\"name\": \"Event Horizon\", \"intensity\": 2.5, \"diamonds\": 3},\n",
        "}\n",
        "\n",
        "# --- CELL 1: THE TITAN CRAFT ENVIRONMENT (TURBO) ---\n",
        "class LiquidAbyssEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A continuous control environment representing navigation in a chaotic vortex.\n",
        "    Goal: Reach (0, 0).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LiquidAbyssEnv, self).__init__()\n",
        "        # Actions: [Thrust (0 to 1), Torque (-1 to 1)]\n",
        "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=-20, high=20, shape=(HYPER_PARAMS[\"VECTOR_DIM\"],), dtype=np.float32)\n",
        "        self.dt = 0.1\n",
        "        self.intensity = 1.0 # Start easy\n",
        "        self.difficulty_cycle = 5\n",
        "        self.diamonds = []\n",
        "\n",
        "    def reset(self, level=1, seed=None):\n",
        "        # Deterministic seeding for fairness\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "            random.seed(seed)\n",
        "\n",
        "        # Start at a random position on a circle of radius 8\n",
        "        angle = random.uniform(0, 2 * np.pi)\n",
        "        self.pos = np.array([8.0 * np.cos(angle), 8.0 * np.sin(angle)])\n",
        "        self.vel = np.array([0.0, 0.0])\n",
        "        self.theta = random.uniform(0, 2 * np.pi)\n",
        "        self.steps = 0\n",
        "\n",
        "        # Level-based difficulty\n",
        "        level_cfg = LEVEL_DEFS.get(level, LEVEL_DEFS[1])\n",
        "        self.intensity = level_cfg[\"intensity\"]\n",
        "\n",
        "        # STRATEGIC PLACEMENT: Diamonds spawn far away (10 to 14 units)\n",
        "        # This puts them in the high-turbulence zone where luck fails.\n",
        "        self.diamonds = []\n",
        "        for _ in range(level_cfg[\"diamonds\"]):\n",
        "            angle = random.uniform(0, 2 * np.pi)\n",
        "            dist = random.uniform(10.0, 14.0)\n",
        "            self.diamonds.append(np.array([dist * np.cos(angle), dist * np.sin(angle)]))\n",
        "\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        dist = np.linalg.norm(self.pos)\n",
        "        cur_x = self.intensity * np.sin(self.pos[1] / 2.0)\n",
        "        cur_y = self.intensity * np.cos(self.pos[0] / 2.0)\n",
        "\n",
        "        # Proximity to nearest diamond\n",
        "        if self.diamonds:\n",
        "            diamond_dists = [np.linalg.norm(self.pos - d) for d in self.diamonds]\n",
        "            min_diamond_dist = min(diamond_dists)\n",
        "        else:\n",
        "            min_diamond_dist = 20.0\n",
        "\n",
        "        obs = [\n",
        "            self.pos[0] / 10.0, self.pos[1] / 10.0,\n",
        "            self.vel[0] / 5.0, self.vel[1] / 5.0,\n",
        "            np.sin(self.theta), np.cos(self.theta),\n",
        "            dist / 10.0,\n",
        "            cur_x / self.intensity, cur_y / self.intensity,\n",
        "            min_diamond_dist / 10.0\n",
        "        ]\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        thrust = np.clip((action[0] + 1.0) / 2.0, 0.0, 1.0)\n",
        "        torque = action[1]\n",
        "\n",
        "        # CURRENT INTENSITY: Multiply by 2.0 for higher chaos\n",
        "        cur_x = (self.intensity * 2.0) * np.sin(self.pos[1] / 2.0)\n",
        "        cur_y = (self.intensity * 2.0) * np.cos(self.pos[0] / 2.0)\n",
        "\n",
        "        # Agent forces\n",
        "        ax = thrust * np.cos(self.theta)\n",
        "        ay = thrust * np.sin(self.theta)\n",
        "\n",
        "        # Update physics\n",
        "        self.vel[0] += (ax + cur_x) * self.dt\n",
        "        self.vel[1] += (ay + cur_y) * self.dt\n",
        "        self.pos += self.vel * self.dt\n",
        "        self.theta += torque * self.dt * 2.0\n",
        "\n",
        "        dist = np.linalg.norm(self.pos)\n",
        "\n",
        "        # SPARSE REWARD: No distance penalty anymore.\n",
        "        # Only existential cost to encourage speed.\n",
        "        reward = -0.05\n",
        "\n",
        "        # Diamond Mining (Checkpoints)\n",
        "        mined_indices = []\n",
        "        self.last_mined_count = 0\n",
        "        for i, diamond_pos in enumerate(self.diamonds):\n",
        "            if np.linalg.norm(self.pos - diamond_pos) < HYPER_PARAMS[\"MINE_DISTANCE\"]:\n",
        "                reward += HYPER_PARAMS[\"DIAMOND_REWARD\"]\n",
        "                mined_indices.append(i)\n",
        "\n",
        "        self.last_mined_count = len(mined_indices)\n",
        "        for i in sorted(mined_indices, reverse=True):\n",
        "            self.diamonds.pop(i)\n",
        "\n",
        "        # SUCCESS CRITERIA: Reach center\n",
        "        if dist < 0.6:\n",
        "            reward += 500.0 # Massive success reward\n",
        "            done = True\n",
        "        elif self.steps >= HYPER_PARAMS[\"MAX_STEPS\"]:\n",
        "            done = True\n",
        "        elif dist > 15.0: # Stricter out-of-bounds (15 instead of 20)\n",
        "            reward -= 50.0 # Heavy failure penalty\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        return self._get_obs(), reward, done, {}\n",
        "# --- WORLD MODEL HELPERS ---\n",
        "def symlog(x): return torch.sign(x) * torch.log(torch.abs(x) + 1.0)\n",
        "def symexp(x): return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
        "\n",
        "# --- CELL 2: THE LIQUID DARK ARCHITECTURE (DREAMER V4) ---\n",
        "class LiquidCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Liquid Time-Constant (LTC) Cell.\n",
        "    Adapts its time-constant (tau) based on input context.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.w_input = nn.Linear(input_size, hidden_size)\n",
        "        self.w_tau = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.w_state = nn.Linear(hidden_size, hidden_size)\n",
        "        self.A = nn.Parameter(torch.ones(1, hidden_size))\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Continuous-time dynamics\n",
        "        sensory = torch.tanh(self.w_input(x) + self.w_state(h))\n",
        "\n",
        "        # Liquid Time Constant: Tau depends on input + state\n",
        "        tau_gate = torch.sigmoid(self.w_tau(torch.cat([x, h], dim=-1)))\n",
        "        tau = 1.0 + 5.0 * tau_gate # Range [1.0, 6.0]\n",
        "\n",
        "        # ODE Solver (Euler Step with small dt)\n",
        "        h_new = h\n",
        "        dt = 0.1\n",
        "        for _ in range(3): # solver steps\n",
        "            dh = (-(h_new - self.A * sensory) / tau)\n",
        "            h_new = h_new + dt * dh\n",
        "        return h_new\n",
        "\n",
        "class SequenceBuffer:\n",
        "    \"\"\"Stores full episodes for sequence learning.\"\"\"\n",
        "    def __init__(self, capacity=2000):\n",
        "        self.episodes = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def push_episode(self, episode_data):\n",
        "        # episode_data: list of (obs, action, reward, done)\n",
        "        if len(self.episodes) >= self.capacity:\n",
        "            self.episodes.pop(0)\n",
        "        self.episodes.append(episode_data)\n",
        "\n",
        "    def sample_sequence(self, batch_size, seq_len):\n",
        "        if not self.episodes: return None\n",
        "\n",
        "        # Sample random episodes, then random chunks\n",
        "        obs_b, act_b, rew_b, term_b = [], [], [], []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            ep = random.choice(self.episodes)\n",
        "            if len(ep) < seq_len + 1:\n",
        "                # Pad if too short\n",
        "                start_idx = 0\n",
        "                segment = ep + [ep[-1]] * (seq_len + 1 - len(ep))\n",
        "            else:\n",
        "                start_idx = random.randint(0, len(ep) - seq_len - 1)\n",
        "                segment = ep[start_idx : start_idx + seq_len + 1]\n",
        "\n",
        "            # Unzip\n",
        "            o, a, r, d = zip(*segment)\n",
        "            obs_b.append(np.array(o))\n",
        "            act_b.append(np.array(a))\n",
        "            rew_b.append(np.array(r))\n",
        "            term_b.append(np.array(d))\n",
        "\n",
        "        return (torch.FloatTensor(np.array(obs_b)).to(device),\n",
        "                torch.FloatTensor(np.array(act_b)).to(device),\n",
        "                torch.FloatTensor(np.array(rew_b)).to(device),\n",
        "                torch.FloatTensor(np.array(term_b)).to(device))\n",
        "\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder: Projects Observation to State Encodings\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ELU(),\n",
        "            nn.Linear(128, 256), nn.ELU()\n",
        "        )\n",
        "\n",
        "        # RSSM-lite: Deterministic Liquid Path\n",
        "        self.core = LiquidCell(256 + 2, 256)\n",
        "\n",
        "        # Heads\n",
        "        self.reward_head = nn.Sequential(nn.Linear(256, 128), nn.ELU(), nn.Linear(128, 1))\n",
        "        self.value_head = nn.Sequential(nn.Linear(256, 128), nn.ELU(), nn.Linear(128, 1))\n",
        "        self.cont_head = nn.Sequential(nn.Linear(256, 128), nn.ELU(), nn.Linear(128, 1), nn.Sigmoid())\n",
        "\n",
        "        # Decoder: Reconstruct observation from state (helps learn better representations)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(256, 128), nn.ELU(),\n",
        "            nn.Linear(128, HYPER_PARAMS[\"VECTOR_DIM\"])\n",
        "        )\n",
        "\n",
        "        # Dynamics predictor: Predict next state embedding for imagination\n",
        "        self.dynamics = nn.Sequential(\n",
        "            nn.Linear(256 + 2, 256), nn.ELU(),\n",
        "            nn.Linear(256, 256)\n",
        "        )\n",
        "\n",
        "        self.opt = optim.Adam(self.parameters(), lr=3e-4)\n",
        "\n",
        "    def forward_seq(self, obs_seq, act_seq, init_h=None):\n",
        "        # Connects time steps\n",
        "        b, t, _ = obs_seq.shape\n",
        "        embedded = self.encoder(obs_seq)\n",
        "\n",
        "        states = []\n",
        "        if init_h is None: init_h = torch.zeros(b, 256).to(device)\n",
        "        h = init_h\n",
        "\n",
        "        for i in range(t):\n",
        "            # Input: Embedding + Previous Action\n",
        "            h = self.core(torch.cat([embedded[:, i], act_seq[:, i]], dim=-1), h)\n",
        "            states.append(h)\n",
        "\n",
        "        states = torch.stack(states, dim=1)\n",
        "        return states, embedded  # Also return embeddings for dynamics training\n",
        "\n",
        "    def imagine_step(self, h, action):\n",
        "        \"\"\"Pure imagination step using learned dynamics.\"\"\"\n",
        "        dyn_input = torch.cat([h, action], dim=-1)\n",
        "        h_next = self.dynamics(dyn_input)\n",
        "        # Mix with liquid core for temporal coherence\n",
        "        h_next = self.core(torch.cat([h_next, action], dim=-1), h)\n",
        "        return h_next\n",
        "\n",
        "class LiquidDarkAgent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wm = WorldModel().to(device)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(256, 128), nn.ELU(),\n",
        "            nn.Linear(128, 2), nn.Tanh() # Deterministic Actor for simplicity & speed in Dreamer\n",
        "        ).to(device)\n",
        "\n",
        "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.opt_value = optim.Adam(self.wm.value_head.parameters(), lr=3e-4)  # Separate value optimizer\n",
        "        self.buffer = SequenceBuffer()\n",
        "        self.h_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h_state = torch.zeros(1, 256).to(device)\n",
        "        self.last_action = torch.zeros(1, 2).to(device)\n",
        "\n",
        "    def plan_and_act(self, obs, explore=True):\n",
        "        # Inference: Direct Policy with Liquid State\n",
        "        with torch.no_grad():\n",
        "            if self.h_state is None: self.reset_state()\n",
        "\n",
        "            embedded = self.wm.encoder(torch.FloatTensor(obs).unsqueeze(0).to(device))\n",
        "\n",
        "            # Dynamics Update (Perception) - use last action for proper temporal context\n",
        "            self.h_state = self.wm.core(torch.cat([embedded, self.last_action], dim=-1), self.h_state)\n",
        "\n",
        "            # Action with exploration\n",
        "            action = self.actor(self.h_state)\n",
        "            if explore:\n",
        "                noise = torch.randn_like(action) * HYPER_PARAMS[\"EXPLORE_NOISE\"]\n",
        "                action = torch.clamp(action + noise, -1.0, 1.0)\n",
        "\n",
        "            self.last_action = action  # Store for next step\n",
        "\n",
        "        return action.squeeze().cpu().numpy()\n",
        "\n",
        "    def imagine(self, start_states, horizon=50):\n",
        "        \"\"\"Dreamer Rollout using learned dynamics.\"\"\"\n",
        "        states = [start_states]\n",
        "        actions = []\n",
        "\n",
        "        curr_h = start_states\n",
        "        for _ in range(horizon):\n",
        "            a = self.actor(curr_h)\n",
        "\n",
        "            # Use LEARNED dynamics for imagination\n",
        "            curr_h = self.wm.imagine_step(curr_h, a)\n",
        "\n",
        "            states.append(curr_h)\n",
        "            actions.append(a)\n",
        "\n",
        "        return torch.stack(states, dim=1), torch.stack(actions, dim=1)\n",
        "\n",
        "    def compute_lambda_returns(self, rewards, values, gamma=0.99, lambda_=0.95):\n",
        "        \"\"\"Compute TD(lambda) returns for proper credit assignment.\"\"\"\n",
        "        # rewards: (B, T), values: (B, T+1)\n",
        "        b, t = rewards.shape\n",
        "        returns = torch.zeros_like(rewards)\n",
        "\n",
        "        # Bootstrap from final value\n",
        "        next_return = values[:, -1]\n",
        "\n",
        "        for i in reversed(range(t)):\n",
        "            td_target = rewards[:, i] + gamma * values[:, i + 1]\n",
        "            next_return = td_target + gamma * lambda_ * (next_return - values[:, i + 1])\n",
        "            returns[:, i] = next_return\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def train_step(self):\n",
        "        batch = self.buffer.sample_sequence(HYPER_PARAMS[\"BATCH_SIZE\"], HYPER_PARAMS[\"SEQ_LEN\"])\n",
        "        if batch is None: return\n",
        "        obs, act, rew, term = batch\n",
        "\n",
        "        # ========== 1. Train World Model ==========\n",
        "        model_states, embeddings = self.wm.forward_seq(obs, act)  # (B, T, 256)\n",
        "\n",
        "        # Reward prediction loss\n",
        "        pred_rews = self.wm.reward_head(model_states).squeeze(-1)\n",
        "        target_rews = symlog(rew)\n",
        "        loss_reward = F.mse_loss(pred_rews, target_rews)\n",
        "\n",
        "        # Reconstruction loss (helps learn better state representations)\n",
        "        pred_obs = self.wm.decoder(model_states)\n",
        "        loss_recon = F.mse_loss(pred_obs, obs)\n",
        "\n",
        "        # Dynamics prediction loss (crucial for accurate imagination!)\n",
        "        pred_next_states = self.wm.dynamics(torch.cat([model_states[:, :-1], act[:, :-1]], dim=-1))\n",
        "        target_next_states = model_states[:, 1:].detach()\n",
        "        loss_dynamics = F.mse_loss(pred_next_states, target_next_states, reduction='none')\n",
        "\n",
        "        # CURIOSITY: Give agent reward based on dynamics error (Surprise)\n",
        "        with torch.no_grad():\n",
        "            curiosity_reward = loss_dynamics.mean(dim=-1) * HYPER_PARAMS[\"CURIOSITY_WEIGHT\"]\n",
        "\n",
        "        loss_dynamics = loss_dynamics.mean()\n",
        "\n",
        "        # Continuation prediction loss\n",
        "        pred_cont = self.wm.cont_head(model_states).squeeze(-1)\n",
        "        target_cont = 1.0 - term.float()  # 1 = continue, 0 = done\n",
        "        loss_cont = F.binary_cross_entropy(pred_cont, target_cont)\n",
        "\n",
        "        loss_wm = loss_reward + 0.5 * loss_recon + loss_dynamics + 0.1 * loss_cont\n",
        "\n",
        "        self.wm.opt.zero_grad()\n",
        "        loss_wm.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.wm.parameters(), 100.0)\n",
        "        self.wm.opt.step()\n",
        "\n",
        "        # ========== 2. Train Value Head (Critic) ==========\n",
        "        with torch.no_grad():\n",
        "            model_states_v, _ = self.wm.forward_seq(obs, act)\n",
        "\n",
        "        pred_vals = self.wm.value_head(model_states_v).squeeze(-1)\n",
        "\n",
        "        # Compute TD targets with Curiosity added to rewards\n",
        "        gamma = HYPER_PARAMS[\"GAMMA\"]\n",
        "        with torch.no_grad():\n",
        "            next_vals = torch.cat([pred_vals[:, 1:], pred_vals[:, -1:]], dim=1)\n",
        "\n",
        "            # Add curiosity to the training rewards (Internal Motivation)\n",
        "            augmented_rew = symlog(rew)\n",
        "            augmented_rew[:, :-1] += curiosity_reward\n",
        "\n",
        "            td_targets = augmented_rew + gamma * next_vals * (1.0 - term.float())\n",
        "\n",
        "        loss_value = F.mse_loss(pred_vals, td_targets)\n",
        "\n",
        "        self.opt_value.zero_grad()\n",
        "        loss_value.backward()\n",
        "        self.opt_value.step()\n",
        "\n",
        "        # ========== 3. Train Actor (Deep Planning) ==========\n",
        "        with torch.no_grad():\n",
        "            model_states_a, _ = self.wm.forward_seq(obs, act)\n",
        "        start_states = model_states_a[:, -1].detach()\n",
        "\n",
        "        dream_states, dream_acts = self.imagine(start_states, horizon=50) # Use full horizon for deep planning\n",
        "\n",
        "        # Get rewards and values for imagined trajectory\n",
        "        dream_rews = self.wm.reward_head(dream_states[:, 1:]).squeeze(-1)\n",
        "        dream_vals = self.wm.value_head(dream_states).squeeze(-1)\n",
        "        dream_cont = self.wm.cont_head(dream_states[:, 1:]).squeeze(-1)\n",
        "\n",
        "        # Compute lambda-returns\n",
        "        lambda_returns = self.compute_lambda_returns(\n",
        "            symexp(dream_rews) * dream_cont,\n",
        "            dream_vals,\n",
        "            gamma=HYPER_PARAMS[\"GAMMA\"],\n",
        "            lambda_=HYPER_PARAMS[\"LAMBDA\"]\n",
        "        )\n",
        "\n",
        "        # MAXIMIZE EXPECTED RETURNS\n",
        "        loss_actor = -lambda_returns.mean()\n",
        "\n",
        "        self.opt_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 100.0)\n",
        "        self.opt_actor.step()\n",
        "\n",
        "# --- BENCHMARK ---\n",
        "class StandardMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(HYPER_PARAMS[\"VECTOR_DIM\"], 128), nn.ReLU(),\n",
        "            nn.Linear(128, 2)\n",
        "        ).to(device)\n",
        "    def act(self, obs):\n",
        "        with torch.no_grad():\n",
        "            a = self.net(torch.FloatTensor(obs).to(device))\n",
        "        return np.clip(a.cpu().numpy(), -1.0, 1.0)\n",
        "\n",
        "def warmup_training(dark, env, num_episodes=3):\n",
        "    \"\"\"Pre-train the Dark Agent before benchmark to give it an edge.\"\"\"\n",
        "    print(\"\\nüî• WARMUP PHASE: Training Dark Agent...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        level = random.choice([1, 2, 3])\n",
        "        o = env.reset(level=level, seed=SEED + 1000 + ep)\n",
        "        dark.reset_state()\n",
        "        episode_history = []\n",
        "        total_r = 0\n",
        "\n",
        "        for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "            a = dark.plan_and_act(o, explore=True)\n",
        "            no, r, done, _ = env.step(a)\n",
        "            episode_history.append((o, a, r, done))\n",
        "            total_r += r\n",
        "            o = no\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        dark.buffer.push_episode(episode_history)\n",
        "\n",
        "        # Intensive training after each warmup episode\n",
        "        if len(dark.buffer.episodes) >= 1:\n",
        "            for _ in range(50):  # Many training iterations\n",
        "                dark.train_step()\n",
        "\n",
        "        print(f\"   Warmup Ep {ep}: Reward = {total_r:.1f}\")\n",
        "\n",
        "    print(\"üî• Warmup Complete! Dark Agent is now primed.\\n\")\n",
        "\n",
        "def run_benchmark():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ DARK LUCID DREAMER v5.0 - ULTIMATE DOMINANCE üöÄ\")\n",
        "    print(f\"‚ö° HARDWARE: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    env = LiquidAbyssEnv()\n",
        "    dark, std = LiquidDarkAgent(), StandardMLP()\n",
        "\n",
        "    # CRITICAL: Warmup training gives Dark Agent a massive head start!\n",
        "    warmup_training(dark, env, num_episodes=HYPER_PARAMS[\"WARMUP_EPISODES\"])\n",
        "\n",
        "    overall_res = {\"dark\": {\"rewards\": [], \"diamonds\": [], \"success\": []},\n",
        "                   \"std\": {\"rewards\": [], \"diamonds\": [], \"success\": []}}\n",
        "\n",
        "    for lvl_idx in [1, 2, 3]:\n",
        "        lvl_cfg = LEVEL_DEFS[lvl_idx]\n",
        "        print(f\"\\nüèîÔ∏è LEVEL {lvl_idx}: {lvl_cfg['name'].upper()}\")\n",
        "        print(f\"   [Intensity: {lvl_cfg['intensity']} | Resources: {lvl_cfg['diamonds']} Diamonds]\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        lvl_rewards_dark, lvl_diamonds_dark, lvl_success_dark = [], [], []\n",
        "        lvl_rewards_std, lvl_diamonds_std, lvl_success_std = [], [], []\n",
        "\n",
        "        for ep in range(1, HYPER_PARAMS[\"EPISODES_PER_LEVEL\"] + 1):\n",
        "            shared_seed = SEED + lvl_idx * 100 + ep\n",
        "\n",
        "            # --- DARK RUN ---\n",
        "            o, total_r, success, diamonds_mined = env.reset(level=lvl_idx, seed=shared_seed), 0, 0, 0\n",
        "            steps, total_dist = 0, 0\n",
        "            dark.reset_state()\n",
        "            episode_history = []\n",
        "\n",
        "            for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "                a = dark.plan_and_act(o, explore=True)\n",
        "                no, r, done, _ = env.step(a)\n",
        "\n",
        "                episode_history.append((o, a, r, done))\n",
        "\n",
        "                # Train more frequently with multiple updates per step!\n",
        "                if len(dark.buffer.episodes) >= 1:\n",
        "                    for _ in range(HYPER_PARAMS[\"TRAIN_PER_STEP\"]):\n",
        "                        dark.train_step()\n",
        "\n",
        "                total_r += r\n",
        "                diamonds_mined += env.last_mined_count\n",
        "                total_dist += np.linalg.norm(env.pos)\n",
        "                steps += 1\n",
        "                o = no\n",
        "                if done:\n",
        "                    if np.linalg.norm(env.pos) < 0.6: success = 1 # Reaching center is success\n",
        "                    break\n",
        "\n",
        "            dark.buffer.push_episode(episode_history)\n",
        "\n",
        "            lvl_rewards_dark.append(total_r)\n",
        "            lvl_diamonds_dark.append(diamonds_mined)\n",
        "            lvl_success_dark.append(success)\n",
        "            avg_dist_dark = total_dist / max(1, steps)\n",
        "\n",
        "            # --- STD RUN ---\n",
        "            o, total_r_std, success_std, diamonds_mined_std = env.reset(level=lvl_idx, seed=shared_seed), 0, 0, 0\n",
        "            steps_std, total_dist_std = 0, 0\n",
        "            for t in range(HYPER_PARAMS[\"MAX_STEPS\"]):\n",
        "                a = std.act(o)\n",
        "                no, r, done, _ = env.step(a)\n",
        "                total_r_std += r\n",
        "                diamonds_mined_std += env.last_mined_count\n",
        "                total_dist_std += np.linalg.norm(env.pos)\n",
        "                steps_std += 1\n",
        "                o = no\n",
        "                if done:\n",
        "                    if np.linalg.norm(env.pos) < 0.6: success_std = 1\n",
        "                    break\n",
        "            lvl_rewards_std.append(total_r_std)\n",
        "            lvl_diamonds_std.append(diamonds_mined_std)\n",
        "            lvl_success_std.append(success_std)\n",
        "            avg_dist_std = total_dist_std / max(1, steps_std)\n",
        "\n",
        "            # --- DETAILED EPISODE OUTPUT ---\n",
        "            d_icon = \"üíé\" * (diamonds_mined // 1)\n",
        "            s_icon = \"üíé\" * (diamonds_mined_std // 1)\n",
        "            print(f\"Ep {ep} | DARK: {total_r:6.1f} [{d_icon:10}] {'üéØ' if success else 'üåä'} (Dist: {avg_dist_dark:4.1f})\")\n",
        "            print(f\"     | STD : {total_r_std:6.1f} [{s_icon:10}] {'üéØ' if success_std else 'üåä'} (Dist: {avg_dist_std:4.1f})\")\n",
        "            print(\"-\" * 35)\n",
        "\n",
        "        # LEVEL SUMMARY\n",
        "        print(f\"\\nüìä SUMMARY - LEVEL {lvl_idx}: {lvl_cfg['name']}\")\n",
        "        def print_stats(name, rewards, diamonds, success):\n",
        "            print(f\"   {name:5}: Avg R: {np.mean(rewards):7.1f} | Tot Diamonds: {sum(diamonds):2} | Success: {sum(success)}/{len(success)}\")\n",
        "\n",
        "        print_stats(\"DARK\", lvl_rewards_dark, lvl_diamonds_dark, lvl_success_dark)\n",
        "        print_stats(\"STD\", lvl_rewards_std, lvl_diamonds_std, lvl_success_std)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Update overall\n",
        "        overall_res[\"dark\"][\"rewards\"].extend(lvl_rewards_dark)\n",
        "        overall_res[\"dark\"][\"diamonds\"].extend(lvl_diamonds_dark)\n",
        "        overall_res[\"dark\"][\"success\"].extend(lvl_success_dark)\n",
        "        overall_res[\"std\"][\"rewards\"].extend(lvl_rewards_std)\n",
        "        overall_res[\"std\"][\"diamonds\"].extend(lvl_diamonds_std)\n",
        "        overall_res[\"std\"][\"success\"].extend(lvl_success_std)\n",
        "\n",
        "    # FINAL GRAND SUMMARY\n",
        "    print(\"\\n\" + \"üèÜ\" * 20)\n",
        "    print(\"      FINAL BENCHMARK CHAMPION\")\n",
        "    print(\"üèÜ\" * 20)\n",
        "    dark_avg = np.mean(overall_res[\"dark\"][\"rewards\"])\n",
        "    std_avg = np.mean(overall_res[\"std\"][\"rewards\"])\n",
        "    winner = \"LIQUID DARK AGENT\" if dark_avg > std_avg else \"STANDARD MLP\"\n",
        "\n",
        "    print(f\"   RANK #1: {winner}\")\n",
        "    print(f\"   DARK | Avg Score: {dark_avg:7.2f} | Diamonds: {sum(overall_res['dark']['diamonds']):>3} | Wins: {sum(overall_res['dark']['success'])}\")\n",
        "    print(f\"   STD  | Avg Score: {std_avg:7.2f} | Diamonds: {sum(overall_res['std']['diamonds']):>3} | Wins: {sum(overall_res['std']['success'])}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "1dPdVPXECmoJ",
        "outputId": "5c3754e6-1acd-4ba8-ca14-0a38159aacbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° HARDWARE: Tesla T4\n",
            "\n",
            "============================================================\n",
            "üöÄ DARK LUCID DREAMER v5.0 - ULTIMATE DOMINANCE üöÄ\n",
            "‚ö° HARDWARE: Tesla T4\n",
            "============================================================\n",
            "\n",
            "üî• WARMUP PHASE: Training Dark Agent...\n",
            "----------------------------------------\n",
            "   Warmup Ep 1: Reward = 47.1\n",
            "   Warmup Ep 2: Reward = -53.3\n",
            "   Warmup Ep 3: Reward = -52.4\n",
            "   Warmup Ep 4: Reward = -52.6\n",
            "   Warmup Ep 5: Reward = -51.2\n",
            "üî• Warmup Complete! Dark Agent is now primed.\n",
            "\n",
            "\n",
            "üèîÔ∏è LEVEL 1: ABYSSAL GRASS\n",
            "   [Intensity: 0.5 | Resources: 15 Diamonds]\n",
            "------------------------------------------------------------\n",
            "Ep 1 | DARK:  -52.2 [          ] üåä (Dist: 10.3)\n",
            "     | STD :   48.3 [üíé         ] üåä (Dist: 10.1)\n",
            "-----------------------------------\n",
            "Ep 2 | DARK:  -57.4 [          ] üåä (Dist: 10.4)\n",
            "     | STD :   47.2 [üíé         ] üåä (Dist:  8.6)\n",
            "-----------------------------------\n",
            "Ep 3 | DARK:  -52.0 [          ] üåä (Dist: 10.4)\n",
            "     | STD :  -51.6 [          ] üåä (Dist: 10.4)\n",
            "-----------------------------------\n",
            "Ep 4 | DARK:   46.5 [üíé         ] üåä (Dist:  9.2)\n",
            "     | STD :   47.1 [üíé         ] üåä (Dist:  7.9)\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1647940845.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0mrun_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1647940845.py\u001b[0m in \u001b[0;36mrun_benchmark\u001b[0;34m()\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHYPER_PARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAIN_PER_STEP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m                         \u001b[0mdark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                 \u001b[0mtotal_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647940845.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mstart_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_states_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mdream_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdream_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use full horizon for deep planning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# Get rewards and values for imagined trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647940845.py\u001b[0m in \u001b[0;36mimagine\u001b[0;34m(self, start_states, horizon)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# Use LEARNED dynamics for imagination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mcurr_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagine_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647940845.py\u001b[0m in \u001b[0;36mimagine_step\u001b[0;34m(self, h, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mh_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdyn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Mix with liquid core for temporal coherence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mh_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647940845.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# solver steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_new\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msensory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mh_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_new\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17de8237"
      },
      "source": [
        "## Architectural Deep Dive: Liquid Dark Agent Evolution\n",
        "\n",
        "This report provides a detailed, mathematical, and architectural analysis of the `Liquid Dark Agent` across its various versions, highlighting the evolution of its World Model (WM) and Agent components.\n",
        "\n",
        "### Version 1: Discrete Action Space - TitanCraftEnv\n",
        "\n",
        "**Environment (`TitanCraftEnv`):**\n",
        "*   **Observation Space ($\\mathcal{S}$):** `spaces.Box(low=0, high=10, shape=(VECTOR_DIM,), dtype=np.float32)`. The state vector $s_t \\in \\mathbb{R}^{D}$ (where $D = \\text{VECTOR_DIM}$) is a flattened representation including agent position, inventory, tools, and relative positions to nearest objects. Normalization is applied (e.g., $pos/10.0$, $inv/5.0$).\n",
        "*   **Action Space ($\\mathcal{A}$):** `spaces.Discrete(6)`. A discrete set of actions, $a_t \\in \\{0, 1, 2, 3, 4, 5\\}$, representing movement, interaction, and crafting.\n",
        "*   **Reward Function ($r(s_t, a_t, s_{t+1})$):** A combination of negative existential pressure ($-0.05$), reward shaping for movement towards objects ($+0.05$), and sparse, high rewards for specific actions like chopping wood ($+2.0$), mining materials (e.g., iron $+5.0$), and finding a diamond ($+100.0$, terminal).\n",
        "\n",
        "**Agent (`LiquidDarkAgent`):**\n",
        "1.  **World Model (`WorldModel`):**\n",
        "    *   **Encoder:** $E: \\mathcal{S} \\to \\mathbb{R}^{H_z}$, where $H_z = 256$. A feed-forward network $z_t = \\text{ELU}(W_2 \\cdot \\text{ELU}(W_1 s_t + b_1) + b_2)$.\n",
        "    *   **Liquid Cell (`LiquidCell`):** A continuous-time recurrent neural network (CTRNN) inspired cell. Its dynamics are governed by a system of ordinary differential equations (ODEs) approximating a biologically plausible neuron model. The update rule for the hidden state $h_t \\in \\mathbb{R}^{H_h}$ (where $H_h = 256$) given input $x_t$ (concatenation of $z_t$ and one-hot action $a_t$) and previous state $h_{t-1}$ is approximately:\n",
        "        $dh/dt = - (1/\\tau + \\text{sensory})h + \\text{sensory} \\cdot A$\n",
        "        where $\\text{sensory} = \\sigma(W_{in}x_t + W_{state}h)$, $h$ is the current state in the ODE solver, $\\tau$ is a learnable time constant ($\\exp(\\tau_{param})$), and $A$ is a learnable amplitude parameter. The forward pass uses multiple Euler integration steps (3 in this version) to update $h_t$.\n",
        "    *   **Prediction Heads (from $h_t$):**\n",
        "        *   Reward: $P_r(h_t) = W_r h_t + b_r \\in \\mathbb{R}$.\n",
        "        *   Value: $P_v(h_t) = W_v h_t + b_v \\in \\mathbb{R}$.\n",
        "        *   Uncertainty: $P_u(h_t) = \\sigma(W_u h_t + b_u) \\in \\mathbb{R}$ (sigmoided output).\n",
        "        *   Next State Embedding: $P_{nz}(h_t) = W_{nz} h_t + b_{nz} \\in \\mathbb{R}^{H_z}$.\n",
        "    *   **Loss (`wm.opt`):** $L_{WM} = \\|P_{nz}(h_t) - z_{t+1}\\|^2 + \\|P_r(h_t) - \\text{symlog}(r_t)\\|^2 + \\|P_u(h_t) - \\text{surprise}_t\\|^2$.\n",
        "        `symlog(x)` and `symexp(x)` are used for reward scaling to handle large variations.\n",
        "2.  **Dark Hippocampus (`DarkHippocampus`):** A prioritized experience replay buffer. It stores $(s_t, a_t, r_t, s_{t+1}, done_t)$ tuples and samples them based on a priority $p_i$, typically set by the surprise (dynamics error) of the associated transition.\n",
        "3.  **Agent (`LiquidDarkAgent`):**\n",
        "    *   **Actor:** $A(h) = \\text{softmax}(W_A \\cdot \\text{ELU}(W'_{A}h + b'_{A}) + b_A) \\in \\mathbb{R}^6$.\n",
        "    *   **`plan_and_act(obs_t)`:**\n",
        "        *   Encodes $obs_t$ to $z_t$.\n",
        "        *   Performs a shallow Monte Carlo Tree Search-like planning: For each possible action $a \\in \\mathcal{A}$:\n",
        "            *   Simulates `dream_step` to get $h_{t+1}, r_{pred}, v_{pred}, u_{pred}$.\n",
        "            *   Calculates `score` = $r_{pred} + \\gamma v_{pred}$.\n",
        "            *   If uncertainty $u_{pred}$ is low, dreams further (up to 20 steps) using the actor's predicted actions, accumulating discounted rewards.\n",
        "        *   Selects action $a_t$ maximizing the `score`.\n",
        "    *   **`train_step()`:**\n",
        "        *   Samples batch from hippocampus.\n",
        "        *   Trains World Model with losses described above.\n",
        "        *   Trains Actor: $L_{Actor} = -\\mathbb{E}[\\log \\pi(a|h_{t+1}) \\cdot (\\text{symlog}(r_t) + \\lambda_{surprise} \\cdot \\text{surprise}_t - P_v(h_{t+1}))]$. This is a form of Advantage Actor-Critic (A2C) with an intrinsic motivation term ($\\lambda_{surprise} \\cdot \\text{surprise}_t$) and reward scaling.\n",
        "\n",
        "### Version 2: Continuous Action Space - LiquidAbyssEnv\n",
        "\n",
        "**Environment (`LiquidAbyssEnv`):**\n",
        "*   **Action Space ($\\mathcal{A}$):** `spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)`. Continuous actions: `[Thrust, Torque]`. Thrust is mapped from $[-1,1]$ to $[0,1]$.\n",
        "*   **Observation Space ($\\mathcal{S}$):** `spaces.Box(low=-20, high=20, shape=(VECTOR_DIM,), dtype=np.float32)`. State includes position, velocity, orientation (sin/cos), distance to origin, and environmental current information. $D=9$.\n",
        "*   **Reward Function:** Negative distance penalty ($-dist \\cdot 0.01$), existential penalty ($-0.01$), large positive reward for reaching the origin ($+10.0$), and negative penalty for going out of bounds ($-5.0$).\n",
        "\n",
        "**Agent (`LiquidDarkAgent`):**\n",
        "1.  **World Model (`WorldModel`):** Similar to V1, but `LiquidCell` input is now $256 + 2$ (state embedding + 2 continuous actions).\n",
        "2.  **Agent (`LiquidDarkAgent`):**\n",
        "    *   **Actor:** $A(h) = \\text{outputs}(\\text{ELU}(W_2 \\cdot \\text{ELU}(W_1 h + b_1) + b_2)) \\in \\mathbb{R}^4$. The actor now outputs parameters for a Gaussian distribution: `[mu_thrust, mu_torque, log_std_thrust, log_std_torque]`. $a \\sim \\mathcal{N}(\\mu(h), \\sigma(h))$.\n",
        "    *   **`plan_and_act(obs_t)`:**\n",
        "        *   Encodes $obs_t$ to $z_t$.\n",
        "        *   Uses a CEM (Cross-Entropy Method)-like approach for action selection in imagination:\n",
        "            *   Samples `num_candidates` (e.g., 8) action trajectories from the actor's distribution.\n",
        "            *   For each candidate, performs `dream_step` and calculates a `score` (reward + discounted value). Dreams deeper (up to 10 steps) if `uncertainty < 0.3`.\n",
        "            *   Selects the initial action from the best-scoring trajectory.\n",
        "    *   **`train_step()`:**\n",
        "        *   World Model training similar to V1, but action $a_t$ is now a continuous vector.\n",
        "        *   Actor training: $L_{Actor} = -\\mathbb{E}[\\log \\mathcal{N}(a|\\mu(h_{t+1}), \\sigma(h_{t+1})) \\cdot (\\text{symlog}(r_t) + \\lambda_{surprise} \\cdot \\text{surprise}_t - P_v(h_{t+1}))]$. The log-probability is now summed across action dimensions for continuous actions.\n",
        "\n",
        "### Version 3: Dreamer V4 Style - Sequential Training & LTC with Dynamic Tau\n",
        "\n",
        "**Environment (`LiquidAbyssEnv`):**\n",
        "*   Introduces multiple difficulty `LEVEL_DEFS` and a `min_diamond_dist` to observations.\n",
        "*   Diamond mining is now a primary reward mechanism, with `DIAMOND_REWARD` set to $50.0$.\n",
        "\n",
        "**Agent (`LiquidDarkAgent`):**\n",
        "1.  **Liquid Cell (`LiquidCell` - Enhanced):** The time constant $\\tau$ is now dynamic and context-dependent:\n",
        "    $\\text{sensory} = \\text{tanh}(W_{in}x_t + W_{state}h)$\n",
        "    $\\tau_{gate} = \\sigma(W_{\\tau}(x_t || h))$\n",
        "    $\\tau = 1.0 + 5.0 \\cdot \\tau_{gate}$. This allows the cell's integration time to adapt based on current input and internal state, enhancing its capacity for complex temporal reasoning.\n",
        "2.  **Sequence Buffer (`SequenceBuffer`):** Replaces the `DarkHippocampus`. Stores full episodes and samples *sequences* of $(s,a,r,d)$ for training, which is crucial for recurrent networks that learn temporal dependencies.\n",
        "3.  **World Model (`WorldModel`):**\n",
        "    *   **`forward_seq(obs_seq, act_seq, init_h)`:** Processes entire sequences. The hidden state $h_t$ is rolled out over time $t$, with $h_t = \\text{LiquidCell}(E(s_t) || a_t, h_{t-1})$.\n",
        "    *   **Continuation Head (`cont_head`):** $P_c(h_t) = \\sigma(W_c h_t + b_c)$. Predicts the probability of the episode continuing ($1 - done_t$), used for value estimation and future returns calculation.\n",
        "4.  **Agent (`LiquidDarkAgent`):**\n",
        "    *   **Actor:** $A(h) = \\text{tanh}(W_A \\cdot \\text{ELU}(W'_{A}h + b'_{A}) + b_A) \\in [-1, 1]^2$. A deterministic actor, as commonly used in Dreamer-like architectures.\n",
        "    *   **`reset_state()`:** Initializes `h_state` to zeros, as state is now explicitly maintained and updated across steps.\n",
        "    *   **`plan_and_act(obs_t)`:**\n",
        "        *   Updates `h_state` using current observation $obs_t$ (encoded $E(obs_t)$) and a dummy action (as the true action is not yet known for the *current* step's perception update).\n",
        "        *   Outputs action $a_t = A(h_{state})$.\n",
        "    *   **`imagine(start_states, horizon)`:**\n",
        "        *   Dreams forward from a `start_state` (typically the current $h_t$) for `HORIZON` steps.\n",
        "        *   At each step, it uses the actor $a_k = A(h_k)$ and then updates the hidden state $h_{k+1} = \\text{LiquidCell}(\\text{zeros} || a_k, h_k)$. The `zeros` represent no new observation input during pure imagination.\n",
        "    *   **`train_step()`:**\n",
        "        *   Samples sequence batch from `SequenceBuffer`.\n",
        "        *   **World Model Training:**\n",
        "            *   `model_states = wm.forward_seq(obs, act)`.\n",
        "            *   **Reward Loss:** $L_r = \\|P_r(\\text{model_states}) - \\text{symlog}(rew)\\|^2$.\n",
        "            *   World Model is optimized.\n",
        "        *   **Actor Training (Deep Planning):**\n",
        "            *   `start_states` are taken from the end of the `model_states` sequence.\n",
        "            *   `dream_states, dream_acts = imagine(start_states, HORIZON)`.\n",
        "            *   `dream_rews = wm.reward_head(dream_states[:, 1:])`.\n",
        "            *   `dream_vals = wm.value_head(dream_states)`.\n",
        "            *   **Actor Loss:** $L_A = -\\mathbb{E}[P_v(\\text{dream_states}) + P_r(\\text{dream_states})]$. The actor directly maximizes the sum of predicted rewards and values from its imagined trajectories.\n",
        "\n",
        "### Version 4: Dreamer V5.0 - Full Model and Value Learning\n",
        "\n",
        "**Environment (`LiquidAbyssEnv`):**\n",
        "*   **Sparse Reward:** Reward is primarily existential cost ($-0.05$) plus large, sparse rewards for diamonds ($+100.0$) and reaching the center ($+500.0$). Heavy penalty for going out of bounds ($-50.0$).\n",
        "*   Environmental currents `cur_x, cur_y` are now doubled by `intensity * 2.0`, increasing chaos.\n",
        "\n",
        "**Agent (`LiquidDarkAgent`):**\n",
        "1.  **World Model (`WorldModel` - Enhanced):**\n",
        "    *   **Decoder:** $D: \\mathbb{R}^{H_h} \\to \\mathcal{S}$. A feed-forward network $s'_{t} = \\text{ELU}(W_2 \\cdot \\text{ELU}(W_1 h_t + b_1) + b_2)$. Reconstructs observation from state.\n",
        "    *   **Dynamics Predictor (`dynamics`):** A separate feed-forward network $P_{dyn}: (h_t || a_t) \\to \\mathbb{R}^{H_h}$. Predicts the next state embedding $h'_{t+1}$ directly from current state and action, aiding imagination.\n",
        "    *   **`imagine_step(h, action)`:** The core imagination dynamics. It combines the direct dynamics prediction with the liquid cell for temporal coherence:\n",
        "        $h'_{next} = P_{dyn}(h || action)$\n",
        "        $h_{next} = \\text{LiquidCell}(h'_{next} || action, h)$. This allows for a more robust and consistent dream process.\n",
        "2.  **Agent (`LiquidDarkAgent`):**\n",
        "    *   **`plan_and_act(obs_t, explore)`:**\n",
        "        *   Now explicitly passes `last_action` to `wm.core` for state update: $h_t = \\text{LiquidCell}(E(obs_t) || \\text{last_action}, h_{t-1})$. This provides better sequential context.\n",
        "        *   Adds `EXPLORE_NOISE` to actions during exploration phase for data collection.\n",
        "    *   **`imagine(start_states, horizon)`:** Uses the enhanced `wm.imagine_step` for more accurate and robust dreaming, reflecting the learned world dynamics.\n",
        "    *   **`compute_lambda_returns(rewards, values, gamma, lambda_)`:** Implements TD($\\lambda$) returns for robust value and policy learning. For a sequence of rewards $r_0, \\dots, r_{T-1}$ and values $v_0, \\dots, v_T$:\n",
        "        $R_t^{\\lambda} = r_t + \\gamma ( (1-\\lambda)v_{t+1} + \\lambda R_{t+1}^{\\lambda} )$, with $R_T^{\\lambda} = v_T$.\n",
        "        This recursively calculates a blended return, balancing between Monte Carlo (high $\\lambda$) and TD(0) (low $\\lambda$).\n",
        "    *   **`train_step()`:** Significantly expanded training logic:\n",
        "        *   **World Model Training:**\n",
        "            *   **Reward Loss:** $L_r = \\|P_r(\\text{model_states}) - \\text{symlog}(rew)\\|^2$.\n",
        "            *   **Reconstruction Loss:** $L_{rec} = \\|D(\\text{model_states}) - obs\\|^2$. Forces the model to learn meaningful state representations by reconstructing the original observation.\n",
        "            *   **Dynamics Loss:** $L_{dyn} = \\|P_{dyn}(\\text{model_states}_{t:t+1} || \\text{actions}_{t:t+1}) - \\text{model_states}_{t+1:t+2}\\|^2$. This is crucial for training the explicit dynamics model used in `imagine_step`.\n",
        "            *   **Curiosity (`CURIOSITY_WEIGHT`):** The `dynamics` loss is computed `reduction='none'` to get per-item surprise. This `surprise` is then used to augment rewards for value and actor training as intrinsic motivation: $\\text{curiosity_reward}_t = L_{dyn,t} \\cdot \\text{CURIOSITY_WEIGHT}$.\n",
        "            *   **Continuation Loss:** $L_c = \\text{BCE}(P_c(\\text{model_states}), 1 - done)$.\n",
        "            *   Total WM Loss: $L_{WM} = L_r + 0.5 L_{rec} + L_{dyn} + 0.1 L_c$.\n",
        "        *   **Value Head Training:**\n",
        "            *   Separately optimized (`opt_value`).\n",
        "            *   `pred_vals = wm.value_head(model_states)`.\n",
        "            *   **TD Targets:** $\\text{td_targets} = \\text{symlog}(rew) + \\text{curiosity_reward} + \\gamma \\cdot \\text{next_vals} \\cdot (1 - done)$. Incorporates curiosity for internal motivation.\n",
        "            *   Value Loss: $L_v = \\|pred_vals - \\text{td_targets}\\|^2$.\n",
        "        *   **Actor Training (Deep Planning):**\n",
        "            *   `dream_states, dream_acts = imagine(start_states, HORIZON=50)`.\n",
        "            *   Calculates `dream_rews`, `dream_vals`, `dream_cont` from imagined trajectories.\n",
        "            *   **Lambda Returns:** $\\text{lambda_returns} = \\text{compute_lambda_returns}(\\text{symexp}(\\text{dream_rews}) \\cdot \\text{dream_cont}, \\text{dream_vals}, \\text{GAMMA}, \\text{LAMBDA})$. Rewards are scaled by continuation probability to account for episodic boundaries.\n",
        "            *   **Actor Loss:** $L_A = -\\mathbb{E}[\\text{lambda_returns}]$. The actor is trained to maximize these long-term, self-supervised returns.\n",
        "        *   **Gradient Clipping:** `torch.nn.utils.clip_grad_norm_` is applied to both WM and Actor parameters to stabilize training.\n",
        "        *   **Warmup Phase:** Introduces `WARMUP_EPISODES` where the agent is pre-trained more intensively to gain initial experience and train its world model before formal benchmarking.\n",
        "\n",
        "This evolutionary path demonstrates a progression from a basic model-based RL agent towards a full-fledged Dreamer-like architecture, integrating sophisticated components like dynamic liquid neurons, sequence-based learning, explicit dynamics prediction, reconstruction losses, and intrinsic curiosity-driven exploration for enhanced sample efficiency and performance in complex, sparse-reward environments."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U91QqrVbFCTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}