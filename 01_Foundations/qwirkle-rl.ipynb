{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:30:36.707153Z","iopub.execute_input":"2025-12-13T14:30:36.707434Z","iopub.status.idle":"2025-12-13T14:30:36.712066Z","shell.execute_reply.started":"2025-12-13T14:30:36.707414Z","shell.execute_reply":"2025-12-13T14:30:36.711366Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\nimport zipfile\nimport io\nimport math\nimport time\nfrom collections import deque\nfrom copy import deepcopy\n\n# ============================================================================\n# 1. CONSTANTS & TILE CLASS\n# ============================================================================\nCOLORS = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']\nSHAPES = ['circle', 'square', 'diamond', 'star', 'clover', 'cross']\n\nclass Tile:\n    def __init__(self, color, shape):\n        self.color = color\n        self.shape = shape\n    def __eq__(self, other): return self.color == other.color and self.shape == other.shape\n    def __hash__(self): return hash((self.color, self.shape))\n    def __repr__(self): return f\"{self.color[0].upper()}{self.shape[0].upper()}\"\n\n# ============================================================================\n# 2. QWIRKLE ENVIRONMENT (Headless)\n# ============================================================================\nclass Qwirkle:\n    def __init__(self, num_players=2):\n        self.num_players = num_players\n        self.reset()\n    \n    def reset(self):\n        self.bag = []\n        for color in COLORS:\n            for shape in SHAPES:\n                for _ in range(3): self.bag.append(Tile(color, shape))\n        random.shuffle(self.bag)\n        self.board = {}\n        self.hands = [self._draw_tiles(6) for _ in range(self.num_players)]\n        self.current_player = 0\n        self.scores = [0] * self.num_players\n        self.game_over = False\n        self.winner = None\n        self.passes = 0\n        return self.get_state()\n    \n    def _draw_tiles(self, count):\n        drawn = []\n        for _ in range(min(count, len(self.bag))):\n            if self.bag: drawn.append(self.bag.pop())\n        return drawn\n    \n    def get_state(self):\n        # Simplified state for Q-Table hashing\n        state_data = {\n            'board_size': len(self.board),\n            'hand_size': len(self.hands[self.current_player]),\n            'tiles_left': len(self.bag),\n            'score_diff': self.scores[self.current_player] - max([s for i, s in enumerate(self.scores) if i != self.current_player], default=0)\n        }\n        return tuple(sorted(state_data.items()))\n    \n    def get_available_actions(self):\n        actions = []\n        hand = self.hands[self.current_player]\n        if not self.board:\n            # First move logic\n            for i in range(len(hand)):\n                actions.append(('place', [(0, 0, i)]))\n                for j in range(i + 1, len(hand)):\n                    if self._tiles_compatible([hand[i], hand[j]]):\n                        actions.append(('place', [(0, 0, i), (0, 1, j)]))\n        else:\n            # Standard move logic\n            empty_neighbors = self._get_empty_neighbors()\n            for pos in empty_neighbors:\n                for idx, tile in enumerate(hand):\n                    if self._is_valid_placement(pos, tile):\n                        actions.append(('place', [(pos[0], pos[1], idx)]))\n            # Limited multi-tile check for speed\n            if len(hand) >= 2:\n                for pos1 in list(empty_neighbors)[:5]: \n                    for idx1, tile1 in enumerate(hand[:4]):\n                        if self._is_valid_placement(pos1, tile1):\n                            for dr, dc in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n                                pos2 = (pos1[0] + dr, pos1[1] + dc)\n                                if pos2 not in self.board:\n                                    for idx2, tile2 in enumerate(hand[:4]):\n                                        if idx2 != idx1 and self._tiles_compatible([tile1, tile2]):\n                                            actions.append(('place', [(pos1[0], pos1[1], idx1), (pos2[0], pos2[1], idx2)]))\n        actions.append(('trade', []))\n        return actions[:50] # Limit actions to prevent memory explosion\n    \n    def _get_empty_neighbors(self):\n        neighbors = set()\n        for (r, c) in self.board:\n            for dr, dc in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n                if (r + dr, c + dc) not in self.board: neighbors.add((r + dr, c + dc))\n        return neighbors\n    \n    def _tiles_compatible(self, tiles):\n        if len(tiles) <= 1: return True\n        colors = set(t.color for t in tiles); shapes = set(t.shape for t in tiles)\n        return (len(colors) == 1 and len(shapes) == len(tiles)) or (len(shapes) == 1 and len(colors) == len(tiles))\n\n    def _is_valid_placement(self, pos, tile):\n        r, c = pos\n        if (r, c) in self.board: return False\n        adj = []\n        for dr, dc in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n            if (r + dr, c + dc) in self.board: adj.append(self.board[(r+dr, c+dc)])\n        if not adj and len(self.board) > 0: return False\n        \n        # Horizontal check\n        h_tiles = [tile]; cc = c - 1\n        while (r, cc) in self.board: h_tiles.insert(0, self.board[(r, cc)]); cc -= 1\n        cc = c + 1\n        while (r, cc) in self.board: h_tiles.append(self.board[(r, cc)]); cc += 1\n        if len(h_tiles) > 1 and not self._tiles_compatible(h_tiles): return False\n        \n        # Vertical check\n        v_tiles = [tile]; rr = r - 1\n        while (rr, c) in self.board: v_tiles.insert(0, self.board[(rr, c)]); rr -= 1\n        rr = r + 1\n        while (rr, c) in self.board: v_tiles.append(self.board[(rr, c)]); rr += 1\n        if len(v_tiles) > 1 and not self._tiles_compatible(v_tiles): return False\n        \n        if len(h_tiles) > 6 or len(v_tiles) > 6: return False\n        return True\n\n    def make_move(self, action):\n        if self.game_over: return self.get_state(), 0, True\n        action_type, data = action\n        reward = 0\n        if action_type == 'place':\n            placed = []\n            for r, c, idx in data:\n                tile = self.hands[self.current_player][idx]\n                self.board[(r, c)] = tile\n                placed.append((r, c, idx))\n            reward = self._calculate_score(placed)\n            self.scores[self.current_player] += reward\n            for _, _, idx in sorted(placed, key=lambda x: x[2], reverse=True):\n                self.hands[self.current_player].pop(idx)\n            self.hands[self.current_player].extend(self._draw_tiles(len(placed)))\n            self.passes = 0\n        elif action_type == 'trade':\n            traded = self.hands[self.current_player][:]\n            self.hands[self.current_player] = []\n            self.bag.extend(traded); random.shuffle(self.bag)\n            self.hands[self.current_player] = self._draw_tiles(len(traded))\n            reward = -2\n            self.passes += 1\n        \n        if (not self.hands[self.current_player] and not self.bag) or self.passes >= 4:\n            self.game_over = True\n            if not self.hands[self.current_player]: self.scores[self.current_player] += 6\n            self.winner = self.scores.index(max(self.scores))\n            return self.get_state(), reward + 50, True\n            \n        self.current_player = (self.current_player + 1) % self.num_players\n        return self.get_state(), reward, False\n\n    def _calculate_score(self, placed):\n        score = 0; scored = set()\n        for r, c, _ in placed:\n            for dr, dc in [(0, 1), (1, 0)]:\n                line = set([(r, c)])\n                for d in [-1, 1]:\n                    curr_r, curr_c = r + d*dr, c + d*dc\n                    while (curr_r, curr_c) in self.board:\n                        line.add((curr_r, curr_c))\n                        curr_r += d*dr; curr_c += d*dc\n                if len(line) > 1:\n                    ls = len(line) + (6 if len(line) == 6 else 0)\n                    if not line.issubset(scored): score += ls; scored.update(line)\n        return score if score > 0 else len(placed)\n\n    def evaluate_position(self, player):\n        if self.winner == player: return 100000\n        if self.winner is not None: return -100000\n        score = (self.scores[player] - max([self.scores[i] for i in range(2) if i!=player], default=0)) * 100\n        score += len(self.hands[player]) * 10\n        return score\n\n# ============================================================================\n# 3. AGENT CLASSES (MCTS + Strategic Agent)\n# ============================================================================\nclass MCTSNode:\n    def __init__(self, state, parent=None, action=None):\n        self.state = state; self.parent = parent; self.action = action\n        self.children = []; self.visits = 0; self.value = 0.0; self.untried_actions = None\n    def uct(self, c=1.41):\n        if self.visits == 0: return float('inf')\n        return self.value / self.visits + c * math.sqrt(math.log(self.parent.visits) / self.visits)\n    def best_child(self): return max(self.children, key=lambda c: c.uct())\n    def most_visited(self): return max(self.children, key=lambda c: c.visits)\n\nclass StrategicQwirkleAgent:\n    def __init__(self, pid, lr=0.1, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, minimax_depth=0):\n        self.pid = pid; self.lr = lr; self.gamma = gamma\n        self.epsilon = epsilon; self.epsilon_decay = epsilon_decay; self.epsilon_min = 0.05\n        self.minimax_depth = minimax_depth; self.mcts_simulations = 50\n        self.q_table = {}; self.wins = 0; self.losses = 0; self.draws = 0\n        self.total_score = 0; self.games_played = 0\n\n    def get_q(self, state, action): return self.q_table.get((state, str(action)), 0.0)\n\n    def choose_action(self, env, training=True):\n        acts = env.get_available_actions()\n        if not acts: return None\n        \n        # 1. Minimax\n        if self.minimax_depth > 0:\n            _, best = self._minimax(env, self.minimax_depth, -float('inf'), float('inf'), True)\n            return best if best else random.choice(acts)\n        \n        # 2. Epsilon Greedy\n        if training and random.random() < self.epsilon: return random.choice(acts)\n        \n        # 3. MCTS\n        return self._mcts(env, acts)\n\n    def _minimax(self, env, depth, alpha, beta, maxing):\n        if depth == 0 or env.game_over: return env.evaluate_position(self.pid), None\n        acts = env.get_available_actions()[:5] # Limit breadth\n        best_act = random.choice(acts) if acts else None\n        \n        if maxing:\n            max_eval = -float('inf')\n            for a in acts:\n                sim = self._copy(env); sim.make_move(a)\n                val, _ = self._minimax(sim, depth-1, alpha, beta, False)\n                if val > max_eval: max_eval = val; best_act = a\n                alpha = max(alpha, val)\n                if beta <= alpha: break\n            return max_eval, best_act\n        else:\n            min_eval = float('inf')\n            for a in acts:\n                sim = self._copy(env); sim.make_move(a)\n                val, _ = self._minimax(sim, depth-1, alpha, beta, True)\n                if val < min_eval: min_eval = val; best_act = a\n                beta = min(beta, val)\n                if beta <= alpha: break\n            return min_eval, best_act\n\n    def _mcts(self, env, acts):\n        root = MCTSNode(env.get_state()); root.untried_actions = acts[:]\n        for _ in range(self.mcts_simulations):\n            node = root; sim = self._copy(env)\n            while not node.untried_actions and node.children:\n                node = node.best_child(); \n                if node.action: sim.make_move(node.action)\n            if node.untried_actions:\n                a = random.choice(node.untried_actions); node.untried_actions.remove(a)\n                sim.make_move(a); child = MCTSNode(sim.get_state(), node, a)\n                node.children.append(child); node = child\n            \n            # Rollout\n            rw = 0; steps = 0\n            while not sim.game_over and steps < 20:\n                ac = sim.get_available_actions()\n                if not ac: break\n                _, r, _ = sim.make_move(random.choice(ac))\n                rw += r; steps += 1\n            if sim.game_over and sim.winner == self.pid: rw += 100\n            \n            # Backprop\n            while node: node.visits+=1; node.value+=rw; node=node.parent\n            \n        return root.most_visited().action if root.children else acts[0]\n\n    def _copy(self, env):\n        new = Qwirkle(env.num_players); new.board = env.board.copy()\n        new.hands = [h[:] for h in env.hands]; new.bag = env.bag[:]\n        new.current_player = env.current_player; new.scores = env.scores[:]\n        new.game_over = env.game_over; new.winner = env.winner; new.passes = env.passes\n        return new\n\n    def update_q(self, state, action, reward, next_state, next_acts):\n        k = str(action); curr = self.get_q(state, action)\n        mx = max([self.get_q(next_state, a) for a in next_acts[:10]], default=0) if next_acts else 0\n        self.q_table[(state, k)] = curr + self.lr * (reward + self.gamma * mx - curr)\n\n    def decay_epsilon(self): self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    def reset_stats(self): self.wins=0; self.losses=0; self.total_score=0; self.games_played=0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:30:36.721707Z","iopub.execute_input":"2025-12-13T14:30:36.722028Z","iopub.status.idle":"2025-12-13T14:30:36.818052Z","shell.execute_reply.started":"2025-12-13T14:30:36.722012Z","shell.execute_reply":"2025-12-13T14:30:36.817437Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ============================================================================\n# 1. SERIALIZATION HELPERS\n# ============================================================================\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer): return int(obj)\n        if isinstance(obj, np.floating): return float(obj)\n        if isinstance(obj, np.ndarray): return obj.tolist()\n        return super(NumpyEncoder, self).default(obj)\n\ndef serialize_q_table(q_table):\n    # Matches Streamlit: \"JSON_List_State|String_Action\"\n    serialized_q = {}\n    for (state, action_key), value in q_table.items():\n        state_str = json.dumps(list(state))\n        key_str = f\"{state_str}|{action_key}\"\n        serialized_q[key_str] = float(value)\n    return serialized_q\n\ndef save_kaggle_brain(agent1, agent2, config, filename=\"qwirkle_brains.zip\"):\n    a1_data = {\n        \"q_table\": serialize_q_table(agent1.q_table),\n        \"epsilon\": float(agent1.epsilon),\n        \"wins\": int(agent1.wins), \"losses\": int(agent1.losses),\n        \"total_score\": int(agent1.total_score),\n        \"games_played\": int(agent1.games_played)\n    }\n    a2_data = {\n        \"q_table\": serialize_q_table(agent2.q_table),\n        \"epsilon\": float(agent2.epsilon),\n        \"wins\": int(agent2.wins), \"losses\": int(agent2.losses),\n        \"total_score\": int(agent2.total_score),\n        \"games_played\": int(agent2.games_played)\n    }\n    \n    print(f\"üíæ Saving: A1 Q-States={len(agent1.q_table)}, A2 Q-States={len(agent2.q_table)}\")\n    with zipfile.ZipFile(filename, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(a1_data, cls=NumpyEncoder, indent=2))\n        zf.writestr(\"agent2.json\", json.dumps(a2_data, cls=NumpyEncoder, indent=2))\n        zf.writestr(\"config.json\", json.dumps(config, cls=NumpyEncoder, indent=2))\n    print(f\"‚úÖ Save Complete! Download '{filename}'\")\n\n# ============================================================================\n# 2. TRAINING RUNNER\n# ============================================================================\ndef play_game(env, agent1, agent2):\n    env.reset()\n    agents = [agent1, agent2]\n    moves = 0\n    while not env.game_over and moves < 150:\n        cp = env.current_player; ag = agents[cp]\n        s = env.get_state(); a = ag.choose_action(env)\n        if not a: break\n        ns, r, done = env.make_move(a)\n        ag.update_q(s, a, r, ns, env.get_available_actions())\n        moves += 1\n        if done:\n            for i, ax in enumerate(agents):\n                ax.games_played += 1\n                ax.total_score += env.scores[i]\n                if env.winner == i: ax.wins += 1\n                elif env.winner is not None: ax.losses += 1\n                else: ax.draws += 1\n    return env.winner\n\ndef run_qwirkle_training():\n    # --- CONFIGURATION ---\n    EPISODES = 50       # Qwirkle is complex, 500-1000 is good for a start\n    \n    # Initialize Environment & Agents\n    # Note: Depth=0 uses MCTS (default), Depth>0 uses Minimax\n    env = Qwirkle()\n    agent1 = StrategicQwirkleAgent(0, lr=0.1, gamma=0.95, epsilon_decay=0.999, minimax_depth=4)\n    agent2 = StrategicQwirkleAgent(1, lr=0.1, gamma=0.95, epsilon_decay=0.999, minimax_depth=4)\n    \n    # Increase MCTS sims for training logic\n    agent1.mcts_simulations = 50\n    agent2.mcts_simulations = 50\n    \n    # --- FIX: Initialize History as Lists of Lists/Scalars ---\n    history = {\n        'agent1_wins': [], 'agent2_wins': [], 'draws': [],\n        'agent1_epsilon': [], 'agent2_epsilon': [],\n        'agent1_q_size': [], 'agent2_q_size': [],\n        'agent1_avg_score': [], 'agent2_avg_score': [],\n        'episode': []\n    }\n    \n    print(f\"üöÄ Starting Qwirkle Training ({EPISODES} Episodes)...\")\n    start_time = time.time()\n    \n    for ep in range(1, EPISODES + 1):\n        play_game(env, agent1, agent2)\n        \n        agent1.decay_epsilon()\n        agent2.decay_epsilon()\n        \n        # Log every 10 episodes to keep history size manageable but detailed\n        if ep % 10 == 0:\n            history['agent1_wins'].append(agent1.wins)\n            history['agent2_wins'].append(agent2.wins)\n            history['draws'].append(agent1.draws)\n            history['agent1_epsilon'].append(agent1.epsilon)\n            history['agent2_epsilon'].append(agent2.epsilon)\n            history['agent1_q_size'].append(len(agent1.q_table))\n            history['agent2_q_size'].append(len(agent2.q_table))\n            history['agent1_avg_score'].append(agent1.total_score / max(1, agent1.games_played))\n            history['agent2_avg_score'].append(agent2.total_score / max(1, agent2.games_played))\n            history['episode'].append(ep)\n        \n        if ep % 1 == 0:\n            elapsed = time.time() - start_time\n            print(f\"Ep {ep}/{EPISODES} | A1 Wins: {agent1.wins} | Avg Score: {agent1.total_score/ep:.1f} | {elapsed:.1f}s\")\n\n    # Save Config\n    config = {\n        \"lr1\": agent1.lr, \"gamma1\": agent1.gamma, \"epsilon_decay1\": agent1.epsilon_decay,\n        \"mcts_sims1\": agent1.mcts_simulations, \"depth1\": agent1.minimax_depth,\n        \"lr2\": agent2.lr, \"gamma2\": agent2.gamma, \"epsilon_decay2\": agent2.epsilon_decay,\n        \"mcts_sims2\": agent2.mcts_simulations, \"depth2\": agent2.minimax_depth,\n        \"training_history\": history\n    }\n    \n    print(\"\\nüèÜ Training Finished!\")\n    save_kaggle_brain(agent1, agent2, config)\n\nif __name__ == \"__main__\":\n    run_qwirkle_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:30:36.819077Z","iopub.execute_input":"2025-12-13T14:30:36.819332Z","iopub.status.idle":"2025-12-13T14:38:32.048729Z","shell.execute_reply.started":"2025-12-13T14:30:36.819311Z","shell.execute_reply":"2025-12-13T14:38:32.047913Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting Qwirkle Training (50 Episodes)...\nEp 1/50 | A1 Wins: 0 | Avg Score: 144.0 | 9.7s\nEp 2/50 | A1 Wins: 1 | Avg Score: 158.0 | 18.9s\nEp 3/50 | A1 Wins: 1 | Avg Score: 165.0 | 29.3s\nEp 4/50 | A1 Wins: 2 | Avg Score: 165.2 | 39.2s\nEp 5/50 | A1 Wins: 3 | Avg Score: 164.2 | 49.0s\nEp 6/50 | A1 Wins: 4 | Avg Score: 167.2 | 58.7s\nEp 7/50 | A1 Wins: 4 | Avg Score: 166.3 | 68.5s\nEp 8/50 | A1 Wins: 5 | Avg Score: 166.0 | 79.4s\nEp 9/50 | A1 Wins: 6 | Avg Score: 148.2 | 79.5s\nEp 10/50 | A1 Wins: 7 | Avg Score: 150.6 | 88.6s\nEp 11/50 | A1 Wins: 7 | Avg Score: 151.6 | 98.4s\nEp 12/50 | A1 Wins: 8 | Avg Score: 152.1 | 107.9s\nEp 13/50 | A1 Wins: 8 | Avg Score: 151.8 | 117.6s\nEp 14/50 | A1 Wins: 8 | Avg Score: 152.1 | 127.6s\nEp 15/50 | A1 Wins: 9 | Avg Score: 153.2 | 137.4s\nEp 16/50 | A1 Wins: 9 | Avg Score: 153.3 | 147.4s\nEp 17/50 | A1 Wins: 9 | Avg Score: 152.8 | 156.2s\nEp 18/50 | A1 Wins: 10 | Avg Score: 153.9 | 165.8s\nEp 19/50 | A1 Wins: 11 | Avg Score: 154.4 | 176.0s\nEp 20/50 | A1 Wins: 11 | Avg Score: 154.8 | 185.2s\nEp 21/50 | A1 Wins: 12 | Avg Score: 155.7 | 194.8s\nEp 22/50 | A1 Wins: 12 | Avg Score: 155.7 | 204.4s\nEp 23/50 | A1 Wins: 13 | Avg Score: 156.0 | 213.7s\nEp 24/50 | A1 Wins: 14 | Avg Score: 156.4 | 222.7s\nEp 25/50 | A1 Wins: 15 | Avg Score: 156.7 | 231.2s\nEp 26/50 | A1 Wins: 16 | Avg Score: 157.4 | 240.8s\nEp 27/50 | A1 Wins: 16 | Avg Score: 157.6 | 249.9s\nEp 28/50 | A1 Wins: 16 | Avg Score: 157.2 | 260.2s\nEp 29/50 | A1 Wins: 16 | Avg Score: 157.1 | 270.7s\nEp 30/50 | A1 Wins: 17 | Avg Score: 157.4 | 280.8s\nEp 31/50 | A1 Wins: 17 | Avg Score: 157.4 | 292.4s\nEp 32/50 | A1 Wins: 18 | Avg Score: 158.2 | 302.3s\nEp 33/50 | A1 Wins: 18 | Avg Score: 157.6 | 312.4s\nEp 34/50 | A1 Wins: 19 | Avg Score: 158.0 | 322.8s\nEp 35/50 | A1 Wins: 20 | Avg Score: 158.6 | 332.9s\nEp 36/50 | A1 Wins: 21 | Avg Score: 158.8 | 342.3s\nEp 37/50 | A1 Wins: 21 | Avg Score: 158.8 | 351.2s\nEp 38/50 | A1 Wins: 22 | Avg Score: 159.2 | 361.1s\nEp 39/50 | A1 Wins: 23 | Avg Score: 159.6 | 370.7s\nEp 40/50 | A1 Wins: 24 | Avg Score: 159.6 | 380.6s\nEp 41/50 | A1 Wins: 24 | Avg Score: 159.2 | 390.1s\nEp 42/50 | A1 Wins: 25 | Avg Score: 159.4 | 399.3s\nEp 43/50 | A1 Wins: 26 | Avg Score: 159.6 | 408.7s\nEp 44/50 | A1 Wins: 27 | Avg Score: 159.7 | 417.4s\nEp 45/50 | A1 Wins: 27 | Avg Score: 159.4 | 427.7s\nEp 46/50 | A1 Wins: 27 | Avg Score: 159.4 | 438.1s\nEp 47/50 | A1 Wins: 28 | Avg Score: 159.7 | 447.2s\nEp 48/50 | A1 Wins: 28 | Avg Score: 159.8 | 455.8s\nEp 49/50 | A1 Wins: 29 | Avg Score: 159.7 | 465.4s\nEp 50/50 | A1 Wins: 30 | Avg Score: 160.0 | 475.2s\n\nüèÜ Training Finished!\nüíæ Saving: A1 Q-States=2588, A2 Q-States=2576\n‚úÖ Save Complete! Download 'qwirkle_brains.zip'\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}