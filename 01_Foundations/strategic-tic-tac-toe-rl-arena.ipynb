{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:04:06.271171Z","iopub.execute_input":"2025-12-13T15:04:06.271406Z","iopub.status.idle":"2025-12-13T15:04:08.412718Z","shell.execute_reply.started":"2025-12-13T15:04:06.271385Z","shell.execute_reply":"2025-12-13T15:04:08.411823Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\nimport zipfile\nimport io\nimport time\nfrom collections import deque\n\n# ============================================================================\n# 1. TIC-TAC-TOE ENVIRONMENT (Exact Copy)\n# ============================================================================\nclass TicTacToe:\n    def __init__(self, grid_size=3, win_length=None):\n        self.grid_size = grid_size\n        self.win_length = win_length if win_length else grid_size\n        self.reset()\n    \n    def reset(self):\n        self.board = np.zeros((self.grid_size, self.grid_size), dtype=int)\n        self.current_player = 1\n        self.game_over = False\n        self.winner = None\n        self.move_history = []\n        return self.get_state()\n    \n    def get_state(self):\n        return tuple(self.board.flatten())\n    \n    def get_available_actions(self):\n        actions = [(r, c) for r in range(self.grid_size) \n                   for c in range(self.grid_size) if self.board[r, c] == 0]\n        \n        # --- TOURNAMENT RULE FIX (Crucial for 4x4) ---\n        if len(self.move_history) == 0:\n            center = self.grid_size // 2\n            if self.grid_size % 2 == 1:\n                if (center, center) in actions: actions.remove((center, center))\n            else:\n                forbidden = [(center-1, center-1), (center-1, center), \n                             (center, center-1), (center, center)]\n                actions = [a for a in actions if a not in forbidden]\n        return actions\n\n    def make_move(self, position):\n        if self.game_over: return self.get_state(), 0, True\n        i, j = position\n        if self.board[i, j] != 0: return self.get_state(), -10, True\n        \n        self.board[i, j] = self.current_player\n        self.move_history.append((position, self.current_player))\n        \n        if self._check_win(self.current_player):\n            self.game_over = True; self.winner = self.current_player\n            return self.get_state(), 100, True\n        \n        if len(self.get_available_actions()) == 0:\n            self.game_over = True; self.winner = 0 \n            # Defender Bonus\n            if self.current_player == 2: return self.get_state(), 50, True \n            else: return self.get_state(), 0, True \n\n        self.current_player = 3 - self.current_player\n        return self.get_state(), 0, False\n    \n    def _check_win(self, player):\n        board = self.board; n = self.grid_size; w = self.win_length\n        for r in range(n):\n            for c in range(n - w + 1):\n                if np.all(board[r, c:c+w] == player): return True\n        for r in range(n - w + 1):\n            for c in range(n):\n                if np.all(board[r:r+w, c] == player): return True\n        for r in range(n - w + 1):\n            for c in range(n - w + 1):\n                if np.all([board[r+k, c+k] == player for k in range(w)]): return True\n                if np.all([board[r+k, c+w-1-k] == player for k in range(w)]): return True\n        return False\n    \n    def evaluate_position(self, player):\n        if self.winner == player: return 100000\n        if self.winner == (3 - player): return -100000\n        if self.game_over: return 0 \n        \n        opponent = 3 - player\n        score = 0\n        is_large_grid = self.grid_size > 3\n        is_defensive_agent = (player == 2)\n        \n        # IRON WALL Strategy weights\n        if is_large_grid and is_defensive_agent:\n            threat_2_weight = 400; threat_win_minus_1 = 8000\n            my_attack_weight = 10; center_control_penalty = 200\n        else:\n            threat_2_weight = 60; threat_win_minus_1 = 1000\n            my_attack_weight = 50; center_control_penalty = 50\n\n        # Center Control\n        center = self.grid_size // 2\n        centers = []\n        if self.grid_size % 2 == 0:\n            centers = [(center-1, center-1), (center-1, center), (center, center-1), (center, center)]\n        else: centers = [(center, center)]\n\n        for r, c in centers:\n            if self.board[r, c] == player: score += 50\n            elif self.board[r, c] == opponent: \n                score -= 50\n                if is_defensive_agent: score -= center_control_penalty\n\n        # Corner Control\n        corners = [(0,0), (0, self.grid_size-1), (self.grid_size-1, 0), (self.grid_size-1, self.grid_size-1)]\n        for r,c in corners:\n            if self.board[r,c] == player: score += 10\n            elif self.board[r,c] == opponent: score -= 10\n\n        # Simplified heuristics for training speed\n        return score\n\n# ============================================================================\n# 2. STRATEGIC AGENT\n# ============================================================================\nclass StrategicAgent:\n    def __init__(self, player_id, lr=0.2, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n        self.player_id = player_id; self.lr = lr; self.gamma = gamma\n        self.epsilon = epsilon; self.epsilon_decay = epsilon_decay; self.epsilon_min = epsilon_min\n        self.q_table = {}\n        self.minimax_depth = 4\n        self.wins = 0; self.losses = 0; self.draws = 0\n    \n    def get_q_value(self, state, action):\n        return self.q_table.get((state, action), 0.0)\n    \n    def choose_action(self, env, training=True):\n        available_actions = env.get_available_actions()\n        if not available_actions: return None\n        \n        # LEVEL 1: Survival\n        for action in available_actions:\n            sim = self._simulate_move(env, action, self.player_id)\n            if sim.winner == self.player_id: return action\n        opponent = 3 - self.player_id\n        for action in available_actions:\n            sim = self._simulate_move(env, action, opponent)\n            if sim.winner == opponent: return action \n\n        # LEVEL 2: Strategic Planning\n        if training and random.random() < self.epsilon:\n            return random.choice(available_actions)\n\n        # Dynamic Depth Logic\n        empty_spots = len(available_actions)\n        depth_bonus = 2 if (env.grid_size > 1 and self.player_id == 2) else 0\n        current_depth = min(self.minimax_depth + depth_bonus, empty_spots)\n        if env.grid_size == 3: current_depth = 9\n\n        best_score = -float('inf'); best_actions = []\n        center = env.grid_size // 2\n        available_actions.sort(key=lambda x: abs(x[0]-center) + abs(x[1]-center))\n        alpha = -float('inf'); beta = float('inf')\n\n        for action in available_actions:\n            sim_env = self._simulate_move(env, action, self.player_id)\n            score = self._minimax(sim_env, current_depth - 1, alpha, beta, False)\n            # Q-Table Tiebreaker\n            q_boost = self.get_q_value(env.get_state(), action) * 0.01\n            total_score = score + q_boost\n\n            if total_score > best_score:\n                best_score = total_score; best_actions = [action]\n            elif total_score == best_score:\n                best_actions.append(action)\n            alpha = max(alpha, best_score)\n        \n        return random.choice(best_actions) if best_actions else random.choice(available_actions)\n\n    def _minimax(self, env, depth, alpha, beta, is_maximizing):\n        if env.winner == self.player_id: return 1000 + depth\n        if env.winner == (3 - self.player_id): return -1000 - depth\n        if env.game_over: return 0 \n        if depth == 0: return env.evaluate_position(self.player_id)\n        available_actions = env.get_available_actions()\n        if is_maximizing:\n            max_eval = -float('inf')\n            for action in available_actions:\n                sim_env = self._simulate_move(env, action, self.player_id)\n                eval = self._minimax(sim_env, depth - 1, alpha, beta, False)\n                max_eval = max(max_eval, eval); alpha = max(alpha, eval)\n                if beta <= alpha: break\n            return max_eval\n        else:\n            min_eval = float('inf'); opponent = 3 - self.player_id\n            for action in available_actions:\n                sim_env = self._simulate_move(env, action, opponent)\n                eval = self._minimax(sim_env, depth - 1, alpha, beta, True)\n                min_eval = min(min_eval, eval); beta = min(beta, eval)\n                if beta <= alpha: break\n            return min_eval\n\n    def _simulate_move(self, env, action, player):\n        sim = TicTacToe(env.grid_size, env.win_length)\n        sim.board = env.board.copy(); sim.current_player = player\n        sim.make_move(action)\n        return sim\n\n    def update_q_value(self, state, action, reward, next_state, next_available_actions):\n        current_q = self.get_q_value(state, action)\n        max_next = max([self.get_q_value(next_state, a) for a in next_available_actions]) if next_available_actions else 0\n        self.q_table[(state, action)] = current_q + self.lr * (reward + self.gamma * max_next - current_q)\n\n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    \n    def reset_stats(self):\n        self.wins = 0; self.losses = 0; self.draws = 0\n\ndef play_game(env, agent1, agent2, training=True):\n    env.reset(); history = []\n    agents = {1: agent1, 2: agent2}\n    while not env.game_over:\n        cp = env.current_player; ag = agents[cp]\n        s = env.get_state(); a = ag.choose_action(env, training)\n        if a is None: break\n        history.append((s, a, cp))\n        ns, r, d = env.make_move(a)\n        if training: ag.update_q_value(s, a, r, ns, env.get_available_actions())\n        if d:\n            if env.winner == 1:\n                agent1.wins+=1; agent2.losses+=1\n                if training: _update_outcome(agent1, history, 1, 100); _update_outcome(agent2, history, 2, -50)\n            elif env.winner == 2:\n                agent2.wins+=1; agent1.losses+=1\n                if training: _update_outcome(agent1, history, 1, -50); _update_outcome(agent2, history, 2, 100)\n            else:\n                agent1.draws+=1; agent2.draws+=1\n                if training: _update_outcome(agent1, history, 1, -5); _update_outcome(agent2, history, 2, -5)\n    return env.winner\n\ndef _update_outcome(agent, history, pid, reward):\n    moves = [(s, a) for s, a, p in history if p == pid]\n    for i in range(len(moves)-1, -1, -1):\n        s, a = moves[i]\n        disc = agent.gamma ** (len(moves)-1-i)\n        curr = agent.get_q_value(s, a)\n        agent.q_table[(s, a)] = curr + agent.lr * (reward * disc - curr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:04:08.414323Z","iopub.execute_input":"2025-12-13T15:04:08.414681Z","iopub.status.idle":"2025-12-13T15:04:08.518945Z","shell.execute_reply.started":"2025-12-13T15:04:08.414660Z","shell.execute_reply":"2025-12-13T15:04:08.518030Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# 1. SERIALIZATION (Matches Streamlit Inputs EXACTLY)\n# ============================================================================\ndef serialize_q_table(q_table):\n    \"\"\"Safe serialization for JSON keys\"\"\"\n    serialized_q = {}\n    for (state, action), value in q_table.items():\n        state_list = [int(x) for x in state]\n        action_list = [int(x) for x in action]\n        # Exact key format: JSON string of Tuple(List, List)\n        key_str = json.dumps((state_list, action_list))\n        serialized_q[key_str] = float(value)\n    return serialized_q\n\ndef save_kaggle_brain(agent1, agent2, config, filename=\"agi_agents.zip\"):\n    # Prepare Data Dictionaries\n    agent1_state = {\n        \"q_table\": serialize_q_table(agent1.q_table),\n        \"epsilon\": agent1.epsilon, \"lr\": agent1.lr, \n        \"gamma\": agent1.gamma, \"wins\": agent1.wins, \n        \"losses\": agent1.losses, \"draws\": agent1.draws\n    }\n    agent2_state = {\n        \"q_table\": serialize_q_table(agent2.q_table),\n        \"epsilon\": agent2.epsilon, \"lr\": agent2.lr, \n        \"gamma\": agent2.gamma, \"wins\": agent2.wins, \n        \"losses\": agent2.losses, \"draws\": agent2.draws\n    }\n    \n    print(f\"ðŸ’¾ Saving: A1 Q-States={len(agent1.q_table)}, A2 Q-States={len(agent2.q_table)}\")\n    with zipfile.ZipFile(filename, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(agent1_state))\n        zf.writestr(\"agent2.json\", json.dumps(agent2_state))\n        zf.writestr(\"config.json\", json.dumps(config))\n    print(f\"âœ… Save Complete! Download '{filename}'\")\n\n# ============================================================================\n# 2. TRAINING RUNNER: SERIOUS MODE\n# ============================================================================\ndef run_serious_training():\n    # --- CONFIGURATION FOR SERIOUS 4x4 PLAY ---\n    EPISODES = 200      # 10k episodes for 4x4 complexity\n    GRID_SIZE = 4\n    WIN_LENGTH = 3        # As you requested\n    \n    print(f\"ðŸš€ Starting SERIOUS Mode Training ({GRID_SIZE}x{GRID_SIZE}, Win {WIN_LENGTH})\")\n    print(f\"ðŸŽ¯ Target: {EPISODES} Episodes...\")\n    \n    # Initialize Environment\n    env = TicTacToe(GRID_SIZE, WIN_LENGTH)\n    \n    # Initialize Agents (Optimized for Larger Grid)\n    agent1 = StrategicAgent(1, lr=0.2, gamma=0.95, epsilon_decay=0.9995)\n    agent2 = StrategicAgent(2, lr=0.2, gamma=0.95, epsilon_decay=0.9995)\n    \n    # Increase Thinking Depth for Training\n    agent1.minimax_depth = 5\n    agent2.minimax_depth = 5\n    \n    # --- FIX: Initialize History Lists ---\n    history = {\n        'agent1_wins': [], 'agent2_wins': [], 'draws': [],\n        'agent1_epsilon': [], 'agent2_epsilon': [],\n        'agent1_q_size': [], 'agent2_q_size': [],\n        'episode': []\n    }\n    \n    start_time = time.time()\n    \n    # Training Loop\n    for ep in range(1, EPISODES + 1):\n        play_game(env, agent1, agent2, training=True)\n        \n        agent1.decay_epsilon()\n        agent2.decay_epsilon()\n        \n        # Record stats every 50 episodes (prevents huge file size)\n        if ep % 50 == 0:\n            history['agent1_wins'].append(agent1.wins)\n            history['agent2_wins'].append(agent2.wins)\n            history['draws'].append(agent1.draws)\n            history['agent1_epsilon'].append(agent1.epsilon)\n            history['agent2_epsilon'].append(agent2.epsilon)\n            history['agent1_q_size'].append(len(agent1.q_table))\n            history['agent2_q_size'].append(len(agent2.q_table))\n            history['episode'].append(ep)\n        \n        if ep % 10 == 0:\n            elapsed = time.time() - start_time\n            print(f\"Ep {ep}/{EPISODES} | P1 Wins: {agent1.wins} | P2 Wins: {agent2.wins} | Îµ={agent1.epsilon:.4f} | {elapsed:.1f}s\")\n            \n    # Save Final Brain with FULL history\n    config = {\n        \"grid_size\": GRID_SIZE,\n        \"win_length\": WIN_LENGTH,\n        \"lr1\": agent1.lr, \"gamma1\": agent1.gamma,\n        \"lr2\": agent2.lr, \"gamma2\": agent2.gamma,\n        \"minimax_depth1\": agent1.minimax_depth,\n        \"minimax_depth2\": agent2.minimax_depth,\n        \"training_history\": history  # The fix is here!\n    }\n    \n    print(\"\\nðŸ† Training Finished!\")\n    save_kaggle_brain(agent1, agent2, config)\n\nif __name__ == \"__main__\":\n    run_serious_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:04:08.520324Z","iopub.execute_input":"2025-12-13T15:04:08.521113Z","iopub.status.idle":"2025-12-13T15:09:59.106127Z","shell.execute_reply.started":"2025-12-13T15:04:08.521056Z","shell.execute_reply":"2025-12-13T15:09:59.105239Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting SERIOUS Mode Training (4x4, Win 3)\nðŸŽ¯ Target: 200 Episodes...\nEp 10/200 | P1 Wins: 8 | P2 Wins: 2 | Îµ=0.9950 | 0.3s\nEp 20/200 | P1 Wins: 13 | P2 Wins: 7 | Îµ=0.9900 | 1.6s\nEp 30/200 | P1 Wins: 17 | P2 Wins: 13 | Îµ=0.9851 | 1.9s\nEp 40/200 | P1 Wins: 22 | P2 Wins: 18 | Îµ=0.9802 | 12.4s\nEp 50/200 | P1 Wins: 31 | P2 Wins: 19 | Îµ=0.9753 | 12.7s\nEp 60/200 | P1 Wins: 35 | P2 Wins: 25 | Îµ=0.9704 | 22.3s\nEp 70/200 | P1 Wins: 42 | P2 Wins: 28 | Îµ=0.9656 | 22.6s\nEp 80/200 | P1 Wins: 49 | P2 Wins: 31 | Îµ=0.9608 | 22.9s\nEp 90/200 | P1 Wins: 52 | P2 Wins: 38 | Îµ=0.9560 | 23.2s\nEp 100/200 | P1 Wins: 56 | P2 Wins: 44 | Îµ=0.9512 | 36.2s\nEp 110/200 | P1 Wins: 63 | P2 Wins: 47 | Îµ=0.9465 | 36.5s\nEp 120/200 | P1 Wins: 71 | P2 Wins: 49 | Îµ=0.9418 | 71.9s\nEp 130/200 | P1 Wins: 76 | P2 Wins: 54 | Îµ=0.9371 | 161.6s\nEp 140/200 | P1 Wins: 82 | P2 Wins: 58 | Îµ=0.9324 | 165.6s\nEp 150/200 | P1 Wins: 89 | P2 Wins: 61 | Îµ=0.9277 | 245.6s\nEp 160/200 | P1 Wins: 96 | P2 Wins: 64 | Îµ=0.9231 | 310.4s\nEp 170/200 | P1 Wins: 102 | P2 Wins: 68 | Îµ=0.9185 | 319.5s\nEp 180/200 | P1 Wins: 109 | P2 Wins: 71 | Îµ=0.9139 | 329.5s\nEp 190/200 | P1 Wins: 115 | P2 Wins: 75 | Îµ=0.9094 | 340.6s\nEp 200/200 | P1 Wins: 122 | P2 Wins: 78 | Îµ=0.9048 | 350.5s\n\nðŸ† Training Finished!\nðŸ’¾ Saving: A1 Q-States=698, A2 Q-States=683\nâœ… Save Complete! Download 'agi_agents.zip'\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}