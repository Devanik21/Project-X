{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:16:31.612314Z","iopub.execute_input":"2025-12-13T14:16:31.612782Z","iopub.status.idle":"2025-12-13T14:16:31.617391Z","shell.execute_reply.started":"2025-12-13T14:16:31.612760Z","shell.execute_reply":"2025-12-13T14:16:31.616490Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\nimport zipfile\nimport io\nimport math\nimport time\nfrom copy import deepcopy\n\n# ============================================================================\n# 1. HELPERS & ENCODERS\n# ============================================================================\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer): return int(obj)\n        if isinstance(obj, np.floating): return float(obj)\n        if isinstance(obj, np.ndarray): return obj.tolist()\n        return super(NumpyEncoder, self).default(obj)\n\n# ============================================================================\n# 2. NIM ENVIRONMENT\n# ============================================================================\nclass Nim:\n    def __init__(self, piles=[1, 3, 5, 7]):\n        self.initial_piles = piles[:]\n        self.reset()\n    \n    def reset(self):\n        self.piles = self.initial_piles[:]\n        self.current_player = 0\n        self.game_over = False\n        self.winner = None\n        self.move_history = []\n        return self.get_state()\n    \n    def get_state(self):\n        # Return hashable state representation (sorted tuple)\n        return tuple(sorted(self.piles))\n    \n    def get_available_actions(self):\n        actions = []\n        for i, pile in enumerate(self.piles):\n            for take in range(1, pile + 1):\n                actions.append((i, take))\n        return actions\n    \n    def make_move(self, action):\n        if self.game_over: return self.get_state(), 0, True\n        \n        pile_idx, take = action\n        if pile_idx >= len(self.piles) or take > self.piles[pile_idx] or take < 1:\n            return self.get_state(), -100, True\n        \n        self.piles[pile_idx] -= take\n        self.move_history.append((self.current_player, action))\n        \n        # Check lose condition (Last stick taker loses)\n        if sum(self.piles) == 0:\n            self.game_over = True\n            self.winner = 1 - self.current_player\n            return self.get_state(), -50, True\n        \n        self.current_player = 1 - self.current_player\n        return self.get_state(), 0, False\n    \n    def copy(self):\n        new_env = Nim(self.initial_piles)\n        new_env.piles = self.piles[:]\n        new_env.current_player = self.current_player\n        new_env.game_over = self.game_over\n        new_env.winner = self.winner\n        return new_env\n\n# ============================================================================\n# 3. MCTS NODE\n# ============================================================================\nclass MCTSNode:\n    def __init__(self, state, parent=None, action=None):\n        self.state = state\n        self.parent = parent\n        self.action = action\n        self.children = []\n        self.visits = 0\n        self.value = 0.0\n        self.untried_actions = None\n    \n    def uct_value(self, exploration=1.41):\n        if self.visits == 0: return float('inf')\n        return self.value / self.visits + exploration * math.sqrt(math.log(self.parent.visits) / self.visits)\n    \n    def best_child(self, exploration=1.41):\n        return max(self.children, key=lambda c: c.uct_value(exploration))\n\n# ============================================================================\n# 4. NIM AGENT\n# ============================================================================\nclass NimAgent:\n    def __init__(self, player_id, lr=0.1, gamma=0.95, epsilon=1.0,\n                 epsilon_decay=0.995, epsilon_min=0.01, mcts_sims=50, minimax_depth=3):\n        self.player_id = player_id; self.lr = lr; self.gamma = gamma\n        self.epsilon = epsilon; self.epsilon_decay = epsilon_decay; self.epsilon_min = epsilon_min\n        self.mcts_simulations = mcts_sims; self.minimax_depth = minimax_depth\n        self.q_table = {}\n        self.wins = 0; self.losses = 0; self.draws = 0; self.total_score = 0; self.games_played = 0\n    \n    def get_q_value(self, state, action):\n        action_key = str(action)\n        return self.q_table.get((state, action_key), 0.0)\n    \n    def choose_action(self, env, training=True):\n        available_actions = env.get_available_actions()\n        if not available_actions: return None\n        if training and random.random() < self.epsilon: return random.choice(available_actions)\n        \n        # Combined Strategy\n        best_action = None; best_score = -float('inf')\n        for action in available_actions:\n            score = 0.0\n            score += self.get_q_value(env.get_state(), action) * 0.3 # Q-Value\n            if self.mcts_simulations > 0: score += self._evaluate_action_mcts(env, action) * 0.4 # MCTS\n            if self.minimax_depth > 0: score += self._evaluate_action_minimax(env, action) * 0.3 # Minimax\n            if score > best_score: best_score = score; best_action = action\n        return best_action if best_action else available_actions[0]\n    \n    def _evaluate_action_mcts(self, env, action):\n        sim = env.copy(); sim.make_move(action)\n        if sim.game_over: return 100 if sim.winner == self.player_id else -100\n        root = MCTSNode(sim.get_state()); root.untried_actions = sim.get_available_actions()[:]\n        for _ in range(self.mcts_simulations):\n            node = root; temp = sim.copy()\n            while node.untried_actions == [] and node.children:\n                node = node.best_child(); \n                if node.action: temp.make_move(node.action)\n            if node.untried_actions and not temp.game_over:\n                na = random.choice(node.untried_actions); node.untried_actions.remove(na)\n                temp.make_move(na); child = MCTSNode(temp.get_state(), parent=node, action=na)\n                node.children.append(child); node = child\n            \n            # Simulation\n            rw = 0; steps = 0\n            t_sim = temp.copy()\n            while not t_sim.game_over and steps < 30:\n                 acts = t_sim.get_available_actions()\n                 if not acts: break\n                 t_sim.make_move(random.choice(acts)); steps += 1\n            if t_sim.game_over: rw = 50 if t_sim.winner == self.player_id else -50\n            \n            # Backprop\n            while node: node.visits += 1; node.value += rw; node = node.parent\n        return root.value / max(root.visits, 1)\n    \n    def _evaluate_action_minimax(self, env, action):\n        sim = env.copy(); sim.make_move(action)\n        if sim.game_over: return 100 if sim.winner == self.player_id else -100\n        return self._minimax(sim, self.minimax_depth - 1, -float('inf'), float('inf'), False)\n    \n    def _minimax(self, env, depth, alpha, beta, is_maximizing):\n        if env.game_over: return 100 if env.winner == self.player_id else -100\n        if depth == 0: return 0\n        actions = env.get_available_actions()\n        if not actions: return 0\n        if is_maximizing:\n            me = -float('inf')\n            for a in actions[:10]:\n                s = env.copy(); s.make_move(a)\n                ev = self._minimax(s, depth - 1, alpha, beta, False)\n                me = max(me, ev); alpha = max(alpha, ev)\n                if beta <= alpha: break\n            return me\n        else:\n            me = float('inf')\n            for a in actions[:10]:\n                s = env.copy(); s.make_move(a)\n                ev = self._minimax(s, depth - 1, alpha, beta, True)\n                me = min(me, ev); beta = min(beta, ev)\n                if beta <= alpha: break\n            return me\n\n    def update_q_value(self, state, action, reward, next_state, next_actions):\n        ak = str(action)\n        cq = self.get_q_value(state, action)\n        mnq = max([self.get_q_value(next_state, a) for a in next_actions], default=0) if next_actions else 0\n        self.q_table[(state, ak)] = cq + self.lr * (reward + self.gamma * mnq - cq)\n    \n    def decay_epsilon(self): self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    def reset_stats(self): self.wins = 0; self.losses = 0; self.total_score = 0; self.games_played = 0\n\ndef play_game(env, agent1, agent2, training=True):\n    env.reset()\n    agents = [agent1, agent2]\n    moves = 0\n    while not env.game_over and moves < 50:\n        cp = env.current_player; agent = agents[cp]\n        s = env.get_state()\n        a = agent.choose_action(env, training)\n        if a is None: break\n        ns, r, done = env.make_move(a)\n        if training: agent.update_q_value(s, a, r, ns, env.get_available_actions())\n        moves += 1\n        if done and env.winner is not None:\n            agents[env.winner].wins += 1; agents[1-env.winner].losses += 1\n            for ag in agents: ag.games_played += 1\n    return env.winner","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:16:31.618606Z","iopub.execute_input":"2025-12-13T14:16:31.618786Z","iopub.status.idle":"2025-12-13T14:16:31.648281Z","shell.execute_reply.started":"2025-12-13T14:16:31.618772Z","shell.execute_reply":"2025-12-13T14:16:31.647687Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import time\nimport json\nimport zipfile\n\n# ============================================================================\n# 1. SERIALIZATION \n# ============================================================================\ndef serialize_q_table(q_table):\n    serialized_q = {}\n    for (state, action_key), value in q_table.items():\n        state_str = json.dumps(list(state))\n        key_str = f\"{state_str}|{action_key}\"\n        serialized_q[key_str] = float(value)\n    return serialized_q\n\ndef save_kaggle_brain(agent1, agent2, config, filename=\"nim_brains.zip\"):\n    # Construct data dictionaries \n    agent1_data = {\n        \"q_table\": serialize_q_table(agent1.q_table),\n        \"epsilon\": float(agent1.epsilon),\n        \"wins\": int(agent1.wins), \"losses\": int(agent1.losses),\n        \"total_score\": int(agent1.total_score),\n        \"games_played\": int(agent1.games_played),\n        \"mcts_simulations\": int(agent1.mcts_simulations),\n        \"minimax_depth\": int(agent1.minimax_depth)\n    }\n    agent2_data = {\n        \"q_table\": serialize_q_table(agent2.q_table),\n        \"epsilon\": float(agent2.epsilon),\n        \"wins\": int(agent2.wins), \"losses\": int(agent2.losses),\n        \"total_score\": int(agent2.total_score),\n        \"games_played\": int(agent2.games_played),\n        \"mcts_simulations\": int(agent2.mcts_simulations),\n        \"minimax_depth\": int(agent2.minimax_depth)\n    }\n    \n    print(f\"ðŸ’¾ Saving: A1 Q-States={len(agent1.q_table)}, A2 Q-States={len(agent2.q_table)}\")\n    \n    with zipfile.ZipFile(filename, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(agent1_data, cls=NumpyEncoder, indent=2))\n        zf.writestr(\"agent2.json\", json.dumps(agent2_data, cls=NumpyEncoder, indent=2))\n        zf.writestr(\"config.json\", json.dumps(config, cls=NumpyEncoder, indent=2))\n    \n    print(f\"âœ… Save Complete! Download '{filename}'\")\n\n# ============================================================================\n# 2. TRAINING RUNNER (Now with HISTORY!)\n# ============================================================================\ndef run_nim_training():\n    # --- CONFIGURATION ---\n    EPISODES = 100     \n    PILES = [1, 3, 5, 7] \n    \n    env = Nim(PILES)\n    \n    # === ADJUST BRAIN POWER HERE ===\n    agent1 = NimAgent(0, lr=0.1, gamma=0.95, epsilon_decay=0.9995, \n                      mcts_sims=200, minimax_depth=10)\n    agent2 = NimAgent(1, lr=0.1, gamma=0.95, epsilon_decay=0.9995, \n                      mcts_sims=200, minimax_depth=10)\n    \n    # --- FIX: Initialize History Lists ---\n    history = {\n        'agent1_wins': [], 'agent2_wins': [],\n        'agent1_epsilon': [], 'agent2_epsilon': [],\n        'agent1_q_size': [], 'agent2_q_size': [],\n        'episode': []\n    }\n    \n    print(f\"ðŸš€ Starting HIGH-IQ Nim Training ({EPISODES} Episodes)...\")\n    start_time = time.time()\n    \n    for ep in range(1, EPISODES + 1):\n        play_game(env, agent1, agent2, training=True)\n        \n        agent1.decay_epsilon()\n        agent2.decay_epsilon()\n        \n        # Record stats every 50 episodes (Matches Streamlit default)\n        if ep % 5 == 0:\n            history['agent1_wins'].append(agent1.wins)\n            history['agent2_wins'].append(agent2.wins)\n            history['agent1_epsilon'].append(agent1.epsilon)\n            history['agent2_epsilon'].append(agent2.epsilon)\n            history['agent1_q_size'].append(len(agent1.q_table))\n            history['agent2_q_size'].append(len(agent2.q_table))\n            history['episode'].append(ep)\n            \n        if ep % 5 == 0:\n            elapsed = time.time() - start_time\n            print(f\"Episode {ep}/{EPISODES} | A1 Wins: {agent1.wins} | Îµ={agent1.epsilon:.4f} | {elapsed:.1f}s\")\n    \n    # Save the FULL history dictionary\n    config = {\n        \"lr1\": agent1.lr, \"gamma1\": agent1.gamma,\n        \"lr2\": agent2.lr, \"gamma2\": agent2.gamma,\n        \"piles\": PILES,\n        \"training_history\": history \n    }\n    \n    print(\"\\nðŸ† Training Finished!\")\n    save_kaggle_brain(agent1, agent2, config)\n\nif __name__ == \"__main__\":\n    run_nim_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:16:31.812499Z","iopub.execute_input":"2025-12-13T14:16:31.812990Z","iopub.status.idle":"2025-12-13T14:17:11.245162Z","shell.execute_reply.started":"2025-12-13T14:16:31.812965Z","shell.execute_reply":"2025-12-13T14:17:11.244389Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting HIGH-IQ Nim Training (100 Episodes)...\nEpisode 5/100 | A1 Wins: 1 | Îµ=0.9975 | 0.0s\nEpisode 10/100 | A1 Wins: 3 | Îµ=0.9950 | 0.0s\nEpisode 15/100 | A1 Wins: 6 | Îµ=0.9925 | 0.0s\nEpisode 20/100 | A1 Wins: 9 | Îµ=0.9900 | 10.5s\nEpisode 25/100 | A1 Wins: 13 | Îµ=0.9876 | 10.5s\nEpisode 30/100 | A1 Wins: 16 | Îµ=0.9851 | 10.6s\nEpisode 35/100 | A1 Wins: 19 | Îµ=0.9826 | 10.6s\nEpisode 40/100 | A1 Wins: 23 | Îµ=0.9802 | 10.6s\nEpisode 45/100 | A1 Wins: 27 | Îµ=0.9777 | 10.6s\nEpisode 50/100 | A1 Wins: 30 | Îµ=0.9753 | 16.2s\nEpisode 55/100 | A1 Wins: 33 | Îµ=0.9729 | 16.2s\nEpisode 60/100 | A1 Wins: 35 | Îµ=0.9704 | 16.2s\nEpisode 65/100 | A1 Wins: 36 | Îµ=0.9680 | 21.8s\nEpisode 70/100 | A1 Wins: 39 | Îµ=0.9656 | 27.5s\nEpisode 75/100 | A1 Wins: 42 | Îµ=0.9632 | 27.5s\nEpisode 80/100 | A1 Wins: 46 | Îµ=0.9608 | 27.9s\nEpisode 85/100 | A1 Wins: 48 | Îµ=0.9584 | 33.5s\nEpisode 90/100 | A1 Wins: 52 | Îµ=0.9560 | 33.7s\nEpisode 95/100 | A1 Wins: 55 | Îµ=0.9536 | 33.7s\nEpisode 100/100 | A1 Wins: 59 | Îµ=0.9512 | 39.4s\n\nðŸ† Training Finished!\nðŸ’¾ Saving: A1 Q-States=214, A2 Q-States=225\nâœ… Save Complete! Download 'nim_brains.zip'\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}