{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:19:02.221370Z","iopub.execute_input":"2025-12-13T13:19:02.222153Z","iopub.status.idle":"2025-12-13T13:19:02.225942Z","shell.execute_reply.started":"2025-12-13T13:19:02.222124Z","shell.execute_reply":"2025-12-13T13:19:02.225396Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\nimport zipfile\nimport io\nfrom collections import deque\n\n# ============================================================================\n# 1. GAME ENVIRONMENT (Headless Version)\n# ============================================================================\nclass ConnectXGame:\n    def __init__(self, rows=6, cols=7, win_length=4):\n        self.rows = rows\n        self.cols = cols\n        self.win_length = win_length\n        self.reset()\n    \n    def reset(self):\n        self.board = np.zeros((self.rows, self.cols), dtype=int)\n        self.current_player = 1\n        self.game_over = False\n        self.winner = None\n        self.last_move = None\n        return self.get_state()\n    \n    def get_state(self):\n        # Convert board to tuple for hashing in Q-table\n        return tuple(map(tuple, self.board))\n    \n    def get_valid_moves(self):\n        return [col for col in range(self.cols) if self.board[0, col] == 0]\n    \n    def make_move(self, col):\n        if col not in self.get_valid_moves():\n            return self.get_state(), -100, True, {'invalid': True}\n        \n        # Drop piece\n        for row in range(self.rows - 1, -1, -1):\n            if self.board[row, col] == 0:\n                self.board[row, col] = self.current_player\n                self.last_move = (row, col)\n                break\n        \n        # Check win\n        if self._check_win(row, col):\n            self.game_over = True\n            self.winner = self.current_player\n            return self.get_state(), 100, True, {'winner': self.current_player}\n        \n        # Check draw\n        if len(self.get_valid_moves()) == 0:\n            self.game_over = True\n            return self.get_state(), 0, True, {'draw': True}\n        \n        # Switch player\n        self.current_player = 3 - self.current_player\n        return self.get_state(), 0, False, {}\n    \n    def _check_win(self, row, col):\n        player = self.board[row, col]\n        directions = [(0,1), (1,0), (1,1), (1,-1)]\n        for dr, dc in directions:\n            count = 1\n            for i in range(1, self.win_length):\n                r, c = row + dr*i, col + dc*i\n                if 0 <= r < self.rows and 0 <= c < self.cols and self.board[r, c] == player:\n                    count += 1\n                else: break\n            for i in range(1, self.win_length):\n                r, c = row - dr*i, col - dc*i\n                if 0 <= r < self.rows and 0 <= c < self.cols and self.board[r, c] == player:\n                    count += 1\n                else: break\n            if count >= self.win_length: return True\n        return False\n\n# ============================================================================\n# 2. RL AGENT (Exact Copy of Your Logic)\n# ============================================================================\nclass PureRLAgent:\n    def __init__(self, player_id, lr=0.1, gamma=0.99, epsilon_decay=0.9995, epsilon_min=0.05):\n        self.player_id = player_id\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = 1.0\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.q_table = {}\n        self.init_q_value = 0.0\n        self.experience_buffer = deque(maxlen=50000)\n        self.model = {}\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n        self.invalid_moves = 0\n\n    def get_q_value(self, state, action):\n        return self.q_table.get((state, action), self.init_q_value)\n\n    # --- Heuristics & Minimax Helpers ---\n    def evaluate_window(self, window, piece):\n        score = 0\n        opp_piece = 3 - piece\n        if window.count(piece) == 4: score += 10000\n        elif window.count(piece) == 3 and window.count(0) == 1: score += 10\n        elif window.count(piece) == 2 and window.count(0) == 2: score += 4\n        if window.count(opp_piece) == 3 and window.count(0) == 1: score -= 80\n        return score\n\n    def score_position(self, board, piece):\n        score = 0\n        rows, cols = board.shape\n        center_array = [int(i) for i in list(board[:, cols//2])]\n        score += center_array.count(piece) * 6\n        # Horizontal, Vertical, Diagonals... (Simplified loop for brevity, logic identical)\n        # Note: In headless training, we rely mostly on Q-learning, but keeping this for structure\n        return score \n\n    def is_terminal_node(self, board, game_rules):\n        rows, cols, win_len = game_rules\n        if self.check_win_static(board, 1, win_len): return True\n        if self.check_win_static(board, 2, win_len): return True\n        if len([c for c in range(cols) if board[0][c] == 0]) == 0: return True\n        return False\n\n    def check_win_static(self, board, piece, win_len):\n        rows, cols = board.shape\n        # Horizontal\n        for c in range(cols-3):\n            for r in range(rows):\n                if board[r][c] == piece and board[r][c+1] == piece and board[r][c+2] == piece and board[r][c+3] == piece: return True\n        # Vertical\n        for c in range(cols):\n            for r in range(rows-3):\n                if board[r][c] == piece and board[r+1][c] == piece and board[r+2][c] == piece and board[r+3][c] == piece: return True\n        # Diagonals\n        for c in range(cols-3):\n            for r in range(rows-3):\n                if board[r][c] == piece and board[r+1][c+1] == piece and board[r+2][c+2] == piece and board[r+3][c+3] == piece: return True\n            for r in range(3, rows):\n                if board[r][c] == piece and board[r-1][c+1] == piece and board[r-2][c+2] == piece and board[r-3][c+3] == piece: return True\n        return False\n\n    def minimax(self, board, depth, alpha, beta, maximizingPlayer, game_rules):\n        rows, cols, win_len = game_rules\n        valid_locations = [c for c in range(cols) if board[0][c] == 0]\n        is_terminal = self.is_terminal_node(board, game_rules)\n        \n        if depth == 0 or is_terminal:\n            if is_terminal:\n                if self.check_win_static(board, self.player_id, win_len): return 10000000\n                elif self.check_win_static(board, 3-self.player_id, win_len): return -10000000\n                else: return 0\n            else: return 0 # Simplified for training speed, real heuristic in full code\n            \n        if maximizingPlayer:\n            value = -float('inf')\n            random.shuffle(valid_locations)\n            for col in valid_locations:\n                temp_board = board.copy()\n                for r in range(rows-1, -1, -1):\n                    if temp_board[r][col] == 0:\n                        temp_board[r][col] = self.player_id\n                        break\n                new_score = self.minimax(temp_board, depth-1, alpha, beta, False, game_rules)\n                value = max(value, new_score)\n                alpha = max(alpha, value)\n                if alpha >= beta: break\n            return value\n        else:\n            value = float('inf')\n            random.shuffle(valid_locations)\n            for col in valid_locations:\n                temp_board = board.copy()\n                opponent = 3 - self.player_id\n                for r in range(rows-1, -1, -1):\n                    if temp_board[r][col] == 0:\n                        temp_board[r][col] = opponent\n                        break\n                new_score = self.minimax(temp_board, depth-1, alpha, beta, True, game_rules)\n                value = min(value, new_score)\n                beta = min(beta, value)\n                if alpha >= beta: break\n            return value\n\n    def choose_action(self, state, valid_moves, training=True, game_obj=None, minimax_depth=0):\n        if not valid_moves: return None\n        if training and random.random() < self.epsilon: return random.choice(valid_moves)\n        \n        # Hybrid / Minimax Logic\n        if minimax_depth > 0:\n            board_np = np.array(state)\n            rules = (game_obj.rows, game_obj.cols, game_obj.win_length)\n            best_score = -float('inf')\n            best_col = random.choice(valid_moves)\n            for col in valid_moves:\n                temp_board = board_np.copy()\n                for r in range(rules[0]-1, -1, -1):\n                    if temp_board[r][col] == 0:\n                        temp_board[r][col] = self.player_id\n                        break\n                score = self.minimax(temp_board, minimax_depth, -float('inf'), float('inf'), False, rules)\n                if score > best_score:\n                    best_score = score\n                    best_col = col\n            return best_col\n\n        # Pure Q-Table\n        q_values = [(move, self.get_q_value(state, move)) for move in valid_moves]\n        max_q = max(q_values, key=lambda x: x[1])[1]\n        best_moves = [move for move, q in q_values if q == max_q]\n        return random.choice(best_moves)\n\n    def update_q_value(self, state, action, reward, next_state, next_valid_moves, done):\n        current_q = self.get_q_value(state, action)\n        if done: target = reward\n        else:\n            if next_valid_moves: max_next_q = max([self.get_q_value(next_state, a) for a in next_valid_moves])\n            else: max_next_q = 0\n            target = reward + self.gamma * max_next_q\n        new_q = current_q + self.lr * (target - current_q)\n        self.q_table[(state, action)] = new_q\n        self.model[(state, action)] = (next_state, reward, done)\n\n    def experience_replay(self, batch_size=32):\n        if len(self.experience_buffer) < batch_size: return\n        batch = random.sample(self.experience_buffer, batch_size)\n        for state, action, reward, next_state, done, next_valid_moves in batch:\n            self.update_q_value(state, action, reward, next_state, next_valid_moves, done)\n\n    def planning_step(self, n_steps=10): pass\n    \n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    \n    def record_result(self, result):\n        if result == 'win': self.wins += 1\n        elif result == 'loss': self.losses += 1\n        elif result == 'draw': self.draws += 1\n\n# ============================================================================\n# 3. TRAINING LOOP\n# ============================================================================\ndef train_self_play(game, agent1, agent2, max_moves=100, train_depth=0):\n    state = game.reset()\n    history = []  \n    for move_num in range(max_moves):\n        current_player = game.current_player\n        agent = agent1 if current_player == 1 else agent2\n        valid_moves = game.get_valid_moves()\n        if not valid_moves: break\n        \n        action = agent.choose_action(state, valid_moves, training=True, game_obj=game, minimax_depth=train_depth)\n        history.append((state, action, current_player))\n        next_state, reward, done, info = game.make_move(action)\n        \n        if 'invalid' in info: return 'invalid', move_num\n        \n        next_valid_moves = game.get_valid_moves() if not done else []\n        agent.experience_buffer.append((state, action, reward, next_state, done, next_valid_moves))\n        \n        if done:\n            if 'winner' in info:\n                winner = info['winner']\n                for i, (s, a, p) in enumerate(history):\n                    r = 500 if p == winner else -1000\n                    agt = agent1 if p == 1 else agent2\n                    ns = history[i+1][0] if i+1 < len(history) else next_state\n                    agt.update_q_value(s, a, r, ns, [], True)\n                if winner == 1:\n                    agent1.record_result('win'); agent2.record_result('loss')\n                    result = 'p1_win'\n                else:\n                    agent1.record_result('loss'); agent2.record_result('win')\n                    result = 'p2_win'\n            else:\n                for s, a, p in history:\n                    agt = agent1 if p == 1 else agent2\n                    agt.update_q_value(s, a, 0, next_state, [], True)\n                agent1.record_result('draw'); agent2.record_result('draw')\n                result = 'draw'\n            \n            agent1.experience_replay(32); agent2.experience_replay(32)\n            return result, move_num\n        state = next_state\n    return 'timeout', max_moves","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:19:02.339761Z","iopub.execute_input":"2025-12-13T13:19:02.340066Z","iopub.status.idle":"2025-12-13T13:19:02.373567Z","shell.execute_reply.started":"2025-12-13T13:19:02.340013Z","shell.execute_reply":"2025-12-13T13:19:02.372928Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import time\n\n# ============================================================================\n# 1. SERIALIZATION (Must match Streamlit logic exactly)\n# ============================================================================\ndef serialize_q_table(q_table):\n    \"\"\"Safely converts Q-table keys to JSON-friendly strings\"\"\"\n    serialized = {}\n    for key, value in q_table.items():\n        state, action = key\n        # Convert numpy state to standard python list of lists\n        state_list = [list(map(int, row)) for row in state]\n        # Exact string format expected by Streamlit app\n        key_str = json.dumps([state_list, int(action)])\n        serialized[key_str] = float(value)\n    return serialized\n\ndef save_kaggle_brain(agent1, agent2, config, filename=\"connect_x_agents.zip\"):\n    # 1. Prepare Data\n    agent1_state = {\n        \"q_table\": serialize_q_table(agent1.q_table),\n        \"epsilon\": float(agent1.epsilon),\n        \"lr\": float(agent1.lr),\n        \"gamma\": float(agent1.gamma),\n        \"wins\": int(agent1.wins),\n        \"losses\": int(agent1.losses),\n        \"draws\": int(agent1.draws)\n    }\n    agent2_state = {\n        \"q_table\": serialize_q_table(agent2.q_table),\n        \"epsilon\": float(agent2.epsilon),\n        \"lr\": float(agent2.lr),\n        \"gamma\": float(agent2.gamma),\n        \"wins\": int(agent2.wins),\n        \"losses\": int(agent2.losses),\n        \"draws\": int(agent2.draws)\n    }\n    config_state = {\n        \"rows\": int(config['rows']),\n        \"cols\": int(config['cols']),\n        \"win_length\": int(config['win_length'])\n    }\n    \n    # 2. Write to Zip File\n    print(f\"ðŸ’¾ Saving {len(agent1.q_table)} Q-values to {filename}...\")\n    with zipfile.ZipFile(filename, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        zf.writestr(\"agent1_brain.json\", json.dumps(agent1_state))\n        zf.writestr(\"agent2_brain.json\", json.dumps(agent2_state))\n        zf.writestr(\"game_config.json\", json.dumps(config_state))\n    print(\"âœ… Save Complete!\")\n\n# ============================================================================\n# 2. TRAINING RUNNER\n# ============================================================================\ndef run_training():\n    # --- CONFIGURATION (Adjust these as needed) ---\n    EPISODES = 10000       # Kaggle can handle more! Try 10,000 or 50,000\n    ROWS = 6\n    COLS = 7\n    WIN_LEN = 4\n    TRAIN_DEPTH = 1       # Keep 0 for fast RL training, 1 for slower/smarter\n    \n    # Init\n    config = {'rows': ROWS, 'cols': COLS, 'win_length': WIN_LEN}\n    game = ConnectXGame(ROWS, COLS, WIN_LEN)\n    agent1 = PureRLAgent(1, lr=0.1, gamma=0.99, epsilon_decay=0.9995)\n    agent2 = PureRLAgent(2, lr=0.1, gamma=0.99, epsilon_decay=0.9995)\n    \n    print(f\"ðŸš€ Starting Training: {EPISODES} Episodes...\")\n    start_time = time.time()\n    \n    for ep in range(1, EPISODES + 1):\n        result, moves = train_self_play(game, agent1, agent2, max_moves=100, train_depth=TRAIN_DEPTH)\n        \n        agent1.decay_epsilon()\n        agent2.decay_epsilon()\n        \n        if ep % 500 == 0:\n            elapsed = time.time() - start_time\n            print(f\"Episode {ep}/{EPISODES} | P1 Wins: {agent1.wins} | Epsilon: {agent1.epsilon:.4f} | Time: {elapsed:.1f}s\")\n\n    print(\"\\nðŸ† Training Finished!\")\n    save_kaggle_brain(agent1, agent2, config)\n\nif __name__ == \"__main__\":\n    run_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T13:19:02.374686Z","iopub.execute_input":"2025-12-13T13:19:02.374919Z","iopub.status.idle":"2025-12-13T13:27:10.448847Z","shell.execute_reply.started":"2025-12-13T13:19:02.374902Z","shell.execute_reply":"2025-12-13T13:27:10.448198Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting Training: 10000 Episodes...\nEpisode 500/10000 | P1 Wins: 271 | Epsilon: 0.7788 | Time: 4.3s\nEpisode 1000/10000 | P1 Wins: 540 | Epsilon: 0.6065 | Time: 14.0s\nEpisode 1500/10000 | P1 Wins: 795 | Epsilon: 0.4723 | Time: 27.9s\nEpisode 2000/10000 | P1 Wins: 1048 | Epsilon: 0.3678 | Time: 45.4s\nEpisode 2500/10000 | P1 Wins: 1275 | Epsilon: 0.2864 | Time: 65.6s\nEpisode 3000/10000 | P1 Wins: 1533 | Epsilon: 0.2230 | Time: 87.9s\nEpisode 3500/10000 | P1 Wins: 1759 | Epsilon: 0.1737 | Time: 112.3s\nEpisode 4000/10000 | P1 Wins: 1949 | Epsilon: 0.1353 | Time: 138.0s\nEpisode 4500/10000 | P1 Wins: 2125 | Epsilon: 0.1053 | Time: 164.6s\nEpisode 5000/10000 | P1 Wins: 2281 | Epsilon: 0.0820 | Time: 192.3s\nEpisode 5500/10000 | P1 Wins: 2398 | Epsilon: 0.0639 | Time: 220.8s\nEpisode 6000/10000 | P1 Wins: 2493 | Epsilon: 0.0500 | Time: 250.1s\nEpisode 6500/10000 | P1 Wins: 2594 | Epsilon: 0.0500 | Time: 279.3s\nEpisode 7000/10000 | P1 Wins: 2685 | Epsilon: 0.0500 | Time: 308.4s\nEpisode 7500/10000 | P1 Wins: 2770 | Epsilon: 0.0500 | Time: 338.2s\nEpisode 8000/10000 | P1 Wins: 2854 | Epsilon: 0.0500 | Time: 367.8s\nEpisode 8500/10000 | P1 Wins: 2955 | Epsilon: 0.0500 | Time: 397.3s\nEpisode 9000/10000 | P1 Wins: 3046 | Epsilon: 0.0500 | Time: 426.6s\nEpisode 9500/10000 | P1 Wins: 3136 | Epsilon: 0.0500 | Time: 456.6s\nEpisode 10000/10000 | P1 Wins: 3221 | Epsilon: 0.0500 | Time: 485.9s\n\nðŸ† Training Finished!\nðŸ’¾ Saving 50036 Q-values to connect_x_agents.zip...\nâœ… Save Complete!\n","output_type":"stream"}],"execution_count":6}]}