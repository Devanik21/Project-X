{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a11822",
   "metadata": {
    "id": "iXvv373nFRRb",
    "papermill": {
     "duration": 0.003548,
     "end_time": "2025-12-08T08:36:28.400424",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.396876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## My favourite  reinforcement learning algorithms\n",
    "\n",
    "## Part A : Implementation\n",
    "\n",
    "## Part B : Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502eccef",
   "metadata": {
    "id": "HKo6wpP8GDYX",
    "papermill": {
     "duration": 0.00207,
     "end_time": "2025-12-08T08:36:28.405396",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.403326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#--- Part A ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3306e7f",
   "metadata": {
    "id": "d5d2a7de",
    "papermill": {
     "duration": 0.001833,
     "end_time": "2025-12-08T08:36:28.409272",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.407439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Q-Learning Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def q_learning(env, num_episodes, learning_rate, discount_factor, epsilon):\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()  # Explore\n",
    "            else:\n",
    "                action = np.argmax(q_table[state, :]) # Exploit\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Q-value update\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state, :])\n",
    "\n",
    "            new_value = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            state = next_state\n",
    "    return q_table\n",
    "\n",
    "# Example usage (requires an environment like Gym's FrozenLake)\n",
    "# env = gym.make(\"FrozenLake-v1\")\n",
    "# q_table = q_learning(env, num_episodes=10000, learning_rate=0.1, discount_factor=0.99, epsilon=0.1)\n",
    "# print(q_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8772a78",
   "metadata": {
    "id": "d12ac3d8",
    "papermill": {
     "duration": 0.001808,
     "end_time": "2025-12-08T08:36:28.413128",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.411320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. SARSA Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sarsa(env, num_episodes, learning_rate, discount_factor, epsilon):\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        # Choose action a from state s using policy derived from Q (e.g., epsilon-greedy)\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Choose next action a' from next state s' using policy derived from Q\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = np.argmax(q_table[next_state, :])\n",
    "\n",
    "            # SARSA Q-value update\n",
    "            old_value = q_table[state, action]\n",
    "            target = reward + discount_factor * q_table[next_state, next_action]\n",
    "            new_value = old_value + learning_rate * (target - old_value)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    return q_table\n",
    "\n",
    "# Example usage (requires an environment like Gym's FrozenLake)\n",
    "# env = gym.make(\"FrozenLake-v1\")\n",
    "# q_table = sarsa(env, num_episodes=10000, learning_rate=0.1, discount_factor=0.99, epsilon=0.1)\n",
    "# print(q_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f13c7",
   "metadata": {
    "id": "6541072f",
    "papermill": {
     "duration": 0.002365,
     "end_time": "2025-12-08T08:36:28.418884",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.416519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Deep Q-Networks (DQN) Implementation\n",
    "\n",
    "```python\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, replay_buffer_size=2000, target_update_freq=10):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_step_counter = 0\n",
    "\n",
    "        self.q_network = self._build_model()\n",
    "        self.target_q_network = self._build_model()\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.q_network.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.q_network.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_q_network.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.discount_factor * np.amax(t)\n",
    "            self.q_network.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        self.train_step_counter += 1\n",
    "        if self.train_step_counter % self.target_update_freq == 0:\n",
    "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "# Example usage (requires an environment like Gym's CartPole)\n",
    "# env = gym.make('CartPole-v1')\n",
    "# state_size = env.observation_space.shape[0]\n",
    "# action_size = env.action_space.n\n",
    "# agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# for e in range(num_episodes):\n",
    "#    state = env.reset()\n",
    "#    state = np.reshape(state, [1, state_size])\n",
    "#    done = False\n",
    "#    while not done:\n",
    "#        action = agent.choose_action(state)\n",
    "#        next_state, reward, done, _ = env.step(action)\n",
    "#        next_state = np.reshape(next_state, [1, state_size])\n",
    "#        agent.remember(state, action, reward, next_state, done)\n",
    "#        state = next_state\n",
    "#        agent.train(batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e91ec",
   "metadata": {
    "id": "ef60ca93",
    "papermill": {
     "duration": 0.001897,
     "end_time": "2025-12-08T08:36:28.422920",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.421023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. REINFORCE (Monte Carlo Policy Gradient) Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.policy_network = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='softmax')) # Output probabilities for actions\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss='categorical_crossentropy')\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        action_probabilities = self.policy_network.predict(state, verbose=0)[0]\n",
    "        action = np.random.choice(self.action_size, p=action_probabilities)\n",
    "        return action, action_probabilities[action]\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        # Calculate discounted returns (G_t)\n",
    "        discounted_returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.discount_factor * G\n",
    "            discounted_returns.insert(0, G)\n",
    "\n",
    "        # Standardize returns for more stable training (optional but common)\n",
    "        discounted_returns = np.array(discounted_returns)\n",
    "        discounted_returns = (discounted_returns - np.mean(discounted_returns)) / (np.std(discounted_returns) + 1e-8)\n",
    "\n",
    "        # Prepare for policy update\n",
    "        target_actions = tf.keras.utils.to_categorical(actions, self.action_size)\n",
    "\n",
    "        # Policy gradient update\n",
    "        # In Keras, we use a custom loss function or sample weights to apply G_t as advantage\n",
    "        # For simplicity here, we'll imagine fitting with sample weights\n",
    "        # In a real implementation, you'd calculate the gradient manually or use a custom training loop\n",
    "\n",
    "        # Conceptual fitting with sample_weight (approximation for pedagogical purposes)\n",
    "        self.policy_network.fit(np.array(states), target_actions,\n",
    "                               sample_weight=discounted_returns,\n",
    "                               epochs=1, verbose=0)\n",
    "\n",
    "# Example usage (requires an environment like Gym's CartPole)\n",
    "# env = gym.make('CartPole-v1')\n",
    "# state_size = env.observation_space.shape[0]\n",
    "# action_size = env.action_space.n\n",
    "# agent = REINFORCEAgent(state_size, action_size)\n",
    "\n",
    "# for e in range(num_episodes):\n",
    "#     states, actions, rewards = [], [], []\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action, prob = agent.choose_action(state)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#\n",
    "#         states.append(state)\n",
    "#         actions.append(action)\n",
    "#         rewards.append(reward)\n",
    "#\n",
    "#         state = next_state\n",
    "#\n",
    "#     agent.train(states, actions, rewards)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f1f01",
   "metadata": {
    "id": "de866cfe",
    "papermill": {
     "duration": 0.002758,
     "end_time": "2025-12-08T08:36:28.427884",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.425126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Actor-Critic Methods (A2C Conceptual Implementation)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, state_size, action_size, actor_lr=0.001, critic_lr=0.005, discount_factor=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Actor network (policy)\n",
    "        self.actor = self._build_actor_model(actor_lr)\n",
    "        # Critic network (value function)\n",
    "        self.critic = self._build_critic_model(critic_lr)\n",
    "\n",
    "    def _build_actor_model(self, lr):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=lr))\n",
    "        return model\n",
    "\n",
    "    def _build_critic_model(self, lr):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='linear')) # Output V(s)\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=lr))\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        action_probabilities = self.actor.predict(state, verbose=0)[0]\n",
    "        action = np.random.choice(self.action_size, p=action_probabilities)\n",
    "        return action, action_probabilities[action]\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        next_state = np.reshape(next_state, [1, self.state_size])\n",
    "\n",
    "        # Predict value for current state and next state\n",
    "        value = self.critic.predict(state, verbose=0)[0]\n",
    "        next_value = self.critic.predict(next_state, verbose=0)[0]\n",
    "\n",
    "        # Calculate TD Target and TD Error (Advantage)\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.discount_factor * next_value[0]\n",
    "\n",
    "        advantage = td_target - value[0]\n",
    "\n",
    "        # Critic Update\n",
    "        self.critic.fit(state, np.array([[td_target]]), epochs=1, verbose=0)\n",
    "\n",
    "        # Actor Update\n",
    "        # We want to increase the probability of 'action' proportional to 'advantage'\n",
    "        # In Keras, this is typically done using a custom loss function or `tf.GradientTape`\n",
    "        # For simplicity, we'll represent it conceptually as fitting with advantage as sample_weight\n",
    "        \n",
    "        target_actions = np.zeros(self.action_size)\n",
    "        target_actions[action] = 1 # One-hot encode the action taken\n",
    "\n",
    "        self.actor.fit(state, np.array([target_actions]),\n",
    "                       sample_weight=np.array([advantage]),\n",
    "                       epochs=1, verbose=0)\n",
    "\n",
    "# Example usage (requires an environment like Gym's CartPole)\n",
    "# env = gym.make('CartPole-v1')\n",
    "# state_size = env.observation_space.shape[0]\n",
    "# action_size = env.action_space.n\n",
    "# agent = ActorCriticAgent(state_size, action_size)\n",
    "\n",
    "# for e in range(num_episodes):\n",
    "#    state = env.reset()\n",
    "#    done = False\n",
    "#    while not done:\n",
    "#        action, _ = agent.choose_action(state)\n",
    "#        next_state, reward, done, _ = env.step(action)\n",
    "#        agent.train(state, action, reward, next_state, done)\n",
    "#        state = next_state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9e13f",
   "metadata": {
    "id": "0576f468",
    "papermill": {
     "duration": 0.001783,
     "end_time": "2025-12-08T08:36:28.431717",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.429934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. Proximal Policy Optimization (PPO) Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, actor_lr=0.0003, critic_lr=0.001, discount_factor=0.99, clip_ratio=0.2):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.clip_ratio = clip_ratio\n",
    "\n",
    "        self.actor = self._build_actor_model(actor_lr)\n",
    "        self.critic = self._build_critic_model(critic_lr)\n",
    "        self.old_actor = self._build_actor_model(actor_lr) # For old policy probabilities\n",
    "        self.old_actor.set_weights(self.actor.get_weights())\n",
    "\n",
    "    def _build_actor_model(self, lr):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='softmax')\n",
    "        ])\n",
    "        # PPO actor loss is complex, handled with custom training step or Keras custom loss\n",
    "        # For simplicity, we'll compile with a placeholder and handle loss manually during training\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=lr))\n",
    "        return model\n",
    "\n",
    "    def _build_critic_model(self, lr):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=lr))\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        action_probabilities = self.actor.predict(state, verbose=0)[0]\n",
    "        action = np.random.choice(self.action_size, p=action_probabilities)\n",
    "        return action, action_probabilities[action]\n",
    "\n",
    "    def compute_advantages(self, rewards, values, next_values, dones):\n",
    "        # GAE (Generalized Advantage Estimation) is commonly used, here simplified TD Advantage\n",
    "        advantages = []\n",
    "        for i in range(len(rewards)):\n",
    "            if dones[i]:\n",
    "                td_target = rewards[i]\n",
    "            else:\n",
    "                td_target = rewards[i] + self.discount_factor * next_values[i]\n",
    "            advantage = td_target - values[i]\n",
    "            advantages.append(advantage)\n",
    "        return np.array(advantages)\n",
    "\n",
    "    def train(self, states, actions, old_action_probs, advantages, returns, num_epochs=3, batch_size=64):\n",
    "        # Convert to numpy arrays\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        old_action_probs = np.array(old_action_probs)\n",
    "        advantages = np.array(advantages)\n",
    "        returns = np.array(returns)\n",
    "\n",
    "        # Update old policy network for ratio calculation\n",
    "        self.old_actor.set_weights(self.actor.get_weights())\n",
    "\n",
    "        # Critic update (value function)\n",
    "        self.critic.fit(states, returns, epochs=num_epochs, verbose=0, batch_size=batch_size)\n",
    "\n",
    "        # Actor update (policy function)\n",
    "        for _ in range(num_epochs):\n",
    "            # Custom training step using tf.GradientTape for PPO loss\n",
    "            with tf.GradientTape() as tape:\n",
    "                current_action_probs = self.actor(states)\n",
    "                # Select probabilities for the chosen actions\n",
    "                current_action_probs_gathered = tf.gather_nd(current_action_probs,\n",
    "                                                           tf.stack([tf.range(tf.shape(actions)[0]), actions], axis=1))\n",
    "                \n",
    "                ratio = current_action_probs_gathered / (old_action_probs + 1e-10) # Add epsilon to avoid division by zero\n",
    "                \n",
    "                clipped_ratio = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "                \n",
    "                # PPO Clipped Objective\n",
    "                actor_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n",
    "            \n",
    "            actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            self.actor.optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "\n",
    "# Example usage (requires an environment like Gym's CartPole)\n",
    "# env = gym.make('CartPole-v1')\n",
    "# state_size = env.observation_space.shape[0]\n",
    "# action_size = env.action_space.n\n",
    "# agent = PPOAgent(state_size, action_size)\n",
    "\n",
    "# for e in range(num_episodes):\n",
    "#    states, actions, rewards, old_action_probs_list = [], [], [], []\n",
    "#    state = env.reset()\n",
    "#    done = False\n",
    "#    episode_rewards = 0\n",
    "#    while not done:\n",
    "#        action, old_prob = agent.choose_action(state)\n",
    "#        next_state, reward, done, _ = env.step(action)\n",
    "#\n",
    "#        states.append(state)\n",
    "#        actions.append(action)\n",
    "#        rewards.append(reward)\n",
    "#        old_action_probs_list.append(old_prob)\n",
    "#\n",
    "#        state = next_state\n",
    "#        episode_rewards += reward\n",
    "#    \n",
    "#    # Prepare data for training\n",
    "#    # Calculate value estimates for states in the trajectory\n",
    "#    values = agent.critic.predict(np.array(states), verbose=0).flatten()\n",
    "#    next_values = np.append(values[1:], agent.critic.predict(np.reshape(next_state, [1, state_size]), verbose=0)[0]) # Simplified\n",
    "#    \n",
    "#    advantages = agent.compute_advantages(rewards, values, next_values, [False]*len(rewards[:-1]) + [True])\n",
    "#\n",
    "#    # Calculate returns (sum of discounted rewards)\n",
    "#    returns = []\n",
    "#    G = 0\n",
    "#    for r in reversed(rewards):\n",
    "#        G = r + agent.discount_factor * G\n",
    "#        returns.insert(0, G)\n",
    "#    returns = np.array(returns)\n",
    "#\n",
    "#    agent.train(states, actions, old_action_probs_list, advantages, returns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d19d99",
   "metadata": {
    "id": "U7-PDORtF8Xf",
    "papermill": {
     "duration": 0.001821,
     "end_time": "2025-12-08T08:36:28.435380",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.433559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffee81",
   "metadata": {
    "id": "6e3381aa",
    "papermill": {
     "duration": 0.001688,
     "end_time": "2025-12-08T08:36:28.438833",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.437145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Q-Learning\n",
    "\n",
    "**Theory:** Q-Learning is an **off-policy, model-free, value-based** reinforcement learning algorithm. It learns an action-value function, $Q(s, a)$, which represents the maximum expected future rewards for taking action $a$ in state $s$. 'Off-policy' means it learns the optimal policy's Q-values while following a different (e.g., exploratory) behavior policy.\n",
    "\n",
    "**Equation (Q-Value Update):**\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [R + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "Where:\n",
    "*   $Q(s, a)$: Current Q-value for state $s$ and action $a$.\n",
    "*   $\\alpha$: Learning rate ($0 \\le \\alpha \\le 1$).\n",
    "*   $R$: Immediate reward received after taking action $a$ in state $s$.\n",
    "*   $\\gamma$: Discount factor ($0 \\le \\gamma \\le 1$) for future rewards.\n",
    "*   $s'$: The new state after taking action $a$.\n",
    "*   $\\max_{a'} Q(s', a')$: The maximum Q-value for the next state $s'$ over all possible actions $a'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fd617",
   "metadata": {
    "id": "619c39f0",
    "papermill": {
     "duration": 0.002922,
     "end_time": "2025-12-08T08:36:28.443506",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.440584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. SARSA (State-Action-Reward-State-Action)\n",
    "\n",
    "**Theory:** SARSA is an **on-policy, model-free, value-based** reinforcement learning algorithm. Similar to Q-Learning, it learns an action-value function $Q(s, a)$. However, 'on-policy' means that it updates the Q-values based on the *next action actually taken* by the current policy, not the maximum possible next action. This makes it more sensitive to the agent's exploration strategy.\n",
    "\n",
    "**Equation (Q-Value Update):**\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [R + \\gamma Q(s', a') - Q(s, a)]$$\n",
    "Where:\n",
    "*   $Q(s, a)$: Current Q-value for state $s$ and action $a$.\n",
    "*   $\\alpha$: Learning rate ($0 \\le \\alpha \\le 1$).\n",
    "*   $R$: Immediate reward received.\n",
    "*   $\\gamma$: Discount factor ($0 \\le \\gamma \\le 1$).\n",
    "*   $s'$: The new state.\n",
    "*   $a'$: The action chosen in state $s'$ by the *current policy*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411f143",
   "metadata": {
    "id": "efceccc2",
    "papermill": {
     "duration": 0.001959,
     "end_time": "2025-12-08T08:36:28.447792",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.445833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Deep Q-Networks (DQN)\n",
    "\n",
    "**Theory:** DQN extends Q-Learning by using **deep neural networks** to approximate the Q-function, enabling it to handle high-dimensional state spaces. Key innovations include **experience replay** (storing and sampling past transitions to break correlations) and a **separate target network** (a copy of the Q-network updated less frequently) to stabilize training.\n",
    "\n",
    "**Equation (Loss Function for Network Training):**\n",
    "$$L(\\theta) = E_{(s, a, R, s') \\sim U(D)} \\left[ (R + \\gamma \\max_{a'} Q(s', a'; \\theta_{target}) - Q(s, a; \\theta))^2 \\right]$$\n",
    "Where:\n",
    "*   $\\theta$: Parameters of the current Q-network.\n",
    "*   $\\theta_{target}$: Parameters of the target Q-network.\n",
    "*   $D$: Experience replay buffer.\n",
    "*   $U(D)$: Uniform sampling from the experience replay buffer.\n",
    "*   $R + \\gamma \\max_{a'} Q(s', a'; \\theta_{target})$: The 'target' Q-value, calculated using the target network.\n",
    "*   $Q(s, a; \\theta)$: The predicted Q-value from the current network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd24c8a",
   "metadata": {
    "id": "3d9bb1b5",
    "papermill": {
     "duration": 0.001854,
     "end_time": "2025-12-08T08:36:28.451594",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.449740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. REINFORCE (Monte Carlo Policy Gradient)\n",
    "\n",
    "**Theory:** REINFORCE is an **on-policy, policy-based** algorithm. Instead of learning value functions, it directly learns a parameterized policy $\\pi_{\\theta}(a|s)$ that maps states to actions. It estimates the gradient of the expected return using Monte Carlo rollouts (full episodes) and updates the policy parameters $\\theta$ in the direction that increases the probability of actions that lead to higher returns.\n",
    "\n",
    "**Equation (Policy Parameter Update):**\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla J(\\theta)$$\n",
    "$$J(\\theta) = E_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^T R_t \\right]$$\n",
    "$$\\nabla J(\\theta) \\approx \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) G_t$$\n",
    "Where:\n",
    "*   $\\theta$: Policy parameters.\n",
    "*   $\\alpha$: Learning rate.\n",
    "*   $J(\\theta)$: Objective function (expected total reward).\n",
    "*   $\\nabla J(\\theta)$: Gradient of the objective function.\n",
    "*   $G_t$: The return (total discounted future reward) from time step $t$.\n",
    "*   $\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)$: Gradient of the log-probability of the action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a2d71",
   "metadata": {
    "id": "490bfdae",
    "papermill": {
     "duration": 0.001834,
     "end_time": "2025-12-08T08:36:28.455283",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.453449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Actor-Critic Methods (Conceptual basis for A2C/A3C)\n",
    "\n",
    "**Theory:** Actor-Critic methods combine aspects of both **value-based (critic)** and **policy-based (actor)** approaches. The **Actor** learns a policy $\\pi_{\\theta}(a|s)$ to select actions, and the **Critic** learns a value function $V_{\\phi}(s)$ or $Q_{\\phi}(s, a)$ to estimate the expected return. The critic's value estimates are used to update the actor's policy, often via an **advantage function**, which reduces variance in policy gradient estimates.\n",
    "\n",
    "**Key Concepts & Updates (simplified):**\n",
    "\n",
    "*   **Advantage Function ($A(s, a)$):** Measures how much better an action $a$ is than the average action in state $s$.\n",
    "    *   $A(s, a) = Q(s, a) - V(s)$\n",
    "    *   Or, commonly used in A2C/A3C: $A(s, a) = R + \\gamma V(s') - V(s)$ (Temporal Difference Error)\n",
    "\n",
    "*   **Actor Update:** Policy parameters are updated in the direction of the advantage.\n",
    "    $$\\theta \\leftarrow \\theta + \\alpha_{actor} \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) A(s, a)$$\n",
    "\n",
    "*   **Critic Update:** Value function parameters are updated to minimize the squared error between its estimate and the actual (or estimated) return.\n",
    "    $$\\phi \\leftarrow \\phi - \\alpha_{critic} \\nabla_{\\phi} (R + \\gamma V_{\\phi}(s') - V_{\\phi}(s))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17084f",
   "metadata": {
    "id": "14ae0cd2",
    "papermill": {
     "duration": 0.001945,
     "end_time": "2025-12-08T08:36:28.459126",
     "exception": false,
     "start_time": "2025-12-08T08:36:28.457181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. Proximal Policy Optimization (PPO)\n",
    "\n",
    "**Theory:** PPO is a popular and robust **on-policy, policy-based** algorithm. It's an improvement over traditional policy gradient methods, aiming to achieve the data efficiency and reliable performance of Trust Region Policy Optimization (TRPO) but with a simpler implementation. PPO uses a **clipped surrogate objective function** to constrain policy updates, preventing them from becoming too large and destabilizing training.\n",
    "\n",
    "**Equation (Clipped Surrogate Objective):**\n",
    "$$L^{CLIP}(\\theta) = E_t \\left[ \\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t) \\right]$$\n",
    "Where:\n",
    "*   $\\theta$: Current policy parameters.\n",
    "*   $E_t$: Expectation over time steps (trajectories).\n",
    "*   $A_t$: Estimated advantage at time step $t$.\n",
    "*   $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$: Ratio of the new policy's probability to the old policy's probability for the chosen action.\n",
    "*   $\\text{clip}(x, L, R)$: Clips the value $x$ to be within the range $[L, R]$.\n",
    "*   $\\epsilon$: A small hyperparameter (e.g., 0.1 or 0.2) that defines the clipping range."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMOovjCl6QM0X4Wv7MAmwj1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.552547,
   "end_time": "2025-12-08T08:36:28.982478",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-08T08:36:23.429931",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
