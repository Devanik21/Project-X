{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:12.316613Z","iopub.execute_input":"2025-12-13T12:22:12.316801Z","iopub.status.idle":"2025-12-13T12:22:13.915371Z","shell.execute_reply.started":"2025-12-13T12:22:12.316784Z","shell.execute_reply":"2025-12-13T12:22:13.914576Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\nimport zipfile\nimport io\nimport math\nimport time\nfrom collections import deque, defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional, Dict\nimport ast\n\n# ============================================================================\n# 1. CORE CLASSES (Game Rules & AI)\n# ============================================================================\n\n@dataclass\nclass Move:\n    start: Tuple[int, int]\n    end: Tuple[int, int]\n    piece: str\n    captured: Optional[str] = None\n    promotion: Optional[str] = None\n    is_check: bool = False\n    is_checkmate: bool = False\n    score: float = 0.0\n    \n    def __hash__(self):\n        # FIXED: Includes captured/promotion to prevent hash collisions\n        return hash((self.start, self.end, self.piece, self.captured, self.promotion))\n    \n    def __eq__(self, other):\n        # FIXED: Includes captured/promotion to ensure distinct moves are treated differently\n        return (self.start == other.start and self.end == other.end and \n                self.piece == other.piece and self.captured == other.captured and \n                self.promotion == other.promotion)\n    \n    def to_notation(self):\n        cols = 'abcde'\n        s = f\"{cols[self.start[1]]}{5-self.start[0]}\"\n        e = f\"{cols[self.end[1]]}{5-self.end[0]}\"\n        notation = f\"{s}{e}\"\n        if self.promotion: notation += f\"={self.promotion.upper()}\"\n        if self.is_checkmate: notation += \"#\"\n        elif self.is_check: notation += \"+\"\n        return notation\n\nclass Minichess:\n    PIECE_VALUES = {'P': 100, 'N': 320, 'B': 330, 'R': 500, 'Q': 900, 'K': 20000,\n                    'p': -100, 'n': -320, 'b': -330, 'r': -500, 'q': -900, 'k': -20000}\n    \n    PST = {\n        'P': [[0, 0, 0, 0, 0], [80, 80, 80, 80, 80], [50, 50, 60, 50, 50], [10, 10, 20, 10, 10], [5, 5, 10, 5, 5]],\n        'N': [[-50,-40,-30,-40,-50], [-40,-20,0,-20,-40], [-30,5,15,5,-30], [-40,-20,0,-20,-40], [-50,-40,-30,-40,-50]],\n        'B': [[-20,-10,-10,-10,-20], [-10,5,0,5,-10], [-10,10,10,10,-10], [-10,5,10,5,-10], [-20,-10,-10,-10,-20]],\n        'R': [[0,0,0,0,0], [5,10,10,10,5], [-5,0,0,0,-5], [-5,0,0,0,-5], [0,0,0,0,0]],\n        'Q': [[-20,-10,-10,-10,-20], [-10,0,0,0,-10], [-10,0,5,0,-10], [-10,0,0,0,-10], [-20,-10,-10,-10,-20]],\n        'K': [[-30,-40,-40,-40,-30], [-30,-40,-40,-40,-30], [-30,-40,-40,-40,-30], [-20,-30,-30,-30,-20], [-10,-20,-20,-20,-10]]\n    }\n    \n    OPENING_BOOK = {\n        '((\"k\", \"q\", \"b\", \"n\", \"r\"), (\"p\", \"p\", \"p\", \"p\", \"p\"), (\".\", \".\", \".\", \".\", \".\"), (\"P\", \"P\", \"P\", \"P\", \"P\"), (\"K\", \"Q\", \"B\", \"N\", \"R\"))': [\n            ((3, 2), (2, 2)), ((3, 1), (2, 1))\n        ]\n    }\n    \n    def __init__(self):\n        self.board_size = 5\n        self.transposition_table = {}\n        self.killer_moves = [[] for _ in range(20)]\n        self.history_table = defaultdict(int)\n        self.reset()\n    \n    def reset(self):\n        self.board = np.array([\n            ['k', 'q', 'b', 'n', 'r'],\n            ['p', 'p', 'p', 'p', 'p'],\n            ['.', '.', '.', '.', '.'],\n            ['P', 'P', 'P', 'P', 'P'],\n            ['K', 'Q', 'B', 'N', 'R']\n        ])\n        self.current_player = 1\n        self.game_over = False\n        self.winner = None\n        self.move_history = []\n        self.move_count = 0\n        return self.get_state()\n    \n    def get_state(self):\n        return tuple(tuple(str(c) for c in row) for row in self.board)\n    \n    def copy(self):\n        new_game = Minichess()\n        new_game.board = self.board.copy()\n        new_game.current_player = self.current_player\n        new_game.game_over = self.game_over\n        new_game.winner = self.winner\n        new_game.move_history = self.move_history.copy()\n        new_game.move_count = self.move_count\n        new_game.transposition_table = self.transposition_table\n        new_game.killer_moves = self.killer_moves\n        new_game.history_table = self.history_table\n        return new_game\n    \n    def is_white_piece(self, piece): return piece.isupper() and piece != '.'\n    def is_black_piece(self, piece): return piece.islower() and piece != '.'\n    def is_enemy(self, piece, player): return self.is_black_piece(piece) if player == 1 else self.is_white_piece(piece)\n    def is_friendly(self, piece, player): return self.is_white_piece(piece) if player == 1 else self.is_black_piece(piece)\n    \n    def order_moves(self, moves, ply=0):\n        for move in moves:\n            score = 0\n            if move.captured:\n                victim = abs(self.PIECE_VALUES.get(move.captured, 0))\n                attacker = abs(self.PIECE_VALUES.get(move.piece, 0))\n                score += 10000 + victim - attacker / 100\n            if ply < len(self.killer_moves) and move in self.killer_moves[ply]: score += 9000\n            score += self.history_table.get((move.start, move.end), 0)\n            if move.promotion: score += 8000\n            if move.is_check: score += 5000\n            move.score = score\n        return sorted(moves, key=lambda m: m.score, reverse=True)\n    \n    def get_piece_moves(self, row, col, check_legal=True):\n        piece = self.board[row, col]\n        if piece == '.' or not self.is_friendly(piece, self.current_player): return []\n        \n        moves = []\n        pt = piece.upper()\n        if pt == 'P': moves = self._get_pawn_moves(row, col)\n        elif pt == 'N': moves = self._get_knight_moves(row, col)\n        elif pt == 'B': moves = self._get_sliding_moves(row, col, [(-1,-1),(-1,1),(1,-1),(1,1)])\n        elif pt == 'R': moves = self._get_sliding_moves(row, col, [(-1,0),(1,0),(0,-1),(0,1)])\n        elif pt == 'Q': moves = self._get_sliding_moves(row, col, [(-1,-1),(-1,1),(1,-1),(1,1),(-1,0),(1,0),(0,-1),(0,1)])\n        elif pt == 'K': moves = self._get_king_moves(row, col)\n        \n        if check_legal:\n            legal_moves = []\n            for m in moves:\n                tg = self.copy()\n                tg._make_move_internal(m)\n                if not tg._is_in_check(self.current_player): legal_moves.append(m)\n            return legal_moves\n        return moves\n\n    def _get_pawn_moves(self, row, col):\n        moves = []\n        piece = self.board[row, col]\n        dr = -1 if self.current_player == 1 else 1\n        promo_row = 0 if self.current_player == 1 else 4\n        \n        # Forward\n        nr = row + dr\n        if 0 <= nr < 5 and self.board[nr, col] == '.':\n            if nr == promo_row:\n                for p in ['Q', 'R', 'B', 'N']: moves.append(Move((row, col), (nr, col), piece, promotion=p))\n            else: moves.append(Move((row, col), (nr, col), piece))\n        \n        # Capture\n        for dc in [-1, 1]:\n            nc = col + dc\n            if 0 <= nr < 5 and 0 <= nc < 5:\n                t = self.board[nr, nc]\n                if t != '.' and self.is_enemy(t, self.current_player):\n                    if nr == promo_row:\n                        for p in ['Q', 'R', 'B', 'N']: moves.append(Move((row, col), (nr, nc), piece, captured=t, promotion=p))\n                    else: moves.append(Move((row, col), (nr, nc), piece, captured=t))\n        return moves\n\n    def _get_knight_moves(self, r, c):\n        moves = []\n        p = self.board[r, c]\n        for dr, dc in [(-2,-1),(-2,1),(-1,-2),(-1,2),(1,-2),(1,2),(2,-1),(2,1)]:\n            nr, nc = r+dr, c+dc\n            if 0<=nr<5 and 0<=nc<5:\n                t = self.board[nr, nc]\n                if t == '.' or self.is_enemy(t, self.current_player):\n                    moves.append(Move((r,c), (nr,nc), p, captured=(t if t!='.' else None)))\n        return moves\n\n    def _get_sliding_moves(self, r, c, dirs):\n        moves = []\n        p = self.board[r, c]\n        for dr, dc in dirs:\n            for i in range(1, 5):\n                nr, nc = r + dr*i, c + dc*i\n                if not (0<=nr<5 and 0<=nc<5): break\n                t = self.board[nr, nc]\n                if t == '.': moves.append(Move((r,c), (nr,nc), p))\n                elif self.is_enemy(t, self.current_player):\n                    moves.append(Move((r,c), (nr,nc), p, captured=t))\n                    break\n                else: break\n        return moves\n\n    def _get_king_moves(self, r, c):\n        moves = []\n        p = self.board[r, c]\n        for dr in [-1,0,1]:\n            for dc in [-1,0,1]:\n                if dr==0 and dc==0: continue\n                nr, nc = r+dr, c+dc\n                if 0<=nr<5 and 0<=nc<5:\n                    t = self.board[nr, nc]\n                    if t=='.' or self.is_enemy(t, self.current_player):\n                        moves.append(Move((r,c), (nr,nc), p, captured=(t if t!='.' else None)))\n        return moves\n\n    def get_all_valid_moves(self):\n        state = self.get_state()\n        if state in self.OPENING_BOOK and self.move_count < 3:\n            bm = []\n            for s, e in self.OPENING_BOOK[state]:\n                p = self.board[s[0], s[1]]\n                c = self.board[e[0], e[1]] if self.board[e[0], e[1]] != '.' else None\n                bm.append(Move(s, e, p, captured=c))\n            if bm: return bm\n        \n        all_moves = []\n        for r in range(5):\n            for c in range(5):\n                if self.board[r, c] != '.' and self.is_friendly(self.board[r, c], self.current_player):\n                    all_moves.extend(self.get_piece_moves(r, c))\n        return self.order_moves(all_moves, self.move_count)\n\n    def _find_king(self, player):\n        k = 'K' if player == 1 else 'k'\n        for r in range(5):\n            for c in range(5):\n                if self.board[r,c] == k: return (r,c)\n        return None\n\n    def _is_square_attacked(self, r, c, by_p):\n        orig = self.current_player\n        self.current_player = by_p\n        for row in range(5):\n            for col in range(5):\n                p = self.board[row, col]\n                if p != '.' and self.is_friendly(p, by_p):\n                    for m in self.get_piece_moves(row, col, check_legal=False):\n                        if m.end == (r, c):\n                            self.current_player = orig\n                            return True\n        self.current_player = orig\n        return False\n\n    def _is_in_check(self, p):\n        kp = self._find_king(p)\n        return kp and self._is_square_attacked(kp[0], kp[1], 3-p)\n\n    def _make_move_internal(self, m):\n        sr, sc = m.start\n        er, ec = m.end\n        p = m.promotion if m.promotion else self.board[sr, sc]\n        if m.promotion and self.current_player == 2: p = p.lower()\n        self.board[er, ec] = p\n        self.board[sr, sc] = '.'\n\n    def make_move(self, m: Move):\n        if self.game_over: return self.get_state(), 0, True\n        \n        reward = 0\n        if m.captured: reward = abs(self.PIECE_VALUES.get(m.captured, 0)) / 100\n        if m.promotion: reward += 5\n        \n        self._make_move_internal(m)\n        self.move_history.append(m)\n        self.move_count += 1\n        self.history_table[(m.start, m.end)] += 1\n        self.current_player = 3 - self.current_player\n        \n        om = self.get_all_valid_moves()\n        chk = self._is_in_check(self.current_player)\n        \n        if not om:\n            self.game_over = True\n            if chk:\n                self.winner = 3 - self.current_player\n                reward = 100\n                m.is_checkmate = True\n            else:\n                self.winner = 0\n                reward = 0\n        elif chk:\n            m.is_check = True\n            reward += 1\n            \n        if self.move_count >= 100:\n            self.game_over = True\n            self.winner = 0\n            \n        return self.get_state(), reward, self.game_over\n\n    def evaluate_position(self, p):\n        if self.winner == p: return 100000\n        if self.winner == (3-p): return -100000\n        if self.winner == 0: return 0\n        \n        score = 0\n        mat = 0\n        pos = 0\n        for r in range(5):\n            for c in range(5):\n                pc = self.board[r, c]\n                if pc == '.': continue\n                mine = self.is_friendly(pc, p)\n                mul = 1 if mine else -1\n                mat += mul * abs(self.PIECE_VALUES.get(pc, 0))\n                pt = pc.upper()\n                if pt in self.PST:\n                    val = self.PST[pt][r][c] if pc.isupper() else self.PST[pt][4-r][c]\n                    pos += mul * val\n        \n        score = mat + pos\n        \n        orig = self.current_player\n        self.current_player = p\n        my_m = len(self.get_all_valid_moves())\n        self.current_player = 3-p\n        op_m = len(self.get_all_valid_moves())\n        self.current_player = orig\n        \n        score += (my_m - op_m) * 10\n        if self._is_in_check(p): score -= 50\n        if self._is_in_check(3-p): score += 50\n        return score\n\nclass MCTSNode:\n    def __init__(self, gs, parent=None, move=None, prior=1.0):\n        self.game_state = gs\n        self.parent = parent\n        self.move = move\n        self.prior = prior\n        self.gumbel = np.random.gumbel(0, 1)\n        self.children = {}\n        self.visit_count = 0\n        self.value_sum = 0.0\n        self.is_expanded = False\n    \n    def value(self): return self.value_sum / self.visit_count if self.visit_count > 0 else 0\n    \n    def ucb_score(self, pv, c_puct=1.4, c_visit=50, c_scale=1.0):\n        if self.visit_count == 0: q = 0\n        else: q = (self.value() + 1) / 2\n        u = c_puct * self.prior * math.sqrt(pv) / (1 + self.visit_count)\n        g = self.gumbel + math.log(self.prior) + c_scale * q * c_visit\n        return q + u + g / (1 + pv)\n    \n    def select_child(self, c_puct=1.4):\n        if not self.children: return None\n        cands = list(self.children.values())\n        if len(cands) <= 1: return cands[0] if cands else None\n        cands.sort(key=lambda c: c.ucb_score(self.visit_count, c_puct), reverse=True)\n        if self.visit_count > 10: cands = cands[:max(1, len(cands)//2)]\n        return max(cands, key=lambda c: c.ucb_score(self.visit_count, c_puct))\n    \n    def expand(self, game, priors):\n        vm = game.get_all_valid_moves()\n        if not vm: return\n        tp = sum(priors.values()) or len(vm)\n        if self.parent is None:\n            dn = np.random.dirichlet([0.3] * len(vm))\n        \n        for idx, m in enumerate(vm):\n            p = priors.get(m, 1.0) / tp\n            if self.parent is None: p = 0.75*p + 0.25*dn[idx]\n            cg = game.copy()\n            cg.make_move(m)\n            self.children[m] = MCTSNode(cg, self, m, p)\n        self.is_expanded = True\n    \n    def backup(self, v):\n        self.visit_count += 1\n        self.value_sum += v\n        if self.parent: self.parent.backup(-v)\n\nclass Agent:\n    def __init__(self, pid, lr=0.5, gamma=0.99):\n        self.player_id = pid\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.92\n        self.epsilon_min = 0.05\n        self.mcts_simulations = 200\n        self.minimax_depth = 4\n        self.replay_buffer = deque(maxlen=10000)\n        self.policy_table = defaultdict(lambda: defaultdict(float))\n        self.value_table = {}\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n        self.training_steps = 0\n        self.c_puct = 1.4\n\n    def get_policy_priors(self, game):\n        state = game.get_state()\n        moves = game.get_all_valid_moves()\n        priors = {}\n        for m in moves:\n            if state in self.policy_table and m in self.policy_table[state]:\n                p = self.policy_table[state][m]\n            else:\n                p = 1.0\n                if m.captured: p += (abs(Minichess.PIECE_VALUES.get(m.captured, 0)) - abs(Minichess.PIECE_VALUES.get(m.piece, 0))/100)/100\n                if m.promotion == 'Q': p += 5.0\n                elif m.promotion: p += 3.0\n                if m.is_checkmate: p += 10.0\n                elif m.is_check: p += 2.0\n                if 1 <= m.end[0] <= 3 and 1 <= m.end[1] <= 3: p += 0.5\n            priors[m] = max(p, 0.01)\n        return priors\n\n    def mcts_search(self, game, sims):\n        root = MCTSNode(game.copy())\n        es = min(sims, 50 + self.training_steps // 10)\n        for _ in range(es):\n            node = root\n            sg = game.copy()\n            while node.is_expanded and node.children:\n                node = node.select_child(self.c_puct)\n                if not node: break\n                sg.make_move(node.move)\n            if node and not sg.game_over:\n                pp = self.get_policy_priors(sg)\n                node.expand(sg, pp)\n            if node:\n                val = self._evaluate_leaf(sg)\n                node.backup(val)\n        return root\n\n    def _evaluate_leaf(self, game):\n        if game.game_over:\n            if game.winner == self.player_id: return 1.0\n            elif game.winner == (3-self.player_id): return -1.0\n            return 0.0\n        state = game.get_state()\n        if state in self.value_table: return self.value_table[state]\n        if any(m.captured or m.is_check for m in game.get_all_valid_moves()[:3]):\n            s = self._quiescence_search(game, 3, -float('inf'), float('inf'), True)\n        else:\n            s = self._iterative_deepening_minimax(game, self.minimax_depth)\n        v = np.tanh(s/500)\n        self.value_table[state] = v\n        return v\n\n    def _quiescence_search(self, g, d, a, b, maxing):\n        sp = g.evaluate_position(self.player_id)\n        if d==0: return sp\n        if maxing:\n            if sp >= b: return b\n            a = max(a, sp)\n        else:\n            if sp <= a: return a\n            b = min(b, sp)\n        \n        moves = [m for m in g.get_all_valid_moves() if m.captured]\n        if not moves: return sp\n        \n        if maxing:\n            me = sp\n            for m in moves[:5]:\n                cg = g.copy()\n                cg.make_move(m)\n                v = self._quiescence_search(cg, d-1, a, b, False)\n                me = max(me, v)\n                a = max(a, v)\n                if b <= a: break\n            return me\n        else:\n            me = sp\n            for m in moves[:5]:\n                cg = g.copy()\n                cg.make_move(m)\n                v = self._quiescence_search(cg, d-1, a, b, True)\n                me = min(me, v)\n                b = min(b, v)\n                if b <= a: break\n            return me\n\n    def _iterative_deepening_minimax(self, g, md):\n        s = 0\n        for d in range(1, md+1):\n            try: s = self._minimax(g, d, -float('inf'), float('inf'), True)\n            except: pass\n        return s\n\n    def _minimax(self, g, d, a, b, maxing):\n        if d==0 or g.game_over: return g.evaluate_position(self.player_id)\n        m = g.get_all_valid_moves()\n        if not m: return g.evaluate_position(self.player_id)\n        if len(m)>10: m = m[:max(8, 15-d)]\n        \n        if maxing:\n            me = -float('inf')\n            for mv in m:\n                cg = g.copy()\n                cg.make_move(mv)\n                ev = self._minimax(cg, d-1, a, b, False)\n                me = max(me, ev)\n                a = max(a, ev)\n                if b<=a: \n                    if mv not in g.killer_moves[g.move_count]: g.killer_moves[g.move_count].append(mv)\n                    break\n            return me\n        else:\n            me = float('inf')\n            for mv in m:\n                cg = g.copy()\n                cg.make_move(mv)\n                ev = self._minimax(cg, d-1, a, b, True)\n                me = min(me, ev)\n                b = min(b, ev)\n                if b<=a: break\n            return me\n\n    def choose_action(self, game, training=True):\n        m = game.get_all_valid_moves()\n        if not m: return None\n        if training and random.random() < self.epsilon: return random.choice(m)\n        root = self.mcts_search(game, self.mcts_simulations)\n        if not root.children: return random.choice(m)\n        \n        if training and game.move_count < 10:\n            v = np.array([c.visit_count for c in root.children.values()])\n            p = (v ** 1.0); p /= p.sum()\n            bm = np.random.choice(list(root.children.keys()), p=p)\n        else:\n            bm = max(root.children.items(), key=lambda x: x[1].visit_count)[0]\n        \n        s = game.get_state()\n        tv = sum(c.visit_count for c in root.children.values())\n        for mv, c in root.children.items(): self.policy_table[s][mv] = c.visit_count / tv\n        self.replay_buffer.append((s, bm, root.value()))\n        return bm\n\n    def update_from_game(self, hist, res):\n        self.training_steps += 1\n        for s, m, p in hist:\n            if p != self.player_id: continue\n            rew = 1.0 if res == self.player_id else (0.0 if res == 0 else -1.0)\n            cur = self.policy_table[s][m]\n            self.policy_table[s][m] = cur + self.lr * (rew - cur)\n        \n        if len(self.replay_buffer) > 100:\n            bat = random.sample(list(self.replay_buffer), min(32, len(self.replay_buffer)))\n            for s, m, v in bat:\n                if s in self.policy_table and m in self.policy_table[s]:\n                    ov = self.policy_table[s][m]\n                    self.policy_table[s][m] = ov + 0.1 * self.lr * (v - ov)\n    \n    def decay_epsilon(self): self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    def reset_stats(self): self.wins = 0; self.losses = 0; self.draws = 0\n\ndef play_game(env, a1, a2, training=True):\n    env.reset()\n    gh = []\n    ags = {1: a1, 2: a2}\n    mc = 0\n    while not env.game_over and mc < 100:\n        cp = env.current_player\n        ag = ags[cp]\n        s = env.get_state()\n        m = ag.choose_action(env, training)\n        if m is None: break\n        gh.append((s, m, cp))\n        env.make_move(m)\n        mc += 1\n    \n    if env.winner == 1:\n        a1.wins+=1; a2.losses+=1\n        if training: a1.update_from_game(gh, 1); a2.update_from_game(gh, 1)\n    elif env.winner == 2:\n        a2.wins+=1; a1.losses+=1\n        if training: a1.update_from_game(gh, 2); a2.update_from_game(gh, 2)\n    else:\n        a1.draws+=1; a2.draws+=1\n        if training: a1.update_from_game(gh, 0); a2.update_from_game(gh, 0)\n    return env.winner","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:13.916649Z","iopub.execute_input":"2025-12-13T12:22:13.917124Z","iopub.status.idle":"2025-12-13T12:22:14.034343Z","shell.execute_reply.started":"2025-12-13T12:22:13.917103Z","shell.execute_reply":"2025-12-13T12:22:14.033503Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# 2. TRAINING & SERIALIZATION (Strictly Matches Streamlit App)\n# ============================================================================\n\n# !!! CRITICAL !!! \n# This matches the \"deserialize_move\" function in your Streamlit app exactly.\n# Keys: \"s\" (start), \"e\" (end), \"p\" (piece), \"c\" (captured), \"pr\" (promotion)\ndef serialize_move(move):\n    return {\n        \"s\": [int(x) for x in move.start],\n        \"e\": [int(x) for x in move.end],\n        \"p\": str(move.piece),\n        \"c\": str(move.captured) if move.captured else None,\n        \"pr\": str(move.promotion) if move.promotion else None\n    }\n\ndef save_kaggle_brain(agent1, agent2, filename=\"grandmaster_minichess.zip\"):\n    # Create the config dictionary your app expects\n    config = {\n        \"lr1\": agent1.lr,\n        \"mcts_sims1\": agent1.mcts_simulations,\n        \"minimax_depth1\": agent1.minimax_depth,\n        \"lr2\": agent2.lr,\n        \"mcts_sims2\": agent2.mcts_simulations,\n        \"minimax_depth2\": agent2.minimax_depth,\n        \"training_history\": {\n            \"source\": \"Kaggle GPU Training\",\n            \"final_epsilon\": agent1.epsilon,\n            \"episode\": list(range(1, agent1.training_steps + 1)) # Simple history placeholder\n        }\n    }\n\n    # Helper to clean up the policy table for JSON\n    def serialize_agent(agent, role_name):\n        clean_policy = {}\n        # Copy to avoid changing the active brain during save\n        current_policies = agent.policy_table.copy()\n        \n        for state, moves in current_policies.items():\n            try:\n                # Convert tuple state to string (Matches ast.literal_eval in your app)\n                state_str = str(state)\n                clean_policy[state_str] = {}\n                \n                for move, value in moves.items():\n                    # Serialize using the strict function above\n                    move_json_str = json.dumps(serialize_move(move))\n                    clean_policy[state_str][move_json_str] = float(value)\n            except Exception:\n                continue\n        \n        return {\n            \"metadata\": {\"role\": role_name, \"version\": \"3.1_KAGGLE\"},\n            \"policy_table\": clean_policy,\n            \"epsilon\": float(agent.epsilon),\n            \"wins\": int(agent.wins),\n            \"losses\": int(agent.losses),\n            \"draws\": int(agent.draws),\n            \"mcts_sims\": int(agent.mcts_simulations),\n            \"training_steps\": int(agent.training_steps)\n        }\n    \n    # Generate the JSON data\n    data1 = serialize_agent(agent1, \"White\")\n    data2 = serialize_agent(agent2, \"Black\")\n    \n    # Write to physical file\n    print(f\"ðŸ’¾ Saving {len(data1['policy_table'])} policies to {filename}...\")\n    with zipfile.ZipFile(filename, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(data1))\n        zf.writestr(\"agent2.json\", json.dumps(data2))\n        zf.writestr(\"config.json\", json.dumps(config))\n    print(f\"âœ… Save Complete! Download '{filename}' and upload to your Streamlit app.\")\n\n# --- THE TRAINING LOOP ---\ndef train_grandmaster():\n    # ADJUST THESE FOR KAGGLE POWER\n    EPISODES = 30    # Try 2000 or 5000 if you have time!\n    MCTS_SIMS = 100      # Higher = Smarter logic\n    DEPTH = 5            # Deeper calculation\n    CHECKPOINT_FREQ = 1 \n    \n    env = Minichess()\n    # Initialize agents\n    agent1 = Agent(1, lr=0.3)\n    agent2 = Agent(2, lr=0.3)\n    \n    # Set compute budget\n    agent1.mcts_simulations = MCTS_SIMS\n    agent2.mcts_simulations = MCTS_SIMS\n    agent1.minimax_depth = DEPTH\n    agent2.minimax_depth = DEPTH\n    \n    start_time = time.time()\n    print(f\"âš”ï¸  Beginning Grandmaster Training ({EPISODES} Episodes)...\")\n    \n    for i in range(1, EPISODES + 1):\n        # Play game\n        play_game(env, agent1, agent2, training=True)\n        \n        # Decay epsilon\n        agent1.decay_epsilon()\n        agent2.decay_epsilon()\n        \n        # Log Progress\n        if i % 10 == 0:\n            elapsed = time.time() - start_time\n            rate = i / elapsed\n            print(f\"Game {i:04d} | Policies: {len(agent1.policy_table):,} | \"\n                  f\"W: {agent1.wins} L: {agent1.losses} | Îµ: {agent1.epsilon:.3f} | {rate:.1f} games/s\")\n            \n        # Intermediate Save (Safety)\n        if i % CHECKPOINT_FREQ == 0:\n            save_kaggle_brain(agent1, agent2, filename=f\"checkpoint_{i}.zip\")\n\n    # Final Save\n    print(\"\\nðŸ† Training Finished!\")\n    save_kaggle_brain(agent1, agent2, filename=\"grandmaster_minichess.zip\")\n\n# Start Training\nif __name__ == \"__main__\":\n    train_grandmaster()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:14.035270Z","iopub.execute_input":"2025-12-13T12:22:14.035569Z"}},"outputs":[{"name":"stdout","text":"âš”ï¸  Beginning Grandmaster Training (30 Episodes)...\nðŸ’¾ Saving 49 policies to checkpoint_1.zip...\nâœ… Save Complete! Download 'checkpoint_1.zip' and upload to your Streamlit app.\nðŸ’¾ Saving 98 policies to checkpoint_2.zip...\nâœ… Save Complete! Download 'checkpoint_2.zip' and upload to your Streamlit app.\nðŸ’¾ Saving 146 policies to checkpoint_3.zip...\nâœ… Save Complete! Download 'checkpoint_3.zip' and upload to your Streamlit app.\nðŸ’¾ Saving 173 policies to checkpoint_4.zip...\nâœ… Save Complete! Download 'checkpoint_4.zip' and upload to your Streamlit app.\nðŸ’¾ Saving 220 policies to checkpoint_5.zip...\nâœ… Save Complete! Download 'checkpoint_5.zip' and upload to your Streamlit app.\nðŸ’¾ Saving 230 policies to checkpoint_6.zip...\nâœ… Save Complete! Download 'checkpoint_6.zip' and upload to your Streamlit app.\nðŸ’¾ Saving 242 policies to checkpoint_7.zip...\nâœ… Save Complete! Download 'checkpoint_7.zip' and upload to your Streamlit app.\nðŸ’¾ Saving 290 policies to checkpoint_8.zip...\nâœ… Save Complete! Download 'checkpoint_8.zip' and upload to your Streamlit app.\n","output_type":"stream"}],"execution_count":null}]}