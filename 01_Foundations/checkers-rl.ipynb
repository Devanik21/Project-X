{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:44:43.877693Z","iopub.execute_input":"2025-12-13T14:44:43.877988Z","iopub.status.idle":"2025-12-13T14:44:45.975620Z","shell.execute_reply.started":"2025-12-13T14:44:43.877964Z","shell.execute_reply":"2025-12-13T14:44:45.974753Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\nimport zipfile\nimport io\nimport math\nimport time\nfrom collections import deque, defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\n\n# ============================================================================\n# 1. DATA STRUCTURES\n# ============================================================================\n@dataclass\nclass Move:\n    start: Tuple[int, int]\n    end: Tuple[int, int]\n    captures: List[Tuple[int, int]]\n    promotion: bool = False\n    \n    def __hash__(self): return hash((self.start, self.end, tuple(self.captures)))\n    def __eq__(self, other): return (self.start == other.start and self.end == other.end and self.captures == other.captures)\n\n# ============================================================================\n# 2. CHECKERS ENVIRONMENT (Headless)\n# ============================================================================\nclass Checkers:\n    def __init__(self):\n        self.board_size = 8\n        self.reset()\n    \n    def reset(self):\n        self.board = np.zeros((8, 8), dtype=int)\n        for row in range(5, 8):\n            for col in range(8):\n                if (row + col) % 2 == 1: self.board[row, col] = 1\n        for row in range(0, 3):\n            for col in range(8):\n                if (row + col) % 2 == 1: self.board[row, col] = 2\n        self.current_player = 1\n        self.game_over = False\n        self.winner = None\n        self.move_history = []\n        self.pieces_count = {1: 12, 2: 12}\n        return self.get_state()\n    \n    def get_state(self): return tuple(self.board.flatten())\n    \n    def copy(self):\n        new_game = Checkers()\n        new_game.board = self.board.copy()\n        new_game.current_player = self.current_player\n        new_game.game_over = self.game_over\n        new_game.winner = self.winner\n        new_game.move_history = self.move_history.copy()\n        new_game.pieces_count = self.pieces_count.copy()\n        return new_game\n    \n    def get_piece_moves(self, row, col):\n        piece = self.board[row, col]\n        if piece == 0 or abs(piece) != self.current_player: return []\n        is_king = abs(piece) > 2\n        captures = self._get_captures(row, col, is_king)\n        if captures: return captures\n        return self._get_simple_moves(row, col, is_king)\n    \n    def _get_simple_moves(self, row, col, is_king):\n        moves = []\n        piece = self.board[row, col]\n        if piece == 1: directions = [(-1, -1), (-1, 1)]\n        elif piece == 2: directions = [(1, -1), (1, 1)]\n        else: directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n        \n        for dr, dc in directions:\n            nr, nc = row + dr, col + dc\n            if 0 <= nr < 8 and 0 <= nc < 8 and self.board[nr, nc] == 0:\n                promotion = (piece == 1 and nr == 0) or (piece == 2 and nr == 7)\n                moves.append(Move((row, col), (nr, nc), [], promotion))\n        return moves\n    \n    def _get_captures(self, row, col, is_king, captured=None):\n        if captured is None: captured = []\n        piece = self.board[row, col]\n        moves = []\n        if piece == 1: directions = [(-1, -1), (-1, 1)]\n        elif piece == 2: directions = [(1, -1), (1, 1)]\n        else: directions = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n        \n        for dr, dc in directions:\n            jr, jc = row + dr, col + dc\n            lr, lc = row + 2*dr, col + 2*dc\n            if not (0 <= lr < 8 and 0 <= lc < 8): continue\n            \n            enemy = self.board[jr, jc]\n            if (enemy != 0 and (enemy % 3) == (3 - self.current_player) and \n                self.board[lr, lc] == 0 and (jr, jc) not in captured):\n                \n                new_captured = captured + [(jr, jc)]\n                orig_board = self.board.copy()\n                self.board[lr, lc] = piece; self.board[row, col] = 0\n                for cr, cc in new_captured: self.board[cr, cc] = 0\n                \n                further = self._get_captures(lr, lc, is_king, new_captured)\n                self.board = orig_board\n                \n                if further: moves.extend(further)\n                else:\n                    start = self.move_history[-1].end if captured else (row, col)\n                    promo = ((piece == 1 and lr == 0) or (piece == 2 and lr == 7))\n                    moves.append(Move(start, (lr, lc), new_captured, promo))\n        return moves\n    \n    def get_all_valid_moves(self):\n        all_moves = []\n        has_captures = False\n        for r in range(8):\n            for c in range(8):\n                p = self.board[r, c]\n                if p != 0 and abs(p) % 3 == self.current_player:\n                    moves = self.get_piece_moves(r, c)\n                    if moves and moves[0].captures: has_captures = True\n                    all_moves.extend(moves)\n        if has_captures: all_moves = [m for m in all_moves if m.captures]\n        return all_moves\n    \n    def make_move(self, move):\n        if self.game_over: return self.get_state(), 0, True\n        sr, sc = move.start; er, ec = move.end\n        piece = self.board[sr, sc]\n        self.board[er, ec] = piece; self.board[sr, sc] = 0\n        \n        points = 0\n        for cr, cc in move.captures:\n            cap = self.board[cr, cc]; self.board[cr, cc] = 0\n            points += 3 if abs(cap) > 2 else 1\n            self.pieces_count[3 - self.current_player] -= 1\n            \n        if move.promotion or (piece == 1 and er == 0) or (piece == 2 and er == 7):\n            self.board[er, ec] = piece + 2; points += 2\n            \n        self.move_history.append(move)\n        reward = points\n        opponent = 3 - self.current_player\n        \n        if self.pieces_count[opponent] == 0:\n            self.game_over = True; self.winner = self.current_player; reward = 100\n        else:\n            self.current_player = opponent\n            if not self.get_all_valid_moves():\n                self.game_over = True; self.winner = 3 - opponent; self.current_player = 3 - opponent; reward = 100\n        return self.get_state(), reward, self.game_over\n    \n    def evaluate_position(self, player):\n        if self.winner == player: return 100000\n        if self.winner == (3 - player): return -100000\n        opp = 3 - player; score = 0\n        pst = [[0, 4, 0, 4, 0, 4, 0, 4], [4, 0, 3, 0, 3, 0, 3, 0], [0, 3, 0, 2, 0, 2, 0, 4], [4, 0, 5, 0, 5, 0, 3, 0],\n               [0, 3, 0, 5, 0, 5, 0, 4], [4, 0, 2, 0, 2, 0, 3, 0], [0, 3, 0, 3, 0, 3, 0, 4], [4, 0, 4, 0, 4, 0, 4, 0]]\n        for r in range(8):\n            for c in range(8):\n                p = self.board[r, c]\n                if p == 0: continue\n                is_mine = (abs(p) % 3 == player)\n                val = 500 if abs(p) > 2 else 100\n                pos_val = pst[r][c] if player == 1 else pst[7-r][c]\n                adv = (7 - r) * 3 if player == 1 else r * 3\n                if is_mine: score += val + (pos_val * 5) + adv\n                else: score -= val + (pos_val * 5) + adv\n        return score\n\n# ============================================================================\n# 3. MCTS & AGENT\n# ============================================================================\nclass MCTSNode:\n    def __init__(self, game, parent=None, move=None, prior=1.0):\n        self.game = game; self.parent = parent; self.move = move; self.prior = prior\n        self.children = {}; self.visit_count = 0; self.value_sum = 0.0; self.is_expanded = False\n    def value(self): return self.value_sum / self.visit_count if self.visit_count > 0 else 0\n    def ucb(self, pv, c=1.5):\n        q = self.value() if self.visit_count > 0 else 0\n        return q + c * self.prior * math.sqrt(pv) / (1 + self.visit_count)\n    def select(self, c=1.5): return max(self.children.values(), key=lambda c_node: c_node.ucb(self.visit_count, c))\n    def expand(self, game, priors):\n        moves = game.get_all_valid_moves(); tp = sum(priors.values()) or len(moves)\n        for m in moves:\n            p = priors.get(m, 1.0) / tp\n            cg = game.copy(); cg.make_move(m)\n            self.children[m] = MCTSNode(cg, self, m, p)\n        self.is_expanded = True\n    def backup(self, v):\n        self.visit_count += 1; self.value_sum += v\n        if self.parent: self.parent.backup(-v)\n\nclass Agent:\n    def __init__(self, pid, lr=0.3, gamma=0.99):\n        self.player_id = pid; self.lr = lr; self.gamma = gamma\n        self.epsilon = 1.0; self.epsilon_decay = 0.96; self.epsilon_min = 0.01\n        self.mcts_simulations = 250; self.minimax_depth = 4\n        self.policy_table = defaultdict(lambda: defaultdict(float))\n        self.wins = 0; self.losses = 0; self.draws = 0\n\n    def get_priors(self, game):\n        state = game.get_state(); moves = game.get_all_valid_moves(); priors = {}\n        for m in moves:\n            if state in self.policy_table and m in self.policy_table[state]: priors[m] = self.policy_table[state][m]\n            else:\n                p = 1.0 + (len(m.captures) * 2) + (1.0 if m.promotion else 0)\n                if 2 <= m.end[0] <= 5 and 2 <= m.end[1] <= 5: p += 0.5\n                priors[m] = p\n        return priors\n\n    def mcts(self, game, sims):\n        root = MCTSNode(game.copy())\n        for _ in range(sims):\n            node = root; sg = game.copy()\n            while node.is_expanded and node.children:\n                node = node.select(); sg.make_move(node.move)\n            if not sg.game_over: node.expand(sg, self.get_priors(sg))\n            val = self._eval_leaf(sg); node.backup(val)\n        return root\n\n    def _eval_leaf(self, game):\n        if game.game_over: return 1.0 if game.winner == self.player_id else (-1.0 if game.winner else 0.0)\n        return np.tanh(self._minimax(game, self.minimax_depth, -float('inf'), float('inf'), True) / 500)\n\n    def _minimax(self, g, d, a, b, maxing):\n        if d == 0 or g.game_over: return g.evaluate_position(self.player_id)\n        moves = g.get_all_valid_moves()[:4] # Limit search width\n        if maxing:\n            me = -float('inf')\n            for m in moves:\n                cg = g.copy(); cg.make_move(m)\n                me = max(me, self._minimax(cg, d-1, a, b, False)); a = max(a, me)\n                if b <= a: break\n            return me\n        else:\n            me = float('inf')\n            for m in moves:\n                cg = g.copy(); cg.make_move(m)\n                me = min(me, self._minimax(cg, d-1, a, b, True)); b = min(b, me)\n                if b <= a: break\n            return me\n\n    def choose_action(self, game, training=True):\n        moves = game.get_all_valid_moves()\n        if not moves: return None\n        if training and random.random() < self.epsilon: return random.choice(moves)\n        root = self.mcts(game, self.mcts_simulations)\n        if not root.children: return random.choice(moves)\n        best = max(root.children.items(), key=lambda x: x[1].visit_count)[0]\n        \n        # Policy Learning\n        s = game.get_state(); tv = sum(c.visit_count for c in root.children.values())\n        for m, c in root.children.items(): self.policy_table[s][m] = c.visit_count / tv\n        return best\n\n    def update(self, hist, res):\n        for s, m, p in hist:\n            if p != self.player_id: continue\n            r = 1.0 if res == self.player_id else (0.0 if res == 0 else -1.0)\n            curr = self.policy_table[s][m]\n            self.policy_table[s][m] = curr + self.lr * (r - curr)\n            \n    def decay_epsilon(self): self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    def reset_stats(self): self.wins=0; self.losses=0; self.draws=0\n\ndef play_game(env, a1, a2, training=True):\n    env.reset(); hist = []; agents = {1: a1, 2: a2}; mc = 0\n    while not env.game_over and mc < 200:\n        cp = env.current_player; ag = agents[cp]\n        s = env.get_state(); m = ag.choose_action(env, training)\n        if m is None: break\n        hist.append((s, m, cp)); env.make_move(m); mc += 1\n    \n    if env.winner == 1:\n        a1.wins+=1; a2.losses+=1\n        if training: a1.update(hist, 1); a2.update(hist, 1)\n    elif env.winner == 2:\n        a2.wins+=1; a1.losses+=1\n        if training: a1.update(hist, 2); a2.update(hist, 2)\n    else:\n        a1.draws+=1; a2.draws+=1\n        if training: a1.update(hist, 0); a2.update(hist, 0)\n    return env.winner","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:44:45.977294Z","iopub.execute_input":"2025-12-13T14:44:45.977695Z","iopub.status.idle":"2025-12-13T14:44:46.070908Z","shell.execute_reply.started":"2025-12-13T14:44:45.977676Z","shell.execute_reply":"2025-12-13T14:44:46.070073Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# 1. SERIALIZATION (Matches Streamlit deserialize_move EXACTLY)\n# ============================================================================\ndef serialize_move(move):\n    \"\"\"Convert Move object to JSON-dict for Streamlit compatibility\"\"\"\n    return {\n        \"s\": [int(x) for x in move.start],     # 's' for start\n        \"e\": [int(x) for x in move.end],       # 'e' for end\n        \"c\": [[int(x) for x in c] for c in move.captures], # 'c' for captures\n        \"p\": bool(move.promotion)              # 'p' for promotion\n    }\n\ndef save_kaggle_brain(agent1, agent2, config, filename=\"checkers.zip\"):\n    # Helper to serialize an agent's policy table\n    def serialize_agent(agent, role):\n        clean_policy = {}\n        for state, moves in agent.policy_table.items():\n            try:\n                # 1. Convert state tuple (ints) to string key\n                clean_state = tuple(int(x) for x in state)\n                state_str = str(clean_state)\n                \n                clean_policy[state_str] = {}\n                for move, value in moves.items():\n                    # 2. Serialize Move object using the helper above\n                    move_json_str = json.dumps(serialize_move(move))\n                    clean_policy[state_str][move_json_str] = float(value)\n            except Exception: continue\n            \n        return {\n            \"metadata\": {\"role\": role, \"version\": \"KAGGLE_V1\"},\n            \"policy_table\": clean_policy,\n            \"epsilon\": float(agent.epsilon),\n            \"wins\": int(agent.wins),\n            \"losses\": int(agent.losses),\n            \"draws\": int(agent.draws),\n            \"mcts_sims\": int(agent.mcts_simulations)\n        }\n\n    a1_data = serialize_agent(agent1, \"Red\")\n    a2_data = serialize_agent(agent2, \"White\")\n    \n    print(f\"ðŸ’¾ Saving: Red Policies={len(a1_data['policy_table'])}, White Policies={len(a2_data['policy_table'])}\")\n    \n    with zipfile.ZipFile(filename, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        zf.writestr(\"agent1.json\", json.dumps(a1_data, indent=2))\n        zf.writestr(\"agent2.json\", json.dumps(a2_data, indent=2))\n        zf.writestr(\"config.json\", json.dumps(config, indent=2))\n    print(f\"âœ… Save Complete! Download '{filename}'\")\n\n# ============================================================================\n# 2. TRAINING RUNNER\n# ============================================================================\ndef run_checkers_training():\n    EPISODES = 50  # Checkers is slow; 500 is a good start. Try 2000 if patient.\n    \n    env = Checkers()\n    agent1 = Agent(1, lr=0.3, gamma=0.99)\n    agent2 = Agent(2, lr=0.3, gamma=0.99)\n    \n    # Brain Power\n    agent1.mcts_simulations = 100\n    agent1.minimax_depth = 5\n    agent2.mcts_simulations = 100\n    agent2.minimax_depth = 5\n    \n    history = {\n        'agent1_wins': [], 'agent2_wins': [], 'draws': [],\n        'agent1_epsilon': [], 'agent2_epsilon': [],\n        'agent1_policies': [], 'agent2_policies': [],\n        'episode': []\n    }\n    \n    print(f\"ðŸš€ Starting Checkers Training ({EPISODES} Episodes)...\")\n    start_time = time.time()\n    \n    for ep in range(1, EPISODES + 1):\n        play_game(env, agent1, agent2, training=True)\n        \n        agent1.decay_epsilon()\n        agent2.decay_epsilon()\n        \n        if ep % 10 == 0:\n            history['agent1_wins'].append(agent1.wins)\n            history['agent2_wins'].append(agent2.wins)\n            history['draws'].append(agent1.draws)\n            history['agent1_epsilon'].append(agent1.epsilon)\n            history['agent2_epsilon'].append(agent2.epsilon)\n            history['agent1_policies'].append(len(agent1.policy_table))\n            history['agent2_policies'].append(len(agent2.policy_table))\n            history['episode'].append(ep)\n            \n        if ep % 1 == 0:\n            elapsed = time.time() - start_time\n            print(f\"Ep {ep}/{EPISODES} | Red Wins: {agent1.wins} | Wht Wins: {agent2.wins} | {elapsed:.1f}s\")\n\n    config = {\n        \"lr1\": agent1.lr, \"gamma1\": agent1.gamma,\n        \"lr2\": agent2.lr, \"gamma2\": agent2.gamma,\n        \"training_history\": history\n    }\n    \n    print(\"\\nðŸ† Training Finished!\")\n    save_kaggle_brain(agent1, agent2, config)\n\nif __name__ == \"__main__\":\n    run_checkers_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:46:36.756723Z","iopub.execute_input":"2025-12-13T14:46:36.757249Z","iopub.status.idle":"2025-12-13T15:13:48.107147Z","shell.execute_reply.started":"2025-12-13T14:46:36.757226Z","shell.execute_reply":"2025-12-13T15:13:48.106490Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting Checkers Training (50 Episodes)...\nEp 1/50 | Red Wins: 1 | Wht Wins: 0 | 0.0s\nEp 2/50 | Red Wins: 1 | Wht Wins: 1 | 2.6s\nEp 3/50 | Red Wins: 1 | Wht Wins: 2 | 4.8s\nEp 4/50 | Red Wins: 1 | Wht Wins: 3 | 13.5s\nEp 5/50 | Red Wins: 1 | Wht Wins: 4 | 24.6s\nEp 6/50 | Red Wins: 1 | Wht Wins: 5 | 39.9s\nEp 7/50 | Red Wins: 1 | Wht Wins: 6 | 47.6s\nEp 8/50 | Red Wins: 2 | Wht Wins: 6 | 62.0s\nEp 9/50 | Red Wins: 2 | Wht Wins: 7 | 76.4s\nEp 10/50 | Red Wins: 3 | Wht Wins: 7 | 95.6s\nEp 11/50 | Red Wins: 3 | Wht Wins: 8 | 121.5s\nEp 12/50 | Red Wins: 4 | Wht Wins: 8 | 138.7s\nEp 13/50 | Red Wins: 5 | Wht Wins: 8 | 162.2s\nEp 14/50 | Red Wins: 6 | Wht Wins: 8 | 181.4s\nEp 15/50 | Red Wins: 6 | Wht Wins: 9 | 207.3s\nEp 16/50 | Red Wins: 7 | Wht Wins: 9 | 236.8s\nEp 17/50 | Red Wins: 8 | Wht Wins: 9 | 269.7s\nEp 18/50 | Red Wins: 8 | Wht Wins: 10 | 296.2s\nEp 19/50 | Red Wins: 8 | Wht Wins: 11 | 311.4s\nEp 20/50 | Red Wins: 8 | Wht Wins: 12 | 346.9s\nEp 21/50 | Red Wins: 9 | Wht Wins: 12 | 376.7s\nEp 22/50 | Red Wins: 10 | Wht Wins: 12 | 402.5s\nEp 23/50 | Red Wins: 11 | Wht Wins: 12 | 441.3s\nEp 24/50 | Red Wins: 11 | Wht Wins: 13 | 469.3s\nEp 25/50 | Red Wins: 12 | Wht Wins: 13 | 507.1s\nEp 26/50 | Red Wins: 12 | Wht Wins: 14 | 542.3s\nEp 27/50 | Red Wins: 13 | Wht Wins: 14 | 573.7s\nEp 28/50 | Red Wins: 14 | Wht Wins: 14 | 602.4s\nEp 29/50 | Red Wins: 15 | Wht Wins: 14 | 636.8s\nEp 30/50 | Red Wins: 15 | Wht Wins: 15 | 691.8s\nEp 31/50 | Red Wins: 16 | Wht Wins: 15 | 739.7s\nEp 32/50 | Red Wins: 17 | Wht Wins: 15 | 788.1s\nEp 33/50 | Red Wins: 18 | Wht Wins: 15 | 836.0s\nEp 34/50 | Red Wins: 19 | Wht Wins: 15 | 875.9s\nEp 35/50 | Red Wins: 19 | Wht Wins: 16 | 923.3s\nEp 36/50 | Red Wins: 20 | Wht Wins: 16 | 961.4s\nEp 37/50 | Red Wins: 20 | Wht Wins: 17 | 993.6s\nEp 38/50 | Red Wins: 20 | Wht Wins: 18 | 1059.2s\nEp 39/50 | Red Wins: 21 | Wht Wins: 18 | 1110.9s\nEp 40/50 | Red Wins: 22 | Wht Wins: 18 | 1163.3s\nEp 41/50 | Red Wins: 23 | Wht Wins: 18 | 1214.1s\nEp 42/50 | Red Wins: 24 | Wht Wins: 18 | 1268.6s\nEp 43/50 | Red Wins: 24 | Wht Wins: 19 | 1301.2s\nEp 44/50 | Red Wins: 25 | Wht Wins: 19 | 1345.8s\nEp 45/50 | Red Wins: 25 | Wht Wins: 20 | 1390.6s\nEp 46/50 | Red Wins: 25 | Wht Wins: 21 | 1438.2s\nEp 47/50 | Red Wins: 26 | Wht Wins: 21 | 1477.2s\nEp 48/50 | Red Wins: 27 | Wht Wins: 21 | 1526.8s\nEp 49/50 | Red Wins: 28 | Wht Wins: 21 | 1575.3s\nEp 50/50 | Red Wins: 28 | Wht Wins: 22 | 1631.2s\n\nðŸ† Training Finished!\nðŸ’¾ Saving: Red Policies=1327, White Policies=1315\nâœ… Save Complete! Download 'checkers.zip'\n","output_type":"stream"}],"execution_count":5}]}