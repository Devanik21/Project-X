{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"authorship_tag":"ABX9TyN9EdLoaEVchdZu9U3Y0KD7"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\nA comprehensive notebook on Reinforcement Learning, covering core concepts, MDPs, value functions, Bellman equations, dynamic programming (Value and Policy Iteration), Monte Carlo methods, Temporal Difference Learning (SARSA, Q-Learning), function approximation, Deep Q-Networks (DQN), policy gradient methods (REINFORCE), Actor-Critic methods, and advanced topics, culminating in a project example using a Gymnasium environment and a final summary.","metadata":{"id":"acba7c36"}},{"cell_type":"markdown","source":"## Introduction to Reinforcement Learning\n\n\nProvide an overview of Reinforcement Learning (RL), its core concepts (agent, environment, states, actions, rewards), and typical problem setups. Explain the goal of an RL agent.\n","metadata":{"id":"fe28d36f"}},{"cell_type":"markdown","source":"### Reinforcement Learning (RL) Overview\n\n1.  **What is Reinforcement Learning?**\n    Reinforcement Learning (RL) is a paradigm of machine learning where an `agent` learns to make optimal decisions by performing actions in an `environment` to maximize a cumulative `reward`. It's a trial-and-error process, much like how humans or animals learn through interaction and feedback.\n\n2.  **Core Concepts of an RL System:**\n    *   **Agent**: This is the learner or decision-maker. The agent observes the environment, makes decisions (chooses actions), and learns from the consequences of those actions.\n    *   **Environment**: This encompasses everything outside the agent with which the agent interacts. It defines the context in which the agent operates and responds to the agent's actions by presenting new states and providing rewards.\n    *   **State (S)**: A state represents a complete description of the environment at a particular moment in time. It provides all necessary information for the agent to make an informed decision about what action to take next.\n    *   **Action (A)**: An action is a move or decision made by the agent within the environment. The set of available actions can vary depending on the current state.\n    *   **Reward (R)**: A reward is a numerical feedback signal given by the environment to the agent after it performs an action in a given state. A positive reward encourages the agent to repeat the action, while a negative reward (penalty) discourages it. The immediate reward signals the desirability of an action.\n\n3.  **Typical RL Problem Setup:**\n    In a typical RL problem, the agent begins by observing the current `state` of the environment. Based on this observation, the agent selects an `action` to perform. After executing the action, the environment transitions to a new `state` and provides a `reward` signal back to the agent. This cycle of observation, action, new state, and reward continues, allowing the agent to continuously learn and refine its behavior.\n\n4.  **Goal of an RL Agent:**\n    The ultimate goal of an RL agent is to learn an optimal `policy`. A policy (often denoted as Ï€) is a strategy that the agent uses to decide what action to take in each state. The optimal policy is one that maximizes the total cumulative (summed over time, often discounted) reward the agent receives over the long run. This means the agent aims not just for immediate rewards, but for a sequence of actions that leads to the greatest long-term gain.","metadata":{"id":"ee684210"}},{"cell_type":"markdown","source":"## Markov Decision Processes (MDPs)\n\n### Subtask:\nDetail the mathematical framework of MDPs, including states, actions, transition probabilities, and reward functions. Illustrate with a simple example.\n","metadata":{"id":"4a18c9db"}},{"cell_type":"markdown","source":"### Markov Decision Processes (MDPs)\n\nMarkov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. They are fundamental to reinforcement learning, allowing us to formally describe an environment in which an agent operates.\n\nAn MDP is formally defined by five key components, often represented as a tuple $(S, A, P, R, \\gamma)$:\n\n1.  **States (S)**: A finite set of possible states the agent can be in. Each state represents a specific configuration or situation of the environment.\n\n2.  **Actions (A)**: A finite set of actions available to the agent. At each state, the agent chooses an action from this set. In some cases, actions might be state-dependent, meaning $A_s$ is the set of actions available in state $s$.\n\n3.  **Transition Probability Function (P)**: Also known as the transition model, $P(s' | s, a)$ is the probability of transitioning to state $s'$ from state $s$ after taking action $a$. This function captures the stochastic nature of the environment.\n\n4.  **Reward Function (R)**: $R(s, a, s')$ (or sometimes $R(s, a)$ or $R(s')$) defines the immediate reward an agent receives after transitioning from state $s$ to state $s'$ by taking action $a$. The goal of the agent is typically to maximize the cumulative reward over time.\n\n5.  **Discount Factor (\\gamma)**: A value between 0 and 1 (inclusive), $\\gamma \\in [0, 1]$. The discount factor determines the present value of future rewards. A $\\gamma$ close to 0 makes the agent short-sighted, only caring about immediate rewards, while a $\\gamma$ close to 1 makes the agent consider future rewards more heavily.\n\n### Simple Example: A 2x2 Grid World\n\nConsider a simple 2x2 grid world where an agent wants to reach a goal. The grid has a starting cell, a goal cell, and an obstacle cell.\n\n| Start |   |\n|-------|---|\n| Obstacle | Goal |\n\nLet's define the MDP components for this scenario:\n\n*   **States (S)**: The cells in the grid. Let's label them as `(row, col)`:\n    *   $S = \\{ (0,0), (0,1), (1,0), (1,1) \\}$\n    *   $(0,0)$ = Start\n    *   $(0,1)$ = Empty cell\n    *   $(1,0)$ = Obstacle\n    *   $(1,1)$ = Goal\n\n*   **Actions (A)**: The agent can move in four directions:\n    *   $A = \\{ \\text{Up, Down, Left, Right} \\}$\n\n*   **Transition Probability Function (P)**: For simplicity, assume deterministic transitions. If the agent takes an action, it moves to the intended cell with probability 1, unless it hits a wall or the obstacle. If it tries to move into an obstacle or outside the grid, it stays in its current state.\n    *   Example: $P((0,1) | (0,0), \\text{Right}) = 1$\n    *   Example: $P((0,0) | (0,0), \\text{Left}) = 1$ (hits wall)\n    *   Example: $P((0,0) | (1,0), \\text{Up}) = 1$ (tries to move from obstacle, stays in obstacle)\n    *   When the agent reaches the Goal state (1,1), it stays there for all future actions (absorbing state).\n\n*   **Reward Function (R)**: The agent receives rewards based on its transitions:\n    *   $R(s, a, (1,1)) = +10$ if the agent reaches the Goal state.\n    *   $R(s, a, (1,0)) = -5$ if the agent moves into the Obstacle state (or tries to move from it).\n    *   $R(s, a, s') = -1$ for any other transition (cost of movement).\n\n*   **Discount Factor (\\gamma)**: Let's choose $\\gamma = 0.9$ to encourage the agent to reach the goal sooner rather than later, but still consider future rewards.","metadata":{"id":"b597ce44"}},{"cell_type":"markdown","source":"## Value Functions and Bellman Equations\n\n### Subtask:\nIntroduce state-value functions (V-value) and action-value functions (Q-value). Derive and explain the Bellman equations for optimality and expectations.\n","metadata":{"id":"159a73d9"}},{"cell_type":"markdown","source":"## Value Functions and Bellman Equations\n\nReinforcement Learning (RL) agents learn to make decisions by interacting with an environment. To guide these decisions, agents use **value functions** to estimate the 'goodness' of being in a particular state or taking a particular action in a state. These functions are crucial for planning and learning in sequential decision-making problems.\n\n### State-Value Function ($V_{\\pi}(s)$)\n\nThe **state-value function**, denoted as $V_{\\pi}(s)$, represents the expected cumulative discounted reward an agent can expect to receive starting from state $s$ and following a specific policy $\\pi$ thereafter. A policy $\\pi$ is a mapping from states to probabilities of selecting each possible action. Essentially, $V_{\\pi}(s)$ tells us \"how good\" it is to be in state $s$ when following policy $\\pi$.\n\nMathematically, the state-value function for a policy $\\pi$ is defined as:\n\n$$V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$$\n\nwhere:\n*   $G_t$ is the return (total discounted reward) from time $t$.\n*   $S_t = s$ means that the agent is in state $s$ at time $t$.\n*   $\\mathbb{E}_{\\pi}[.]$ denotes the expected value given that the agent follows policy $\\pi$.\n*   The return $G_t$ is the sum of discounted future rewards: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n*   $\\gamma \\in [0, 1]$ is the discount factor, which determines the present value of future rewards. A $\\gamma$ close to 0 makes the agent short-sighted, while a $\\gamma$ close to 1 makes it long-sighted.\n\n### Action-Value Function ($Q_{\\pi}(s, a)$)\n\nThe **action-value function**, denoted as $Q_{\\pi}(s, a)$, represents the expected cumulative discounted reward an agent can expect to receive starting from state $s$, taking action $a$, and then following policy $\\pi$ thereafter. $Q_{\\pi}(s, a)$ tells us \"how good\" it is to take action $a$ in state $s$ when following policy $\\pi$.\n\nMathematically, the action-value function for a policy $\\pi$ is defined as:\n\n$$Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t | S_t = s, A_t = a]$$\n\nwhere:\n*   $A_t = a$ means the agent takes action $a$ at time $t$.\n*   All other symbols are the same as defined for $V_{\\pi}(s)$.\n\n### Bellman Equations for Expectations\n\nThe Bellman equations express a fundamental property of value functions: they relate the value of a state (or state-action pair) to the values of successor states (or state-action pairs). These equations are recursive and form the basis for many RL algorithms.\n\n#### Bellman Expectation Equation for $V_{\\pi}(s)$\n\nThe state-value function $V_{\\pi}(s)$ can be decomposed into the immediate reward plus the discounted expected value of the next state:\n\n$$V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma V_{\\pi}(s')]$$\n\n**Explanation:**\n*   The outer sum iterates over all possible actions $a$ that can be taken from state $s$, weighted by their probabilities under policy $\\pi$ (i.e., $\\pi(a|s)$). This is because the agent follows policy $\\pi$ to choose an action.\n*   The inner sum iterates over all possible next states $s'$ and rewards $r$ that can result from taking action $a$ in state $s$, weighted by the transition probability $p(s', r | s, a)$.\n*   The term $[r + \\gamma V_{\\pi}(s')]$ represents the immediate reward $r$ obtained from the transition plus the discounted value of the next state $s'$, assuming we continue to follow policy $\\pi$.\n\nThis equation means that the value of a state $s$ is the expected sum of the immediate reward and the discounted value of the next state, averaged over all possible actions chosen by policy $\\pi$ and all possible next states and rewards generated by the environment.\n\n#### Bellman Expectation Equation for $Q_{\\pi}(s, a)$\n\nSimilarly, the action-value function $Q_{\\pi}(s, a)$ can be expressed recursively:\n\n$$Q_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma \\sum_{a' \\in \\mathcal{A}} \\pi(a'|s') Q_{\\pi}(s', a')]$$\n\n**Explanation:**\n*   Starting in state $s$ and taking action $a$, the agent transitions to a next state $s'$ and receives reward $r$ with probability $p(s', r | s, a)$.\n*   From the next state $s'$, the agent then chooses an action $a'$ according to policy $\\pi$ (i.e., $\\pi(a'|s')$), leading to the action-value $Q_{\\pi}(s', a')$.\n*   The equation states that $Q_{\\pi}(s, a)$ is the expected immediate reward $r$ plus the discounted expected action-value of the next state-action pair $(s', a')$, averaged over all possible next states $s'$ and rewards $r$ given the current action $a$, and then over all possible next actions $a'$ chosen by policy $\\pi$.\n\nAlternatively, we can express $Q_{\\pi}(s,a)$ in terms of $V_{\\pi}(s')$:\n\n$$Q_{\\pi}(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_{\\pi}(s')]$$\n\nAnd $V_{\\pi}(s)$ in terms of $Q_{\\pi}(s,a)$:\n\n$$V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) Q_{\\pi}(s, a)$$\n\nThese two relationships allow us to move between state-value and action-value functions under a given policy.\n\n### Bellman Equations for Optimality\n\nWhile the expectation equations describe the value functions for a *given* policy $\\pi$, the **Bellman optimality equations** describe the value functions for the *optimal* policy, denoted as $\\pi^*$. The optimal policy is one that yields the maximum possible expected return from any state. The optimal value functions are denoted as $V^*(s)$ and $Q^*(s, a)$.\n\n#### Optimal State-Value Function ($V^*(s)$)\n\nThe optimal state-value function $V^*(s)$ is the maximum value function over all policies:\n\n$$V^*(s) = \\max_{\\pi} V_{\\pi}(s)$$\n\nThe Bellman optimality equation for $V^*(s)$ states that the value of an optimal state is the maximum over all actions of the expected return from taking that action:\n\n$$V^*(s) = \\max_{a \\in \\mathcal{A}} \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma V^*(s')]$$\n\n**Explanation:**\n*   Unlike the expectation equation, which averages over actions according to a policy $\\pi(a|s)$, the optimality equation uses a $\\max$ operator. This signifies that an optimal agent will always choose the action $a$ that leads to the highest possible expected future return.\n*   The term inside the $\\max$ is the expected return if action $a$ is chosen, which includes the immediate reward $r$ and the discounted optimal value of the next state $V^*(s')$.\n\n#### Optimal Action-Value Function ($Q^*(s, a)$)\n\nThe optimal action-value function $Q^*(s, a)$ is the maximum action-value function over all policies:\n\n$$Q^*(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)$$\n\nThe Bellman optimality equation for $Q^*(s, a)$ states that the value of an optimal state-action pair is the expected return from taking that action, followed by acting optimally thereafter:\n\n$$Q^*(s, a) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma \\max_{a' \\in \\mathcal{A}} Q^*(s', a')]$$\n\n**Explanation:**\n*   After taking action $a$ from state $s$, the environment transitions to $s'$ with reward $r$. From this new state $s'$, the agent then chooses the *optimal* action $a'$ (i.e., the one that maximizes the $Q$-value from $s'$), which is represented by $\\max_{a' \\in \\mathcal{A}} Q^*(s', a')$.\n*   This equation essentially says that the optimal value of taking action $a$ in state $s$ is the immediate reward plus the discounted optimal value of the best possible next action in the subsequent state.\n\nAlternatively, we can express the relationship between $V^*(s)$ and $Q^*(s, a)$ for optimal functions:\n\n$$V^*(s) = \\max_{a \\in \\mathcal{A}} Q^*(s, a)$$\n\n$$Q^*(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$$\n\n### Summary of Differences\n\n| Feature             | Bellman Expectation Equations                                 | Bellman Optimality Equations                                      |\n| :------------------ | :------------------------------------------------------------ | :---------------------------------------------------------------- |\n| **Purpose**         | Evaluate a **given policy** $\\pi$ (predicting returns if we follow $\\pi$). | Find the **optimal policy** $\\pi^*$ (yielding maximal returns).   |\n| **Action Selection**| Averages over actions weighted by the given policy $\\pi(a|s)$. | Uses a $\\max$ operator to select the best action.                  |\n| **Role of Policy**  | Policy $\\pi$ is explicitly part of the equation and determines action probabilities. | The optimal policy $\\pi^*$ is implicit; it's the policy that chooses the actions leading to the maximum value. |\n\nIn essence, the expectation equations help us understand how good a particular way of acting is, while the optimality equations tell us what the best possible way of acting is.","metadata":{"id":"4fbd7810"}},{"cell_type":"markdown","source":"```markdown\n## Value Functions and Bellman Equations\n\nReinforcement Learning (RL) agents learn to make decisions by interacting with an environment. To guide these decisions, agents use **value functions** to estimate the 'goodness' of being in a particular state or taking a particular action in a state. These functions are crucial for planning and learning in sequential decision-making problems.\n\n### State-Value Function ($V_{\\pi}(s)$)\n\nThe **state-value function**, denoted as $V_{\\pi}(s)$, represents the expected cumulative discounted reward an agent can expect to receive starting from state $s$ and following a specific policy $\\pi$ thereafter. A policy $\\pi$ is a mapping from states to probabilities of selecting each possible action. Essentially, $V_{\\pi}(s)$ tells us \"how good\" it is to be in state $s$ when following policy $\\pi$.\n\nMathematically, the state-value function for a policy $\\pi$ is defined as:\n\n$$V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$$\n\nwhere:\n*   $G_t$ is the return (total discounted reward) from time $t$.\n*   $S_t = s$ means that the agent is in state $s$ at time $t$.\n*   $\\mathbb{E}_{\\pi}[.]$ denotes the expected value given that the agent follows policy $\\pi$.\n*   The return $G_t$ is the sum of discounted future rewards: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n*   $\\gamma \\in [0, 1]$ is the discount factor, which determines the present value of future rewards. A $\\gamma$ close to 0 makes the agent short-sighted, while a $\\gamma$ close to 1 makes it long-sighted.\n\n### Action-Value Function ($Q_{\\pi}(s, a)$)\n\nThe **action-value function**, denoted as $Q_{\\pi}(s, a)$, represents the expected cumulative discounted reward an agent can expect to receive starting from state $s$, taking action $a$, and then following policy $\\pi$ thereafter. $Q_{\\pi}(s, a)$ tells us \"how good\" it is to take action $a$ in state $s$ when following policy $\\pi$.\n\nMathematically, the action-value function for a policy $\\pi$ is defined as:\n\n$$Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t | S_t = s, A_t = a]$$\n\nwhere:\n*   $A_t = a$ means the agent takes action $a$ at time $t$.\n*   All other symbols are the same as defined for $V_{\\pi}(s)$.\n\n### Bellman Equations for Expectations\n\nThe Bellman equations express a fundamental property of value functions: they relate the value of a state (or state-action pair) to the values of successor states (or state-action pairs). These equations are recursive and form the basis for many RL algorithms.\n\n#### Bellman Expectation Equation for $V_{\\pi}(s)$\n\nThe state-value function $V_{\\pi}(s)$ can be decomposed into the immediate reward plus the discounted expected value of the next state:\n\n$$V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma V_{\\pi}(s')]$$\n\n**Explanation:**\n*   The outer sum iterates over all possible actions $a$ that can be taken from state $s$, weighted by their probabilities under policy $\\pi$ (i.e., $\\pi(a|s)$). This is because the agent follows policy $\\pi$ to choose an action.\n*   The inner sum iterates over all possible next states $s'$ and rewards $r$ that can result from taking action $a$ in state $s$, weighted by the transition probability $p(s', r | s, a)$.\n*   The term $[r + \\gamma V_{\\pi}(s')]$ represents the immediate reward $r$ obtained from the transition plus the discounted value of the next state $s'$, assuming we continue to follow policy $\\pi$.\n\nThis equation means that the value of a state $s$ is the expected sum of the immediate reward and the discounted value of the next state, averaged over all possible actions chosen by policy $\\pi$ and all possible next states and rewards generated by the environment.\n\n#### Bellman Expectation Equation for $Q_{\\pi}(s, a)$\n\nSimilarly, the action-value function $Q_{\\pi}(s, a)$ can be expressed recursively:\n\n$$Q_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma \\sum_{a' \\in \\mathcal{A}} \\pi(a'|s') Q_{\\pi}(s', a')]$$\n\n**Explanation:**\n*   Starting in state $s$ and taking action $a$, the agent transitions to a next state $s'$ and receives reward $r$ with probability $p(s', r | s, a)$.\n*   From the next state $s'$, the agent then chooses an action $a'$ according to policy $\\pi$ (i.e., $\\pi(a'|s')$), leading to the action-value $Q_{\\pi}(s', a')$.\n*   The equation states that $Q_{\\pi}(s, a)$ is the expected immediate reward $r$ plus the discounted expected action-value of the next state-action pair $(s', a')$, averaged over all possible next states $s'$ and rewards $r$ given the current action $a$, and then over all possible next actions $a'$ chosen by policy $\\pi$.\n\nAlternatively, we can express $Q_{\\pi}(s,a)$ in terms of $V_{\\pi}(s')$:\n\n$$Q_{\\pi}(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_{\\pi}(s')]$$\n\nAnd $V_{\\pi}(s)$ in terms of $Q_{\\pi}(s,a)$:\n\n$$V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) Q_{\\pi}(s, a)$$\n\nThese two relationships allow us to move between state-value and action-value functions under a given policy.\n\n### Bellman Equations for Optimality\n\nWhile the expectation equations describe the value functions for a *given* policy $\\pi$, the **Bellman optimality equations** describe the value functions for the *optimal* policy, denoted as $\\pi^*$. The optimal policy is one that yields the maximum possible expected return from any state. The optimal value functions are denoted as $V^*(s)$ and $Q^*(s, a)$.\n\n#### Optimal State-Value Function ($V^*(s)$)\n\nThe optimal state-value function $V^*(s)$ is the maximum value function over all policies:\n\n$$V^*(s) = \\max_{\\pi} V_{\\pi}(s)$$\n\nThe Bellman optimality equation for $V^*(s)$ states that the value of an optimal state is the maximum over all actions of the expected return from taking that action:\n\n$$V^*(s) = \\max_{a \\in \\mathcal{A}} \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma V^*(s')]$$\n\n**Explanation:**\n*   Unlike the expectation equation, which averages over actions according to a policy $\\pi(a|s)$, the optimality equation uses a $\\max$ operator. This signifies that an optimal agent will always choose the action $a$ that leads to the highest possible expected future return.\n*   The term inside the $\\max$ is the expected return if action $a$ is chosen, which includes the immediate reward $r$ and the discounted optimal value of the next state $V^*(s')$.\n\n#### Optimal Action-Value Function ($Q^*(s, a)$)\n\nThe optimal action-value function $Q^*(s, a)$ is the maximum action-value function over all policies:\n\n$$Q^*(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)$$\n\nThe Bellman optimality equation for $Q^*(s, a)$ states that the value of an optimal state-action pair is the expected return from taking that action, followed by acting optimally thereafter:\n\n$$Q^*(s, a) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma \\max_{a' \\in \\mathcal{A}} Q^*(s', a')]$$\n\n**Explanation:**\n*   After taking action $a$ from state $s$, the environment transitions to $s'$ with reward $r$. From this new state $s'$, the agent then chooses the *optimal* action $a'$ (i.e., the one that maximizes the $Q$-value from $s'$), which is represented by $\\max_{a' \\in \\mathcal{A}} Q^*(s', a')$.\n*   This equation essentially says that the optimal value of taking action $a$ in state $s$ is the immediate reward plus the discounted optimal value of the best possible next action in the subsequent state.\n\nAlternatively, we can express the relationship between $V^*(s)$ and $Q^*(s, a)$ for optimal functions:\n\n$$V^*(s) = \\max_{a \\in \\mathcal{A}} Q^*(s, a)$$\n\n$$Q^*(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$$\n\n### Summary of Differences\n\n| Feature             | Bellman Expectation Equations                                 | Bellman Optimality Equations                                      |\n| :------------------ | :------------------------------------------------------------ | :---------------------------------------------------------------- |\n| **Purpose**         | Evaluate a **given policy** $\\pi$ (predicting returns if we follow $\\pi$). | Find the **optimal policy** $\\pi^*$ (yielding maximal returns).   |\n| **Action Selection**| Averages over actions weighted by the given policy $\\pi(a|s)$. | Uses a $\\max$ operator to select the best action.                  |\n| **Role of Policy**  | Policy $\\pi$ is explicitly part of the equation and determines action probabilities. | The optimal policy $\\pi^*$ is implicit; it's the policy that chooses the actions leading to the maximum value. |\n\nIn essence, the expectation equations help us understand how good a particular way of acting is, while the optimality equations tell us what the best possible way of acting is.\n```","metadata":{"id":"372804e8"}},{"cell_type":"markdown","source":"## Value Functions and Bellman Equations\n\nReinforcement Learning (RL) agents learn to make decisions by interacting with an environment. To guide these decisions, agents use **value functions** to estimate the 'goodness' of being in a particular state or taking a particular action in a state. These functions are crucial for planning and learning in sequential decision-making problems.\n\n### State-Value Function ($V_{\\pi}(s)$)\n\nThe **state-value function**, denoted as $V_{\\pi}(s)$, represents the expected cumulative discounted reward an agent can expect to receive starting from state $s$ and following a specific policy $\\pi$ thereafter. A policy $\\pi$ is a mapping from states to probabilities of selecting each possible action. Essentially, $V_{\\pi}(s)$ tells us \"how good\" it is to be in state $s$ when following policy $\\pi$.\n\nMathematically, the state-value function for a policy $\\pi$ is defined as:\n\n$$V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$$\n\nwhere:\n*   $G_t$ is the return (total discounted reward) from time $t$.\n*   $S_t = s$ means that the agent is in state $s$ at time $t$.\n*   $\\mathbb{E}_{\\pi}[.]$ denotes the expected value given that the agent follows policy $\\pi$.\n*   The return $G_t$ is the sum of discounted future rewards: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n*   $\\gamma \\in [0, 1]$ is the discount factor, which determines the present value of future rewards. A $\\gamma$ close to 0 makes the agent short-sighted, while a $\\gamma$ close to 1 makes it long-sighted.\n\n### Action-Value Function ($Q_{\\pi}(s, a)$)\n\nThe **action-value function**, denoted as $Q_{\\pi}(s, a)$, represents the expected cumulative discounted reward an agent can expect to receive starting from state $s$, taking action $a$, and then following policy $\\pi$ thereafter. $Q_{\\pi}(s, a)$ tells us \"how good\" it is to take action $a$ in state $s$ when following policy $\\pi$.\n\nMathematically, the action-value function for a policy $\\pi$ is defined as:\n\n$$Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t | S_t = s, A_t = a]$$\n\nwhere:\n*   $A_t = a$ means the agent takes action $a$ at time $t$.\n*   All other symbols are the same as defined for $V_{\\pi}(s)$.\n\n### Bellman Equations for Expectations\n\nThe Bellman equations express a fundamental property of value functions: they relate the value of a state (or state-action pair) to the values of successor states (or state-action pairs). These equations are recursive and form the basis for many RL algorithms.\n\n#### Bellman Expectation Equation for $V_{\\pi}(s)$\n\nThe state-value function $V_{\\pi}(s)$ can be decomposed into the immediate reward plus the discounted expected value of the next state:\n\n$$V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma V_{\\pi}(s')]$$\n\n**Explanation:**\n*   The outer sum iterates over all possible actions $a$ that can be taken from state $s$, weighted by their probabilities under policy $\\pi$ (i.e., $\\pi(a|s)$). This is because the agent follows policy $\\pi$ to choose an action.\n*   The inner sum iterates over all possible next states $s'$ and rewards $r$ that can result from taking action $a$ in state $s$, weighted by the transition probability $p(s', r | s, a)$.\n*   The term $[r + \\gamma V_{\\pi}(s')]$ represents the immediate reward $r$ obtained from the transition plus the discounted value of the next state $s'$, assuming we continue to follow policy $\\pi$.\n\nThis equation means that the value of a state $s$ is the expected sum of the immediate reward and the discounted value of the next state, averaged over all possible actions chosen by policy $\\pi$ and all possible next states and rewards generated by the environment.\n\n#### Bellman Expectation Equation for $Q_{\\pi}(s, a)$\n\nSimilarly, the action-value function $Q_{\\pi}(s, a)$ can be expressed recursively:\n\n$$Q_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma \\sum_{a' \\in \\mathcal{A}} \\pi(a'|s') Q_{\\pi}(s', a')]$$\n\n**Explanation:**\n*   Starting in state $s$ and taking action $a$, the agent transitions to a next state $s'$ and receives reward $r$ with probability $p(s', r | s, a)$.\n*   From the next state $s'$, the agent then chooses an action $a'$ according to policy $\\pi$ (i.e., $\\pi(a'|s')$), leading to the action-value $Q_{\\pi}(s', a')$.\n*   The equation states that $Q_{\\pi}(s, a)$ is the expected immediate reward $r$ plus the discounted expected action-value of the next state-action pair $(s', a')$, averaged over all possible next states $s'$ and rewards $r$ given the current action $a$, and then over all possible next actions $a'$ chosen by policy $\\pi$.\n\nAlternatively, we can express $Q_{\\pi}(s,a)$ in terms of $V_{\\pi}(s')$:\n\n$$Q_{\\pi}(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_{\\pi}(s')]$$\n\nAnd $V_{\\pi}(s)$ in terms of $Q_{\\pi}(s,a)$:\n\n$$V_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) Q_{\\pi}(s, a)$$\n\nThese two relationships allow us to move between state-value and action-value functions under a given policy.\n\n### Bellman Equations for Optimality\n\nWhile the expectation equations describe the value functions for a *given* policy $\\pi$, the **Bellman optimality equations** describe the value functions for the *optimal* policy, denoted as $\\pi^*$. The optimal policy is one that yields the maximum possible expected return from any state. The optimal value functions are denoted as $V^*(s)$ and $Q^*(s, a)$.\n\n#### Optimal State-Value Function ($V^*(s)$)\n\nThe optimal state-value function $V^*(s)$ is the maximum value function over all policies:\n\n$$V^*(s) = \\max_{\\pi} V_{\\pi}(s)$$\n\nThe Bellman optimality equation for $V^*(s)$ states that the value of an optimal state is the maximum over all actions of the expected return from taking that action:\n\n$$V^*(s) = \\max_{a \\in \\mathcal{A}} \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma V^*(s')]$$\n\n**Explanation:**\n*   Unlike the expectation equation, which averages over actions according to a policy $\\pi(a|s)$, the optimality equation uses a $\\max$ operator. This signifies that an optimal agent will always choose the action $a$ that leads to the highest possible expected future return.\n*   The term inside the $\\max$ is the expected return if action $a$ is chosen, which includes the immediate reward $r$ and the discounted optimal value of the next state $V^*(s')$.\n\n#### Optimal Action-Value Function ($Q^*(s, a)$)\n\nThe optimal action-value function $Q^*(s, a)$ is the maximum action-value function over all policies:\n\n$$Q^*(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)$$\n\nThe Bellman optimality equation for $Q^*(s, a)$ states that the value of an optimal state-action pair is the expected return from taking that action, followed by acting optimally thereafter:\n\n$$Q^*(s, a) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) [r + \\gamma \\max_{a' \\in \\mathcal{A}} Q^*(s', a')]$$\n\n**Explanation:**\n*   After taking action $a$ from state $s$, the environment transitions to $s'$ with reward $r$. From this new state $s'$, the agent then chooses the *optimal* action $a'$ (i.e., the one that maximizes the $Q$-value from $s'$), which is represented by $\\max_{a' \\in \\mathcal{A}} Q^*(s', a')$.\n*   This equation essentially says that the optimal value of taking action $a$ in state $s$ is the immediate reward plus the discounted optimal value of the best possible next action in the subsequent state.\n\nAlternatively, we can express the relationship between $V^*(s)$ and $Q^*(s, a)$ for optimal functions:\n\n$$V^*(s) = \\max_{a \\in \\mathcal{A}} Q^*(s, a)$$\n\n$$Q^*(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$$\n\n### Summary of Differences\n\n| Feature             | Bellman Expectation Equations                                 | Bellman Optimality Equations                                      |\n| :------------------ | :------------------------------------------------------------ | :---------------------------------------------------------------- |\n| **Purpose**         | Evaluate a **given policy** $\\pi$ (predicting returns if we follow $\\pi$). | Find the **optimal policy** $\\pi^*$ (yielding maximal returns).   |\n| **Action Selection**| Averages over actions weighted by the given policy $\\pi(a|s)$. | Uses a $\\max$ operator to select the best action.                  |\n| **Role of Policy**  | Policy $\\pi$ is explicitly part of the equation and determines action probabilities. | The optimal policy $\\pi^*$ is implicit; it's the policy that chooses the actions leading to the maximum value. |\n\nIn essence, the expectation equations help us understand how good a particular way of acting is, while the optimality equations tell us what the best possible way of acting is.","metadata":{"id":"76bc8de4"}},{"cell_type":"markdown","source":"## Dynamic Programming: Value and Policy Iteration\n\n### Subtask:\nImplement and demonstrate classic dynamic programming algorithms: Value Iteration and Policy Iteration, for solving small, known MDPs. Discuss their strengths and limitations.\n","metadata":{"id":"b85d86d5"}},{"cell_type":"markdown","source":"```markdown\n### Dynamic Programming in Reinforcement Learning\n\nDynamic Programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov Decision Process (MDP). DP methods are fundamental to Reinforcement Learning (RL) because they provide a powerful framework for planning. Unlike Monte Carlo methods or Temporal Difference learning, DP assumes full knowledge of the MDP's dynamics, i.e., the transition probabilities and reward function for all states and actions. This makes DP suitable for problems where the environment model is known.\n\nIn the context of RL, DP algorithms aim to find an optimal policy that maximizes the expected cumulative reward. The core idea behind DP is to break down the problem into smaller, overlapping subproblems and solve each subproblem only once, storing the results to avoid redundant computations. This is achieved by using Bellman equations, which express the value of a state or state-action pair in terms of the values of successor states.\n\nThe two primary DP algorithms for solving MDPs are **Value Iteration** and **Policy Iteration**. Both aim to find an optimal policy, but they approach the problem in slightly different ways.\n```","metadata":{"id":"907cc622"}},{"cell_type":"markdown","source":"```markdown\n### Policy Iteration\n\nPolicy Iteration is an iterative algorithm that finds the optimal policy for a given MDP by alternating between two main steps: **Policy Evaluation** and **Policy Improvement**. It guarantees to converge to an optimal policy in a finite number of steps for finite MDPs.\n\n#### 1. Policy Evaluation\n\nIn this phase, for a given policy $\\pi$, we compute the state-value function $V_\\pi(s)$, which represents the expected return when starting in state $s$ and following policy $\\pi$ thereafter. This is done by solving the Bellman expectation equation for $V_\\pi(s)$:\n\n$$V_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s, a) [r + \\gamma V_\\pi(s')]$$\n\nSince this is a system of linear equations, it can be solved directly for small MDPs. However, for larger MDPs, an iterative approach is typically used. This involves repeatedly updating the value function until it converges:\n\n$$V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s, a) [r + \\gamma V_k(s')]$$\n\nThe iteration stops when the change in $V_\\pi(s)$ between successive iterations is smaller than a predefined threshold $\\theta$, indicating convergence.\n\n#### 2. Policy Improvement\n\nOnce the policy evaluation step has converged and we have an accurate $V_\\pi(s)$ for the current policy $\\pi$, we then improve the policy. This involves selecting actions that maximize the expected future reward from each state, assuming we follow the current value function $V_\\pi(s)$. The new policy $\\pi'$ is greedy with respect to $V_\\pi$:\n\n$$\\pi'(s) = \\arg\\max_a \\sum_{s', r} p(s', r|s, a) [r + \\gamma V_\\pi(s')]$$\n\nIf the new policy $\\pi'$ is better than or equal to the previous policy $\\pi$ for all states, then the algorithm has converged to an optimal policy. If not, the new policy $\\pi'$ becomes the current policy, and the process reiterates with a new policy evaluation phase.\n\n**Convergence:** Policy Iteration is guaranteed to converge to an optimal policy and optimal value function in a finite number of iterations for finite MDPs. This is because there are a finite number of policies, and policy improvement guarantees that each new policy is either strictly better than or equal to the previous one. If it's equal, then an optimal policy has been found.\n```","metadata":{"id":"57a0c19e"}},{"cell_type":"markdown","source":"```markdown\n### Value Iteration\n\nValue Iteration is another iterative algorithm for finding an optimal policy. Unlike Policy Iteration, which alternates between evaluating and improving a policy, Value Iteration directly computes the optimal state-value function $V^*(s)$ by iteratively applying the Bellman Optimality Equation. Once $V^*(s)$ is found, the optimal policy $\\pi^*(s)$ can be easily derived.\n\n#### 1. Iterative Update of Value Function\n\nThe core of Value Iteration is the iterative application of the Bellman Optimality Equation for the state-value function:\n\n$$V_{k+1}(s) = \\max_a \\sum_{s', r} p(s', r|s, a) [r + \\gamma V_k(s')]$$\n\nThis equation states that the value of a state $s$ at iteration $k+1$ is the maximum expected reward achievable by taking any action $a$ from state $s$, considering the immediate reward $r$ and the discounted value of the next state $s'$ according to the value function from iteration $k$. The iteration starts with an arbitrary value function (e.g., all zeros) and continues until the value function converges.\n\nConvergence is typically determined by checking if the maximum change in any state's value between successive iterations is smaller than a small threshold $\\theta$ (e.g., $||V_{k+1} - V_k||_\\infty < \\theta$). This indicates that the value function has stabilized and is close to the optimal value function $V^*(s)$.\n\n#### 2. Extracting the Optimal Policy\n\nOnce the optimal value function $V^*(s)$ has been found, the optimal policy $\\pi^*(s)$ can be directly extracted. For each state $s$, the optimal policy chooses the action $a$ that maximizes the expected return, using the converged optimal value function $V^*(s')$ for the successor states:\n\n$$\\pi^*(s) = \\arg\\max_a \\sum_{s', r} p(s', r|s, a) [r + \\gamma V^*(s')]$$\n\nThis means that once we know the optimal values of all states, we can simply perform a one-step lookahead from each state to find the best action.\n\n**Convergence:** Value Iteration is guaranteed to converge to the optimal value function $V^*(s)$ and an optimal policy for finite MDPs. This is because the Bellman Optimality operator is a contraction mapping, ensuring that repeated application will eventually converge to a unique fixed point, which is $V^*(s)$.\n```","metadata":{"id":"cfdb51af"}},{"cell_type":"markdown","source":"**Reasoning**:\nThe next step is to set up a small, discrete MDP environment in code, as per the instructions. This will involve defining the states, actions, transition probabilities, and reward functions. I will use a code block for this.\n\n","metadata":{"id":"b88ee5e9"}},{"cell_type":"code","source":"import numpy as np\n\n# Define the MDP components\n\n# States: 0, 1, 2, 3 (e.g., a 2x2 grid world)\n# Actions: 0 (Up), 1 (Down), 2 (Left), 3 (Right)\n\nnum_states = 4\nnum_actions = 4\n\n# Transition probabilities P[s, a, s']\n# P is a 3D array where P[s, a, s_prime] is the probability of transitioning from state s to state s_prime\n# when taking action a.\n# We'll simplify this for a deterministic environment for now, so probabilities are either 0 or 1.\n# For a more complex environment, these could be floats.\n\n# Let's imagine a simple 2x2 grid world:\n# 0 1\n# 2 3 (Goal)\n\n# Action effects:\n# Up (0): s -> s-2 (if possible, else stay)\n# Down (1): s -> s+2 (if possible, else stay)\n# Left (2): s -> s-1 (if possible, else stay)\n# Right (3): s -> s+1 (if possible, else stay)\n\n# Boundary conditions: if an action would take the agent off the grid, it stays in the current state.\n\n# Rewards: R[s, a, s']\n# For simplicity, -1 for each step, 0 for reaching the goal state 3.\n\n# Initialize P and R with zeros\nP = np.zeros((num_states, num_actions, num_states))\nR = np.zeros((num_states, num_actions, num_states))\n\n# Define the transitions and rewards\n# State 3 is the goal state. Once in state 3, any action keeps it in state 3 with reward 0.\nfor s in range(num_states):\n    for a in range(num_actions):\n        if s == 3: # Goal state\n            P[s, a, s] = 1.0\n            R[s, a, s] = 0.0\n            continue\n\n        s_prime = s # Default to staying in the same state if action is invalid\n        reward = -1.0 # Default step reward\n\n        if a == 0: # Up\n            if s == 2: s_prime = 0\n            elif s == 3: s_prime = 1\n            else: s_prime = s # Cannot go up from 0 or 1, stay\n        elif a == 1: # Down\n            if s == 0: s_prime = 2\n            elif s == 1: s_prime = 3\n            else: s_prime = s # Cannot go down from 2 or 3, stay\n        elif a == 2: # Left\n            if s == 1: s_prime = 0\n            elif s == 3: s_prime = 2\n            else: s_prime = s # Cannot go left from 0 or 2, stay\n        elif a == 3: # Right\n            if s == 0: s_prime = 1\n            elif s == 2: s_prime = 3\n            else: s_prime = s # Cannot go right from 1 or 3, stay\n\n        # Set transition probability to 1 for the determined next state\n        P[s, a, s_prime] = 1.0\n        R[s, a, s_prime] = reward\n\n# Discount factor (gamma)\ngamma = 0.9\n\nprint(\"MDP Environment Setup Complete:\")\nprint(f\"Number of states: {num_states}\")\nprint(f\"Number of actions: {num_actions}\")\nprint(f\"Discount factor (gamma): {gamma}\")\nprint(\"Transition Probabilities (P[s, a, s']):\")\nprint(P)\nprint(\"Rewards (R[s, a, s']):\")\nprint(R)","metadata":{"id":"4c754ff1","executionInfo":{"status":"ok","timestamp":1765175292730,"user_tz":-330,"elapsed":26,"user":{"displayName":"Debanik Debnath","userId":"16501660051911573481"}},"outputId":"ac6872c8-4785-4465-8b78-52af4d4ea00e","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:15:28.319295Z","iopub.execute_input":"2025-12-08T07:15:28.319659Z","iopub.status.idle":"2025-12-08T07:15:28.332895Z","shell.execute_reply.started":"2025-12-08T07:15:28.319634Z","shell.execute_reply":"2025-12-08T07:15:28.331740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Reasoning**:\nThe next step is to implement the Policy Iteration algorithm in Python, which includes an outer loop for policy iteration and an inner loop for policy evaluation, followed by policy improvement. I will use a code block for this implementation.\n\n","metadata":{"id":"259c65b5"}},{"cell_type":"code","source":"import numpy as np\n\n# --- Policy Iteration Implementation ---\n\ndef policy_evaluation(policy, P, R, gamma, num_states, theta=1e-6):\n    \"\"\"\n    Evaluates the value function for a given policy.\n\n    Args:\n        policy (np.array): A 2D array representing the policy (num_states, num_actions).\n                           policy[s, a] is the probability of taking action a in state s.\n        P (np.array): Transition probability tensor (num_states, num_actions, num_states).\n                      P[s, a, s_prime] is the probability of transitioning from s to s_prime\n                      after taking action a.\n        R (np.array): Reward tensor (num_states, num_actions, num_states).\n                      R[s, a, s_prime] is the reward for transitioning from s to s_prime\n                      after taking action a.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        theta (float): A small threshold for checking convergence.\n\n    Returns:\n        np.array: The state-value function V_pi for the given policy.\n    \"\"\"\n    V = np.zeros(num_states)\n    while True:\n        delta = 0\n        for s in range(num_states):\n            if s == 3: # Goal state, value is 0\n                V_s_new = 0.0\n            else:\n                V_s_new = 0\n                for a in range(num_actions):\n                    # Sum over s', r for p(s', r | s, a) * [r + gamma * V(s')]\n                    q_sa = np.sum(P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n                                  for s_prime in range(num_states))\n                    V_s_new += policy[s, a] * q_sa\n            delta = max(delta, np.abs(V_s_new - V[s]))\n            V[s] = V_s_new\n        if delta < theta:\n            break\n    return V\n\ndef policy_improvement(V, P, R, gamma, num_states, num_actions):\n    \"\"\"\n    Improves the policy based on the given value function.\n\n    Args:\n        V (np.array): The state-value function.\n        P (np.array): Transition probability tensor.\n        R (np.array): Reward tensor.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n\n    Returns:\n        np.array: The improved policy (a 2D array).\n        bool: True if the policy was stable (no improvement needed), False otherwise.\n    \"\"\"\n    policy = np.zeros((num_states, num_actions)) # Store action probabilities\n    policy_stable = True\n\n    for s in range(num_states):\n        old_best_action = np.argmax(policy[s]) if np.sum(policy[s]) > 0 else -1\n\n        # Calculate Q-values for all actions in state s\n        q_values = np.zeros(num_actions)\n        for a in range(num_actions):\n            q_values[a] = np.sum(P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n                                 for s_prime in range(num_states))\n\n        best_action = np.argmax(q_values)\n\n        # If the best action has changed, the policy is not stable\n        if old_best_action != best_action:\n            policy_stable = False\n\n        # Create a new policy that is greedy with respect to the Q-values\n        new_policy_row = np.zeros(num_actions)\n        new_policy_row[best_action] = 1.0 # Deterministic policy\n        policy[s] = new_policy_row\n\n    return policy, policy_stable\n\ndef policy_iteration(P, R, gamma, num_states, num_actions):\n    \"\"\"\n    Finds the optimal policy and value function using Policy Iteration.\n\n    Args:\n        P (np.array): Transition probability tensor.\n        R (np.array): Reward tensor.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n\n    Returns:\n        np.array: The optimal policy.\n        np.array: The optimal state-value function.\n    \"\"\"\n    # Start with an arbitrary policy (e.g., uniformly random policy)\n    policy = np.ones((num_states, num_actions)) / num_actions\n\n    V = np.zeros(num_states) # Initialize value function\n\n    while True:\n        V = policy_evaluation(policy, P, R, gamma, num_states)\n        new_policy, policy_stable = policy_improvement(V, P, R, gamma, num_states, num_actions)\n\n        if policy_stable:\n            break\n        policy = new_policy\n\n    # Convert policy probabilities to actual actions for readability\n    optimal_policy_actions = np.argmax(policy, axis=1)\n\n    return optimal_policy_actions, V\n\nprint(\"Policy Iteration algorithm functions defined.\")\n","metadata":{"id":"9602e287","executionInfo":{"status":"ok","timestamp":1765175306849,"user_tz":-330,"elapsed":35,"user":{"displayName":"Debanik Debnath","userId":"16501660051911573481"}},"outputId":"e5f5de38-e22f-438f-a9a1-1831bac1022b","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:15:28.584118Z","iopub.execute_input":"2025-12-08T07:15:28.584458Z","iopub.status.idle":"2025-12-08T07:15:28.599416Z","shell.execute_reply.started":"2025-12-08T07:15:28.584435Z","shell.execute_reply":"2025-12-08T07:15:28.598440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Reasoning**:\nNow that the Policy Iteration functions are defined, I need to execute the `policy_iteration` algorithm using the previously defined MDP parameters and then print the resulting optimal policy and value function.\n\n","metadata":{"id":"097e6040"}},{"cell_type":"markdown","source":"# Task\n```python\nfor s, value in enumerate(optimal_V_pi):\n    print(f\"State {s}: {value:.4f}\")\n```\n\n**Reasoning**:\nThe previous execution was interrupted while trying to print the optimal value function `optimal_V_pi`. The code above completes this task by iterating through the `optimal_V_pi` array and printing each state's value, formatted to four decimal places for readability. This provides the full output for the Policy Iteration step.\n\nThe next step in the plan is to implement the Value Iteration algorithm, as indicated by the structure of the notebook under \"Dynamic Programming: Value and Policy Iteration\".","metadata":{"id":"a2666150"}},{"cell_type":"markdown","source":"## Dynamic Programming: Value and Policy Iteration\n\n### Subtask:\nComplete the output for Policy Iteration and then implement the Value Iteration algorithm.\n","metadata":{"id":"2a4ce2f9"}},{"cell_type":"markdown","source":"**Reasoning**:\nThe previous code block was interrupted before printing the optimal value function for Policy Iteration. I need to complete this step by printing `optimal_V_pi` as requested and then proceed with Value Iteration.\n\n","metadata":{"id":"0be37e06"}},{"cell_type":"markdown","source":"**Reasoning**:\nThe previous `policy_iteration` call was interrupted and did not assign `optimal_V_pi`, causing a `NameError`. I need to re-run the `policy_iteration` function and ensure all its output is printed in a single code block to correctly define and display the optimal policy and value function.\n\n","metadata":{"id":"7637550f"}},{"cell_type":"markdown","source":"# Task\n```python\nprint(\"\\n--- Running Policy Iteration ---\")\noptimal_policy_pi, optimal_V_pi = policy_iteration(P, R, gamma, num_states, num_actions)\n\nprint(\"Converged Optimal Policy (Policy Iteration):\")\n# Map action indices to meaningful directions for better readability\naction_map = {0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}\nfor s, action_idx in enumerate(optimal_policy_pi):\n    print(f\"State {s}: {action_map[action_idx]}\")\n\nprint(\"\\nConverged Optimal Value Function (Policy Iteration):\")\nfor s, value in enumerate(optimal_V_pi):\n    print(f\"State {s}: {value:.4f}\")\n```\n\n**Reasoning**:\nThe previous execution of the Policy Iteration algorithm was interrupted. This re-executes the `policy_iteration` function to ensure it runs to completion and successfully calculates the `optimal_policy_pi` and `optimal_V_pi`. It then prints both the converged optimal policy (mapping action indices to directions for clarity) and the converged optimal value function, formatted to four decimal places, as intended.\n\nWith the Policy Iteration successfully completed and its results displayed, the next logical step according to the plan is to implement the Value Iteration algorithm. This will involve defining a new function for Value Iteration and then executing it.\n\n**Next subtask:** Implement and execute the Value Iteration algorithm for the same MDP.","metadata":{"id":"572ec535"}},{"cell_type":"markdown","source":"## Implement and execute Value Iteration\n\n### Subtask:\nImplement the Value Iteration algorithm and apply it to the defined MDP to find the optimal policy and value function.\n","metadata":{"id":"40d3c1c9"}},{"cell_type":"markdown","source":"**Reasoning**:\nThe previous execution of Policy Iteration was interrupted. Before proceeding with Value Iteration, I need to successfully run Policy Iteration and display its results to fulfill the previous subtask's requirements and ensure continuity. I will include the printing of the optimal policy and value function in the same code block to avoid further interruptions.\n\n","metadata":{"id":"1c101460"}},{"cell_type":"markdown","source":"# Task\nThe previous execution of the Policy Iteration algorithm was interrupted repeatedly due to an inefficient use of `np.sum` with a generator within the `policy_evaluation` and `policy_improvement` functions. This can lead to performance issues and potentially long execution times for convergence.\n\nTo address this, I will update the `policy_evaluation` and `policy_improvement` functions to use more efficient NumPy array operations for calculating expected values. This will resolve the `DeprecationWarning` and improve the algorithm's performance, allowing it to complete successfully.\n\nAfter redefining the functions, I will re-run the Policy Iteration algorithm to obtain the optimal policy and value function for the defined MDP, and then display the results as intended. This will fulfill the requirements for the Policy Iteration subtask.\n\nFinally, I will proceed to implement and execute the Value Iteration algorithm as the next step in the plan.\n\n```python\nimport numpy as np\n\n# --- Policy Iteration Implementation (Corrected) ---\n\ndef policy_evaluation(policy, P, R, gamma, num_states, theta=1e-6):\n    \"\"\"\n    Evaluates the value function for a given policy.\n\n    Args:\n        policy (np.array): A 2D array representing the policy (num_states, num_actions).\n                           policy[s, a] is the probability of taking action a in state s.\n        P (np.array): Transition probability tensor (num_states, num_actions, num_states).\n                      P[s, a, s_prime] is the probability of transitioning from s to s_prime\n                      after taking action a.\n        R (np.array): Reward tensor (num_states, num_actions, num_states).\n                      R[s, a, s_prime] is the reward for transitioning from s to s_prime\n                      after taking action a.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        theta (float): A small threshold for checking convergence.\n\n    Returns:\n        np.array: The state-value function V_pi for the given policy.\n    \"\"\"\n    V = np.zeros(num_states)\n    while True:\n        delta = 0\n        for s in range(num_states):\n            if s == 3: # Goal state, its value should be 0 and not change\n                if V[s] != 0:\n                    delta = max(delta, np.abs(0 - V[s]))\n                    V[s] = 0.0\n                continue\n\n            V_s_new = 0\n            for a in range(num_actions):\n                # Efficient calculation of expected next value using NumPy array operations\n                # (P[s, a, :] * (R[s, a, :] + gamma * V)) computes for all s_prime\n                # then np.sum sums these up.\n                q_sa = np.sum(P[s, a, :] * (R[s, a, :] + gamma * V))\n                V_s_new += policy[s, a] * q_sa\n            delta = max(delta, np.abs(V_s_new - V[s]))\n            V[s] = V_s_new\n        if delta < theta:\n            break\n    return V\n\ndef policy_improvement(V, P, R, gamma, num_states, num_actions):\n    \"\"\"\n    Improves the policy based on the given value function.\n\n    Args:\n        V (np.array): The state-value function.\n        P (np.array): Transition probability tensor.\n        R (np.array): Reward tensor.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n\n    Returns:\n        np.array: The improved policy (a 2D array).\n        bool: True if the policy was stable (no improvement needed), False otherwise.\n    \"\"\"\n    policy = np.zeros((num_states, num_actions)) # This will be the new policy\n    policy_stable = True\n\n    for s in range(num_states):\n        if s == 3: # Goal state, actions don't matter, it stays there. We can set policy to anything or handle separately.\n            # For simplicity, let's say it just takes action 0 and stays.\n            policy[s, 0] = 1.0\n            continue\n\n        current_policy_action = np.argmax(policy[s]) # Get current best action before recalculation\n\n        # Calculate Q-values for all actions in state s\n        q_values = np.zeros(num_actions)\n        for a in range(num_actions):\n            # Efficient calculation of Q-value using NumPy array operations\n            q_values[a] = np.sum(P[s, a, :] * (R[s, a, :] + gamma * V))\n\n        best_action = np.argmax(q_values)\n\n        # Check if the best action has changed\n        # We need to compare with the previous policy's best action.\n        # Initialize an arbitrary policy at the start of policy_iteration\n        # so current_policy_action can be properly compared.\n        # If the policy was random initially (e.g., 1/num_actions for all), then argmax might be 0 for all.\n        # A more robust check might be to compare the new policy array with the old one directly.\n        # For a deterministic policy, we can check if the best action for state s has changed.\n        # Since 'policy' here is built for the new policy, we need a way to access the *old* policy.\n        # However, the policy_improvement function receives the V, and constructs a new greedy policy from scratch.\n        # The policy_stable check should compare the *newly derived greedy action* with the *action from the previous policy*\n        # passed into the policy_iteration's outer loop.\n        # For simplicity, if np.argmax(q_values) gives a different action than what was previously chosen, policy_stable is False.\n        # The initial policy is uniformly random, so for the first iteration, 'current_policy_action' from a zeroed 'policy' might be 0.\n        # A better way is to pass the *old_policy* into policy_improvement for comparison.\n        # Let's assume the 'policy' passed to policy_improvement is the *current* policy being improved upon,\n        # and we build 'new_policy' and then compare.\n\n        # To fix policy_improvement's policy_stable logic:\n        # We need to compute the best action from q_values and then compare it to the current policy for state s.\n        # The 'policy' argument to policy_evaluation and policy_improvement in policy_iteration's loop is the 'current' policy.\n        # The `policy` variable initialized in `policy_improvement` is for the *new* policy being constructed.\n        # We need to compare `best_action` with `np.argmax(current_policy[s])`.\n\n        # Let's adjust policy_iteration and policy_improvement slightly to make this comparison clear.\n        # For now, let's assume `policy_improvement` implicitly gets the previous best action from the `policy` array.\n        # This will be correct if `policy` passed to `policy_improvement` contains the previous policy.\n        # In our `policy_iteration` function, `new_policy` is built and then assigned to `policy`.\n        # So we need to store the old policy temporarily for comparison.\n\n        # Let's revise the `policy_improvement` function's return to include `old_policy` if needed,\n        # or simplify its comparison. Given the structure, `policy_stable` is usually determined\n        # by comparing the `new_policy` generated against the `old_policy` that produced `V`.\n\n        # Instead of `np.argmax(policy[s])`, we need to compare `best_action` against the action\n        # that was actually chosen by the *old* policy for state s.\n        # If policy is deterministic:\n        old_action_for_s = np.argmax(policy[s]) # This 'policy' is the old policy from the parent loop\n\n        # Update the new policy (greedy)\n        new_policy_row = np.zeros(num_actions)\n        new_policy_row[best_action] = 1.0\n        policy[s] = new_policy_row # This updates the 'policy' variable, which is actually the *new* policy being built.\n\n        if old_action_for_s != best_action:\n            policy_stable = False # If any state's optimal action changes, policy is not stable\n\n    return policy, policy_stable # 'policy' here is the *new* policy\n\ndef policy_iteration(P, R, gamma, num_states, num_actions):\n    \"\"\"\n    Finds the optimal policy and value function using Policy Iteration.\n\n    Args:\n        P (np.array): Transition probability tensor.\n        R (np.array): Reward tensor.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n\n    Returns:\n        np.array: The optimal policy (as actions, not probabilities).\n        np.array: The optimal state-value function.\n    \"\"\"\n    # Start with an arbitrary policy (e.g., uniformly random policy, then convert to deterministic for comparison)\n    # For a small deterministic MDP, it's easier to start with a fixed deterministic policy.\n    # Let's start with action 0 (Up) for all states for simplicity, except goal state.\n    initial_policy = np.zeros((num_states, num_actions))\n    for s in range(num_states):\n        if s != 3: # Not goal\n            initial_policy[s, 0] = 1.0 # Initial policy: always go Up\n        else:\n            initial_policy[s, 0] = 1.0 # Goal state action doesn't matter, stays there.\n\n    policy = initial_policy # Current policy\n    V = np.zeros(num_states) # Initialize value function\n\n    while True:\n        V = policy_evaluation(policy, P, R, gamma, num_states)\n        # policy_improvement will build a new policy based on V and check if it's stable compared to the 'policy' passed in.\n        # 'policy' within policy_improvement is the old policy, which is overwritten with the new greedy policy.\n        # This means the 'policy' variable in the outer loop is directly modified.\n        new_policy, policy_stable = policy_improvement(V, P, R, gamma, num_states, num_actions)\n\n        # A cleaner way:\n        # old_policy = np.copy(policy)\n        # new_policy, policy_stable = policy_improvement(V, P, R, gamma, num_states, num_actions, old_policy)\n        # If policy_improvement returns the new policy, then we can assign it:\n        # policy = new_policy\n\n        # For current implementation, policy_improvement modifies `policy` in place for the new one.\n        # Let's ensure the policy_improvement function returns a *new* policy and we compare explicitly.\n        # Redefining policy_improvement for clarity and correctness:\n        #\n        # def policy_improvement(V, P, R, gamma, num_states, num_actions, current_policy):\n        #     new_policy = np.zeros((num_states, num_actions))\n        #     policy_stable = True\n        #     for s in range(num_states):\n        #         if s == 3:\n        #             new_policy[s, 0] = 1.0 # Goal state\n        #             continue\n        #         q_values = np.array([np.sum(P[s, a, :] * (R[s, a, :] + gamma * V)) for a in range(num_actions)])\n        #         best_action = np.argmax(q_values)\n        #         new_policy[s, best_action] = 1.0\n        #         if np.argmax(current_policy[s]) != best_action:\n        #             policy_stable = False\n        #     return new_policy, policy_stable\n        #\n        # If I use this, then the outer loop would be:\n        # old_policy = policy # Store current policy for comparison\n        # policy, policy_stable = policy_improvement(V, P, R, gamma, num_states, num_actions, old_policy)\n\n        # To avoid re-defining the function, let's use the current `new_policy` returned from `policy_improvement`.\n        # And ensure the comparison logic in `policy_improvement` correctly compares the newly derived greedy policy\n        # with the policy that was *passed into* the function (which is `policy` from the outer loop).\n\n        # The current implementation of `policy_improvement` *modifies the `policy` array that was passed in*.\n        # So `policy` will already contain the new policy after the call.\n        # The `policy_stable` check must therefore be done *before* modifying `policy[s]` inside `policy_improvement`.\n\n        # Let's fix `policy_improvement` again for the `policy_stable` check.\n        # It needs the *old* action to compare against.\n\n        # Modified `policy_improvement` structure (within the loop, conceptually):\n        # old_best_action_for_s = np.argmax(policy[s]) # Get action from the policy *before* updating it for this state\n\n        if policy_stable: # If the policy improvement step did not find any better actions\n            break\n        # If not stable, continue with the new policy (which 'policy' now holds)\n\n    # Convert policy probabilities to actual actions for readability\n    optimal_policy_actions = np.argmax(policy, axis=1)\n\n    return optimal_policy_actions, V\n\n# Redefine policy_improvement to correctly compare old and new actions for policy_stable\ndef policy_improvement_corrected(V, P, R, gamma, num_states, num_actions, current_policy):\n    \"\"\"\n    Improves the policy based on the given value function.\n\n    Args:\n        V (np.array): The state-value function.\n        P (np.array): Transition probability tensor.\n        R (np.array): Reward tensor.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n        current_policy (np.array): The policy being improved (2D array of action probabilities).\n\n    Returns:\n        np.array: The new, improved policy (a 2D array).\n        bool: True if the policy was stable (no improvement needed), False otherwise.\n    \"\"\"\n    new_policy = np.zeros((num_states, num_actions))\n    policy_stable = True\n\n    for s in range(num_states):\n        if s == 3: # Goal state, actions don't matter, it stays there.\n            new_policy[s, 0] = 1.0 # Arbitrary action for goal state, stays there.\n            if np.argmax(current_policy[s]) != 0: # If the old policy didn't choose action 0 for the goal state.\n                 policy_stable = False\n            continue\n\n        q_values = np.zeros(num_actions)\n        for a in range(num_actions):\n            q_values[a] = np.sum(P[s, a, :] * (R[s, a, :] + gamma * V))\n\n        best_action = np.argmax(q_values)\n        new_policy[s, best_action] = 1.0\n\n        # Compare the best action with the action chosen by the current_policy for this state\n        if np.argmax(current_policy[s]) != best_action:\n            policy_stable = False\n\n    return new_policy, policy_stable\n\n# Redefine policy_iteration to use the corrected policy_improvement\ndef policy_iteration_corrected(P, R, gamma, num_states, num_actions):\n    # Start with an arbitrary deterministic policy (e.g., always action 0 'Up')\n    policy = np.zeros((num_states, num_actions))\n    for s in range(num_states):\n        policy[s, 0] = 1.0 # Initialize all states to try action 0\n\n    V = np.zeros(num_states)\n\n    while True:\n        V = policy_evaluation(policy, P, R, gamma, num_states)\n        new_policy, policy_stable = policy_improvement_corrected(V, P, R, gamma, num_states, num_actions, policy)\n\n        if policy_stable:\n            break\n        policy = new_policy # Update policy for the next iteration\n\n    optimal_policy_actions = np.argmax(policy, axis=1)\n    return optimal_policy_actions, V\n\nprint(\"Policy Iteration algorithm functions redefined with performance improvements and corrected stability check.\")\n\nprint(\"\\n--- Running Policy Iteration ---\")\noptimal_policy_pi, optimal_V_pi = policy_iteration_corrected(P, R, gamma, num_states, num_actions)\n\nprint(\"Converged Optimal Policy (Policy Iteration):\")\n# Map action indices to meaningful directions for better readability\naction_map = {0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}\nfor s, action_idx in enumerate(optimal_policy_pi):\n    # Skip printing for the goal state (s=3) or show its value is fixed\n    if s == 3:\n        print(f\"State {s} (Goal): {action_map[action_idx]} (stays)\")\n    else:\n        print(f\"State {s}: {action_map[action_idx]}\")\n\nprint(\"\\nConverged Optimal Value Function (Policy Iteration):\")\nfor s, value in enumerate(optimal_V_pi):\n    print(f\"State {s}: {value:.4f}\")\n\nprint(\"\\nPolicy Iteration has successfully completed.\")\n```","metadata":{"id":"88f570e9"}},{"cell_type":"markdown","source":"## Corrected Policy Iteration Execution\n\n### Subtask:\nUpdate the Policy Iteration functions for efficiency and correctness, then execute the algorithm to find and display the optimal policy and value function.\n","metadata":{"id":"e762f72c"}},{"cell_type":"markdown","source":"**Reasoning**:\nThe previous Policy Iteration execution attempts were interrupted, and the `np.sum(generator)` pattern is inefficient and deprecated. I will provide corrected versions of the `policy_evaluation`, `policy_improvement`, and `policy_iteration` functions that use more efficient NumPy operations to correctly and robustly calculate the optimal policy and value function. After defining these, I will execute the corrected policy iteration and display its results as requested by the subtask.\n\n","metadata":{"id":"65a7b696"}},{"cell_type":"code","source":"import numpy as np\n\n# --- Corrected Policy Iteration Implementation for efficiency and correctness ---\n\ndef policy_evaluation_corrected(policy, P, R, gamma, num_states, theta=1e-6):\n    \"\"\"\n    Evaluates the value function for a given policy using vectorized operations.\n\n    Args:\n        policy (np.array): A 2D array representing the policy (num_states, num_actions).\n                           policy[s, a] is the probability of taking action a in state s.\n        P (np.array): Transition probability tensor (num_states, num_actions, num_states).\n                      P[s, a, s_prime] is the probability of transitioning from s to s_prime\n                      after taking action a.\n        R (np.array): Reward tensor (num_states, num_actions, num_states).\n                      R[s, a, s_prime] is the reward for transitioning from s to s_prime\n                      after taking action a.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        theta (float): A small threshold for checking convergence.\n\n    Returns:\n        np.array: The state-value function V_pi for the given policy.\n    \"\"\"\n    V = np.zeros(num_states)\n    while True:\n        delta = 0\n        for s in range(num_states):\n            if s == 3: # Goal state, value is 0\n                V_s_new = 0.0\n            else:\n                # Calculate expected Q-values for all actions in state s\n                # This is sum_s_prime P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n                # for each action a\n                q_values_for_s = np.sum(P[s, :, :] * (R[s, :, :] + gamma * V[np.newaxis, np.newaxis, :]), axis=2)\n\n                # Sum over actions weighted by policy probabilities\n                V_s_new = np.sum(policy[s, :] * q_values_for_s)\n\n            delta = max(delta, np.abs(V_s_new - V[s]))\n            V[s] = V_s_new\n        if delta < theta:\n            break\n    return V\n\ndef policy_improvement_corrected(V, P, R, gamma, num_states, num_actions):\n    \"\"\"\n    Improves the policy based on the given value function, using vectorized operations.\n\n    Args:\n        V (np.array): The state-value function.\n        P (np.array): Transition probability tensor.\n        R (np.array): Reward tensor.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n\n    Returns:\n        np.array: The improved policy (a 2D array).\n        bool: True if the policy was stable (no improvement needed), False otherwise.\n    \"\"\"\n    new_policy = np.zeros((num_states, num_actions))\n    policy_stable = True\n\n    for s in range(num_states):\n        # Calculate Q-values for all actions in state s using vectorized operations\n        # Q(s, a) = sum_s_prime P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n        q_values = np.sum(P[s, :, :] * (R[s, :, :] + gamma * V[np.newaxis, :]), axis=1)\n\n        best_action = np.argmax(q_values)\n\n        # Check if the policy has changed\n        # If the state already had a deterministic policy, check if the best_action is the same\n        # If the state had a uniform policy, any change means it's not stable.\n        # The `old_best_action` should be derived from the existing policy (which will be `policy` from policy_iteration_corrected before this call).\n        # For simplicity of re-execution, let's assume `policy_stable` is initially True for the current loop.\n        # This check is usually done by comparing `np.argmax(old_policy[s])` with `best_action`.\n        # Since `policy_iteration_corrected` is managing the `policy` update, `policy_stable` can be set here based on a comparison with a temporary `old_policy_actions`.\n        # However, for the initial uniform policy, `np.argmax` might pick an arbitrary one. A more robust check is to compare `new_policy` with `old_policy` at the end of the loop.\n\n        # For this example, let's compare the new best action with the current one from the input policy.\n        # A simpler check: if the new best action doesn't match the current '1' in the policy (if any).\n        # This is implicitly handled by `policy_iteration_corrected` comparing `old_policy` with `new_policy`.\n\n        # Create a new policy row that is greedy with respect to the Q-values\n        new_policy_row = np.zeros(num_actions)\n        new_policy_row[best_action] = 1.0 # Deterministic policy\n        new_policy[s] = new_policy_row\n\n    return new_policy, policy_stable # policy_stable will be updated in the main iteration loop\n\ndef policy_iteration_corrected(P, R, gamma, num_states, num_actions):\n    \"\"\"\n    Finds the optimal policy and value function using Policy Iteration.\n\n    Args:\n        P (np.array): Transition probability tensor.\n        R (np.array): Reward tensor.\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n\n    Returns:\n        np.array: The optimal policy (action indices).\n        np.array: The optimal state-value function.\n    \"\"\"\n    # Start with an arbitrary policy (e.g., uniformly random policy)\n    policy = np.ones((num_states, num_actions)) / num_actions\n\n    V = np.zeros(num_states) # Initialize value function\n\n    while True:\n        V = policy_evaluation_corrected(policy, P, R, gamma, num_states)\n        new_policy, _ = policy_improvement_corrected(V, P, R, gamma, num_states, num_actions)\n\n        # Check for policy stability by comparing the new policy with the old one\n        if np.array_equal(new_policy, policy):\n            break\n        policy = new_policy\n\n    # Convert policy probabilities to actual actions for readability\n    optimal_policy_actions = np.argmax(policy, axis=1)\n\n    return optimal_policy_actions, V\n\nprint(\"Corrected Policy Iteration algorithm functions defined.\")\n\nprint(\"\\n--- Running Corrected Policy Iteration ---\")\noptimal_policy_pi_corrected, optimal_V_pi_corrected = policy_iteration_corrected(P, R, gamma, num_states, num_actions)\n\nprint(\"Converged Optimal Policy (Corrected Policy Iteration):\")\naction_map = {0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}\nfor s, action_idx in enumerate(optimal_policy_pi_corrected):\n    print(f\"State {s}: {action_map[action_idx]}\")\n\nprint(\"\\nConverged Optimal Value Function (Corrected Policy Iteration):\")\nfor s, value in enumerate(optimal_V_pi_corrected):\n    print(f\"State {s}: {value:.4f}\")","metadata":{"id":"194f0c9b","executionInfo":{"status":"ok","timestamp":1765176169653,"user_tz":-330,"elapsed":86,"user":{"displayName":"Debanik Debnath","userId":"16501660051911573481"}},"outputId":"e84ce2e2-0b9f-41c1-9785-32fe130f87dc","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:15:28.879226Z","iopub.execute_input":"2025-12-08T07:15:28.879562Z","iopub.status.idle":"2025-12-08T07:15:28.900295Z","shell.execute_reply.started":"2025-12-08T07:15:28.879537Z","shell.execute_reply":"2025-12-08T07:15:28.899269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implement and execute Value Iteration\n\n### Subtask:\nImplement the Value Iteration algorithm and apply it to the defined MDP to find the optimal policy and value function.","metadata":{"id":"f907b7b4"}},{"cell_type":"markdown","source":"```markdown\n## Implement and execute Value Iteration\n\n### Subtask:\nImplement the Value Iteration algorithm and apply it to the defined MDP to find the optimal policy and value function.\n```","metadata":{"id":"0969e1c3"}},{"cell_type":"markdown","source":"## Implement and execute Value Iteration\n\n### Subtask:\nImplement the Value Iteration algorithm and apply it to the defined MDP to find the optimal policy and value function.","metadata":{"id":"b85339c9"}},{"cell_type":"markdown","source":"**Reasoning**:\nNow that the corrected Policy Iteration is complete, the next step is to implement the Value Iteration algorithm as defined in the subtask.\n\n","metadata":{"id":"c6576460"}},{"cell_type":"code","source":"import numpy as np\n\ndef value_iteration(P, R, gamma, num_states, num_actions, theta=1e-6):\n    \"\"\"\n    Finds the optimal value function and policy using Value Iteration.\n\n    Args:\n        P (np.array): Transition probability tensor (num_states, num_actions, num_states).\n        R (np.array): Reward tensor (num_states, num_actions, num_states).\n        gamma (float): Discount factor.\n        num_states (int): Number of states in the MDP.\n        num_actions (int): Number of actions in the MDP.\n        theta (float): A small threshold for checking convergence.\n\n    Returns:\n        np.array: The optimal policy (action indices).\n        np.array: The optimal state-value function V_star.\n    \"\"\"\n    V = np.zeros(num_states) # Initialize V_star with all zeros\n\n    while True:\n        delta = 0\n        for s in range(num_states):\n            if s == 3: # Goal state, value is 0 and does not change\n                V_s_new = 0.0\n            else:\n                # Calculate Q-values for all actions in state s\n                # Q(s, a) = sum_s_prime P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n                q_values_for_s = np.sum(P[s, :, :] * (R[s, :, :] + gamma * V[np.newaxis, :]), axis=1)\n                V_s_new = np.max(q_values_for_s) # Take the max over actions (Bellman Optimality Eq)\n\n            delta = max(delta, np.abs(V_s_new - V[s]))\n            V[s] = V_s_new\n        if delta < theta:\n            break\n\n    # Extract optimal policy from the converged optimal value function\n    optimal_policy_actions = np.zeros(num_states, dtype=int)\n    for s in range(num_states):\n        if s == 3: # Goal state\n            optimal_policy_actions[s] = 0 # Action doesn't matter, stays in goal\n            continue\n\n        q_values_for_s = np.sum(P[s, :, :] * (R[s, :, :] + gamma * V[np.newaxis, :]), axis=1)\n        optimal_policy_actions[s] = np.argmax(q_values_for_s)\n\n    return optimal_policy_actions, V\n\nprint(\"Value Iteration algorithm functions defined.\")\n\nprint(\"\\n--- Running Value Iteration ---\")\noptimal_policy_vi, optimal_V_vi = value_iteration(P, R, gamma, num_states, num_actions)\n\nprint(\"Converged Optimal Policy (Value Iteration):\")\naction_map = {0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}\nfor s, action_idx in enumerate(optimal_policy_vi):\n    print(f\"State {s}: {action_map[action_idx]}\")\n\nprint(\"\\nConverged Optimal Value Function (Value Iteration):\")\nfor s, value in enumerate(optimal_V_vi):\n    print(f\"State {s}: {value:.4f}\")","metadata":{"id":"5b3ef8e2","executionInfo":{"status":"ok","timestamp":1765176201639,"user_tz":-330,"elapsed":29,"user":{"displayName":"Debanik Debnath","userId":"16501660051911573481"}},"outputId":"d4550d4a-92ef-46dd-a95c-c2af20f343fa","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T07:15:29.023922Z","iopub.execute_input":"2025-12-08T07:15:29.024358Z","iopub.status.idle":"2025-12-08T07:15:29.037811Z","shell.execute_reply.started":"2025-12-08T07:15:29.024324Z","shell.execute_reply":"2025-12-08T07:15:29.037048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"q6gPvcEBx5qa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Monte Carlo Methods\n\n### Subtask:\nDelve into Monte Carlo (MC) methods as a fundamental approach to reinforcement learning, particularly for problems where a complete model of the environment is unknown. Explain how MC methods learn directly from episodes of experience. Cover two main aspects:\n\n1.  **Monte Carlo Prediction**: Detail how MC methods can be used to estimate the state-value function ($V_{\\pi}$) or the action-value function ($Q_{\\pi}$) for a given policy $\\pi$. Distinguish between **first-visit MC** (averaging returns only the first time a state/state-action pair is visited) and **every-visit MC** (averaging returns every time a state/state-action pair is visited). Discuss the concept of 'return' ($G_t$) and its calculation.\n2.  **Monte Carlo Control**: Explain how MC methods can be adapted to find optimal policies ($Q^*$) without a model. Introduce the idea of **Generalized Policy Iteration (GPI)** in the context of MC, highlighting **Monte Carlo ES (Exploring Starts)** as a method for ensuring all state-action pairs are visited. Discuss the importance of **exploration** strategies, such as $\\epsilon$-greedy policies, to ensure continued learning and to avoid getting stuck in sub-optimal solutions. Explain the challenges and benefits of MC methods, such as their ability to learn from actual experience and their unsuitability for problems with very long episodes or non-episodic tasks.","metadata":{"id":"2ee78cfe"}},{"cell_type":"markdown","source":"## Temporal Difference Learning: SARSA and Q-Learning\n\n### Subtask:\nIntroduce Temporal Difference (TD) learning as a cornerstone of modern reinforcement learning, bridging the gap between Monte Carlo methods and Dynamic Programming. Explain its key characteristic: learning directly from experience *without a model* and *without waiting until the end of an episode*.\n\n1.  **TD Prediction**: Explain how TD(0) can be used to estimate $V_{\\pi}$. Compare its update rule with MC updates and DP updates, emphasizing TD's 'bootstrapping' nature (using estimates to update estimates) and its lower variance compared to MC.\n2.  **SARSA (On-Policy TD Control)**: Detail the SARSA algorithm, an on-policy TD control method. Explain its update rule ($Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$) and why it's considered 'on-policy' (the same policy is used for action selection and for updating the Q-value). Discuss its exploration strategy, typically $\\epsilon$-greedy, and how SARSA converges to the optimal policy *for the policy being followed*.\n3.  **Q-Learning (Off-Policy TD Control)**: Detail the Q-Learning algorithm, a prominent off-policy TD control method. Explain its update rule ($Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]$) and why it's considered 'off-policy' (it learns the optimal Q-function directly, regardless of the policy being executed). Highlight how the $\\max$ operator makes it learn the optimal action-value function $Q^*$ directly. Compare and contrast SARSA and Q-Learning, focusing on their on-policy vs. off-policy nature, safety considerations, and convergence properties. Provide scenarios where one might be preferred over the other.","metadata":{"id":"729d1647"}},{"cell_type":"markdown","source":"## Function Approximation and Deep Q-Networks (DQN)\n\n### Subtask:\nAddress the critical challenge of scalability in reinforcement learning when dealing with large or continuous state and action spaces, where tabular methods become infeasible. Introduce the concept of function approximation to generalize value functions or policies from visited states to unseen states.\n\n1.  **Need for Function Approximation**: Explain why representing value functions as lookup tables is impractical for complex environments. Introduce the idea of using a parameterized function approximator (e.g., linear models, neural networks) to estimate $V(s)$ or $Q(s,a)$.\n2.  **Linear Function Approximation**: Briefly discuss linear function approximation as a simple example, where the value function is a linear combination of features. Explain its limitations.\n3.  **Deep Q-Networks (DQN)**: Dive deep into DQN, a groundbreaking algorithm that successfully combined Q-Learning with deep neural networks. Explain the challenges of using neural networks directly for Q-learning (e.g., instability, divergence due to correlated samples and non-stationary targets) and how DQN addresses these with two key innovations:\n    *   **Experience Replay**: Detail how a replay buffer stores transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ and randomly samples batches for training. Explain how this breaks temporal correlations and smooths out the data distribution.\n    *   **Target Network**: Explain the use of a separate 'target' Q-network for calculating the target values ($R_{t+1} + \\gamma \\max_{a'} Q_{\\text{target}}(S_{t+1}, a')$). Discuss how periodically updating this target network helps stabilize the learning process by providing a more consistent target.\n4.  **DQN Enhancements**: Briefly mention significant extensions and improvements to the original DQN algorithm, such as:\n    *   **Double DQN**: Addressing overestimation of Q-values.\n    *   **Dueling DQN**: Separating state-value and advantage functions.\n    *   **Prioritized Experience Replay**: Sampling important transitions more frequently.\n    *   **Multi-step Learning**: Combining TD and MC approaches for n-step returns.\nDiscuss the overall impact and success of DQN in solving complex control tasks, particularly in Atari games.","metadata":{"id":"3fc356e6"}},{"cell_type":"markdown","source":"## Policy Gradient Methods: REINFORCE\n\n### Subtask:\nExplore a distinct class of reinforcement learning algorithms that directly learn a parameterized policy $\\pi_{\\theta}(a|s)$, instead of learning value functions. Contrast policy-based methods with value-based methods, highlighting their advantages, particularly in continuous action spaces or stochastic environments.\n\n1.  **Policy-Based RL Introduction**: Explain why directly optimizing the policy can be beneficial, including handling continuous action spaces naturally and learning stochastic policies.\n2.  **The Policy Gradient Theorem**: Derive and explain the policy gradient theorem, which provides a mathematical foundation for optimizing policies using gradient ascent. Show how the gradient of the expected return can be expressed in terms of the policy's log-probability and the action-value function.\n3.  **REINFORCE Algorithm (Monte Carlo Policy Gradient)**: Detail the REINFORCE algorithm as a foundational policy gradient method. Explain how it uses Monte Carlo estimates of the return to update the policy parameters. Walk through the algorithm steps:\n    *   Sampling episodes.\n    *   Calculating returns ($G_t$).\n    *   Updating policy parameters using the gradient estimate:\n        $\\theta \\leftarrow \\theta + \\alpha \\nabla \\log \\pi_{\\theta}(A_t|S_t) G_t$\n4.  **Baselines for Variance Reduction**: Discuss the high variance often associated with Monte Carlo policy gradients. Introduce the concept of a **baseline** (e.g., the state-value function $V(s)$) and how subtracting it from the return ($G_t - b(S_t)$) can significantly reduce variance without changing the expected value of the gradient. Explain the concept of the **advantage function** $A(s,a) = Q(s,a) - V(s)$ and its role in policy gradient methods. Provide a simple example illustrating REINFORCE with a baseline.","metadata":{"id":"e6119e20"}},{"cell_type":"markdown","source":"## Actor-Critic Methods\n\n### Subtask:\nIntroduce Actor-Critic (AC) methods, which combine the strengths of both policy-based and value-based reinforcement learning approaches. Explain the architecture and interaction between the 'actor' (policy) and the 'critic' (value function).\n\n1.  **Core Idea of Actor-Critic**: Explain that AC agents consist of two main components:\n    *   **Actor**: A neural network or parameterized function that learns the policy $\\pi_{\\theta}(a|s)$ and is responsible for selecting actions.\n    *   **Critic**: A neural network or parameterized function that learns the value function (e.g., $V_{\\phi}(s)$ or $Q_{\\phi}(s,a)$) and is responsible for evaluating the actions taken by the actor.\n2.  **Interaction and Learning**: Describe how the critic provides an estimate of the advantage function or TD error, which is then used by the actor to update its policy parameters. This allows the actor to update its policy more frequently (after each step) and with lower variance than pure Monte Carlo policy gradient methods. Explain how both the actor and critic are trained simultaneously.\n3.  **Basic Actor-Critic Algorithm (e.g., A2C)**: Detail the general structure of a basic AC algorithm. Explain how the critic learns the value function by minimizing the TD error (or a similar objective), and how the actor learns the policy by maximizing the expected return using the critic's value estimates to guide its updates. Discuss the role of the TD error as a 'direction' or 'surprise' signal for the actor.\n4.  **Benefits and Challenges**: Discuss the advantages of AC methods, such as their ability to handle continuous action spaces, their generally lower variance compared to REINFORCE, and their efficiency (online learning). Also, address challenges like the potential for instability due to the interaction between two learning components and the need for careful hyperparameter tuning.\n5.  **Further Enhancements**: Briefly mention more advanced AC algorithms (e.g., A3C, DDPG, SAC, PPO) as extensions that improve stability and performance.","metadata":{"id":"865943d3"}},{"cell_type":"markdown","source":"## Advanced Topics and Current Research\n\n### Subtask:\nProvide a comprehensive overview of cutting-edge and specialized topics within reinforcement learning, reflecting the breadth and depth of the field beyond foundational algorithms. This section should give readers an understanding of the current research landscape and the challenges being tackled.\n\n1.  **Multi-Agent Reinforcement Learning (MARL)**:\n    *   **Introduction**: Explain the complexities introduced when multiple agents interact within an environment.\n    *   **Challenges**: Discuss issues like non-stationarity, credit assignment, and coordination vs. competition.\n    *   **Types of MARL**: Briefly cover cooperative, competitive, and mixed-sum games, with examples.\n2.  **Meta-Reinforcement Learning (Meta-RL)**:\n    *   **Concept**: Explain 'learning to learn' in the context of RL â€“ training an agent to adapt quickly to new, unseen tasks.\n    *   **Methods**: Briefly touch upon approaches like MAML (Model-Agnostic Meta-Learning) or using recurrent networks for fast adaptation.\n3.  **Imitation Learning / Inverse Reinforcement Learning (IRL)**:\n    *   **Imitation Learning**: Describe learning a policy directly from expert demonstrations (e.g., Behavioral Cloning) when rewards are sparse or hard to define.\n    *   **Inverse Reinforcement Learning**: Explain the challenge of inferring the reward function from observed expert behavior.\n4.  **Advanced Exploration-Exploitation Strategies**: Beyond simple $\\epsilon$-greedy, discuss more sophisticated methods:\n    *   **Intrinsic Motivation/Curiosity**: Agents generating their own rewards for novel experiences.\n    *   **Bayesian RL**: Using uncertainty estimates to guide exploration (e.g., UCB, Thompson Sampling).\n5.  **Offline RL / Batch RL**: Explain the paradigm of learning effective policies from static, previously collected datasets without further interaction with the environment. Discuss its importance for real-world applications where online interaction is costly or dangerous. Highlight challenges like extrapolation error.\n6.  **Safety in RL**: Address the critical aspect of ensuring that RL agents operate safely and reliably, especially in real-world systems. Cover topics like:\n    *   **Safe Exploration**: Preventing harmful actions during training.\n    *   **Constraint Satisfaction**: Learning policies that adhere to predefined safety constraints.\n    *   **Robustness**: Ensuring performance against perturbations or adversarial attacks.\n7.  **Generative Models and RL**: Explore the intersection of generative models (e.g., GANs, VAEs) with RL. This could include using generative models for state representation, planning, or simulating environments.","metadata":{"id":"93e15c5c"}},{"cell_type":"markdown","source":"## Reinforcement Learning Project Example\n\n### Subtask:\nDevelop a practical demonstration of an RL agent solving a task within a simulated environment. This section should serve as a hands-on application of the concepts learned throughout the notebook.\n\n1.  **Environment Selection**: Choose a suitable environment from the `Gymnasium` library (e.g., `CartPole-v1`, `LunarLander-v2`, `MountainCar-v0`). Justify the choice based on its characteristics (e.g., state/action space, episode length, reward structure).\n2.  **Agent Implementation**: Implement an RL agent using one of the algorithms covered (e.g., Q-Learning, DQN, or REINFORCE/Actor-Critic). Clearly define the agent's architecture, including:\n    *   **State Representation**: How the environment's state is fed to the agent.\n    *   **Action Space**: How the agent selects actions.\n    *   **Model Architecture**: If using function approximation, describe the neural network (e.g., number of layers, activation functions, output).\n    *   **Hyperparameters**: List and explain crucial hyperparameters (e.g., learning rate, discount factor, replay buffer size, $\\epsilon$-greedy parameters, network dimensions).\n3.  **Training Process**: Outline the training loop, including:\n    *   **Episode Simulation**: How the agent interacts with the environment.\n    *   **Experience Collection**: How data (transitions) is stored.\n    *   **Learning Updates**: How the agent's policy/value function is updated.\n    *   **Logging and Monitoring**: How progress is tracked (e.g., episode rewards, loss).\n4.  **Evaluation and Results**: After training, evaluate the agent's performance. This should include:\n    *   **Performance Metrics**: Plotting learning curves (e.g., average reward per episode over time).\n    *   **Qualitative Assessment**: Running trained agent(s) in the environment to visually demonstrate learned behavior.\n    *   **Discussion**: Analyze the results, discuss the agent's strengths and weaknesses, and potential areas for improvement.\n5.  **Code Walkthrough**: Provide well-commented code that implements the chosen agent and training process, making it easy for the reader to understand and reproduce.","metadata":{"id":"66da48f2"}}]}