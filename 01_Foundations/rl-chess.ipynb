{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:48:57.011306Z","iopub.execute_input":"2025-10-13T14:48:57.011867Z","iopub.status.idle":"2025-10-13T14:48:57.293597Z","shell.execute_reply.started":"2025-10-13T14:48:57.011847Z","shell.execute_reply":"2025-10-13T14:48:57.292967Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"pip install python-chess\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:48:57.294756Z","iopub.execute_input":"2025-10-13T14:48:57.295108Z","iopub.status.idle":"2025-10-13T14:49:04.339659Z","shell.execute_reply.started":"2025-10-13T14:48:57.295079Z","shell.execute_reply":"2025-10-13T14:49:04.338870Z"}},"outputs":[{"name":"stdout","text":"Collecting python-chess\n  Downloading python_chess-1.999-py3-none-any.whl.metadata (776 bytes)\nCollecting chess<2,>=1 (from python-chess)\n  Downloading chess-1.11.2.tar.gz (6.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nDownloading python_chess-1.999-py3-none-any.whl (1.4 kB)\nBuilding wheels for collected packages: chess\n  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147775 sha256=6090ccd0e759bd946b960601fee3ce2f4fc606435dc3b87a86b8c02c98a8c9db\n  Stored in directory: /root/.cache/pip/wheels/fb/5d/5c/59a62d8a695285e59ec9c1f66add6f8a9ac4152499a2be0113\nSuccessfully built chess\nInstalling collected packages: chess, python-chess\nSuccessfully installed chess-1.11.2 python-chess-1.999\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# %% [code]\n# %% [code]\n# advanced_chess_rl.py\n# Advanced RL chess agent with UCB exploration, position encoding, and difficulty levels\n\nfrom collections import defaultdict\nimport random\nimport math\nimport json\nimport os\nimport sys\nfrom typing import Dict, List, Tuple\n\ntry:\n    import chess\n    import chess.pgn\nexcept ImportError:\n    print(\"This script needs python-chess. Install with: pip install python-chess\")\n    sys.exit(1)\n\n# --- Configuration ---\nMODEL_FILE = \"chess_rl_model.json\"\nNUM_TRAINING_GAMES = 500\nMAX_MOVES_PER_GAME = 300\nBASE_TEMPERATURE = 0.8\nUCB_C = 1.4  # UCB exploration constant\n\nclass ChessRLAgent:\n    \"\"\"Advanced RL agent using Q-learning with position-aware features\"\"\"\n    \n    def __init__(self):\n        # Q-values: key = (position_hash, move_uci)\n        self.q_values = defaultdict(float)\n        # Visit counts for UCB\n        self.visit_counts = defaultdict(int)\n        self.state_visits = defaultdict(int)\n        # Learning parameters\n        self.learning_rate = 0.1\n        self.discount_factor = 0.95\n        self.epsilon = 0.2  # exploration rate during training\n        \n    def get_state_key(self, board: chess.Board) -> str:\n        \"\"\"Enhanced state representation with material and position info\"\"\"\n        # Use FEN without move counters for state key\n        fen_parts = board.fen().split()\n        return f\"{fen_parts[0]}_{fen_parts[1]}\"\n    \n    def get_move_features(self, board: chess.Board, move: chess.Move) -> Dict[str, float]:\n        \"\"\"Extract features for a move\"\"\"\n        features = {}\n        \n        # Capture bonus\n        features['capture'] = 1.0 if board.is_capture(move) else 0.0\n        \n        # Check bonus\n        board.push(move)\n        features['check'] = 1.0 if board.is_check() else 0.0\n        board.pop()\n        \n        # Center control (e4, d4, e5, d5)\n        center_squares = [chess.E4, chess.D4, chess.E5, chess.D5]\n        features['center'] = 1.0 if move.to_square in center_squares else 0.0\n        \n        # Development (moving pieces from back rank early)\n        piece = board.piece_at(move.from_square)\n        if piece and piece.piece_type in [chess.KNIGHT, chess.BISHOP]:\n            from_rank = chess.square_rank(move.from_square)\n            if (board.turn == chess.WHITE and from_rank == 0) or \\\n               (board.turn == chess.BLACK and from_rank == 7):\n                features['development'] = 1.0\n            else:\n                features['development'] = 0.0\n        else:\n            features['development'] = 0.0\n            \n        return features\n    \n    def get_q_value(self, board: chess.Board, move: chess.Move) -> float:\n        \"\"\"Get Q-value for a state-action pair\"\"\"\n        state = self.get_state_key(board)\n        key = (state, move.uci())\n        return self.q_values[key]\n    \n    def update_q_value(self, board: chess.Board, move: chess.Move, \n                       reward: float, next_board: chess.Board):\n        \"\"\"Update Q-value using Q-learning update rule\"\"\"\n        state = self.get_state_key(board)\n        key = (state, move.uci())\n        \n        # Get max Q-value for next state\n        if next_board.is_game_over():\n            max_next_q = 0.0\n        else:\n            legal_moves = list(next_board.legal_moves)\n            max_next_q = max([self.get_q_value(next_board, m) for m in legal_moves], \n                           default=0.0)\n        \n        # Q-learning update\n        old_q = self.q_values[key]\n        self.q_values[key] = old_q + self.learning_rate * (\n            reward + self.discount_factor * max_next_q - old_q\n        )\n    \n    def select_move_ucb(self, board: chess.Board, level: int = 5) -> chess.Move:\n        \"\"\"Select move using Upper Confidence Bound algorithm\"\"\"\n        legal_moves = list(board.legal_moves)\n        if not legal_moves:\n            return None\n        \n        state = self.get_state_key(board)\n        self.state_visits[state] += 1\n        \n        # Adjust exploration based on level (lower level = more random)\n        exploration_factor = UCB_C * (11 - level) / 5.0\n        \n        best_score = -float('inf')\n        best_moves = []\n        \n        for move in legal_moves:\n            key = (state, move.uci())\n            q_val = self.q_values[key]\n            visits = self.visit_counts[key]\n            \n            # UCB score\n            if visits == 0:\n                ucb_score = float('inf')  # Try unvisited moves first\n            else:\n                exploration_bonus = exploration_factor * math.sqrt(\n                    math.log(self.state_visits[state]) / visits\n                )\n                ucb_score = q_val + exploration_bonus\n            \n            if ucb_score > best_score:\n                best_score = ucb_score\n                best_moves = [move]\n            elif ucb_score == best_score:\n                best_moves.append(move)\n        \n        selected = random.choice(best_moves)\n        self.visit_counts[(state, selected.uci())] += 1\n        return selected\n    \n    def select_move_level(self, board: chess.Board, level: int) -> chess.Move:\n        \"\"\"Select move based on difficulty level (1-10)\"\"\"\n        legal_moves = list(board.legal_moves)\n        if not legal_moves:\n            return None\n        \n        if level == 1:\n            # Pure random\n            return random.choice(legal_moves)\n        \n        state = self.get_state_key(board)\n        \n        # Get Q-values for all moves\n        move_scores = [(move, self.get_q_value(board, move)) for move in legal_moves]\n        move_scores.sort(key=lambda x: x[1], reverse=True)\n        \n        # Temperature-based selection with level adjustment\n        # Higher level = lower temperature = more deterministic\n        temperature = BASE_TEMPERATURE * (11 - level) / 5.0\n        \n        if level >= 9:\n            # Almost always pick best move\n            if random.random() < 0.95:\n                return move_scores[0][0]\n        \n        # Softmax selection\n        scores = [score for _, score in move_scores]\n        max_score = max(scores)\n        exp_scores = [math.exp((s - max_score) / temperature) for s in scores]\n        total = sum(exp_scores)\n        probs = [e / total for e in exp_scores]\n        \n        return random.choices([m for m, _ in move_scores], weights=probs, k=1)[0]\n    \n    def save_model(self, filename: str):\n        \"\"\"Save model to file\"\"\"\n        data = {\n            'q_values': {f\"{k[0]}|{k[1]}\": v for k, v in self.q_values.items()},\n            'visit_counts': {f\"{k[0]}|{k[1]}\": v for k, v in self.visit_counts.items()},\n            'state_visits': dict(self.state_visits)\n        }\n        with open(filename, 'w') as f:\n            json.dump(data, f)\n        print(f\"Model saved to {filename}\")\n    \n    def load_model(self, filename: str):\n        \"\"\"Load model from file\"\"\"\n        if not os.path.exists(filename):\n            return False\n        \n        with open(filename, 'r') as f:\n            data = json.load(f)\n        \n        self.q_values = defaultdict(float, {\n            tuple(k.split('|')): v for k, v in data['q_values'].items()\n        })\n        self.visit_counts = defaultdict(int, {\n            tuple(k.split('|')): v for k, v in data['visit_counts'].items()\n        })\n        self.state_visits = defaultdict(int, data['state_visits'])\n        print(f\"Model loaded from {filename}\")\n        return True\n\ndef train_agent(agent: ChessRLAgent, num_games: int):\n    \"\"\"Train agent through self-play\"\"\"\n    print(f\"\\nðŸŽ¯ Training agent with {num_games} self-play games...\")\n    \n    results = {'1-0': 0, '0-1': 0, '1/2-1/2': 0, '*': 0}\n    \n    for game_num in range(num_games):\n        board = chess.Board()\n        game_history = []  # Store (board_state, move) tuples\n        \n        # Play one game\n        for move_num in range(MAX_MOVES_PER_GAME):\n            if board.is_game_over():\n                break\n            \n            # Epsilon-greedy exploration during training\n            if random.random() < agent.epsilon:\n                move = random.choice(list(board.legal_moves))\n            else:\n                move = agent.select_move_ucb(board, level=7)\n            \n            game_history.append((board.copy(), move))\n            board.push(move)\n        \n        # Game over - assign rewards\n        result = board.result()\n        results[result] += 1\n        \n        # Backpropagate rewards\n        if result == '1-0':  # White wins\n            final_reward = 1.0\n        elif result == '0-1':  # Black wins\n            final_reward = -1.0\n        elif result == '*':  # Game unfinished (move limit)\n            final_reward = 0.0\n        else:  # Draw\n            final_reward = 0.0\n        \n        # Update Q-values for all moves in the game\n        for i, (board_state, move) in enumerate(game_history):\n            # Alternating rewards for white/black\n            if board_state.turn == chess.WHITE:\n                reward = final_reward\n            else:\n                reward = -final_reward\n            \n            # Incremental reward shaping\n            if i < len(game_history) - 1:\n                next_board = game_history[i + 1][0]\n            else:\n                next_board = board\n            \n            agent.update_q_value(board_state, move, reward * 0.1, next_board)\n        \n        if (game_num + 1) % 50 == 0:\n            w, b, d, u = results['1-0'], results['0-1'], results['1/2-1/2'], results['*']\n            print(f\"  Game {game_num + 1}/{num_games} - \"\n                  f\"W:{w} B:{b} D:{d} Unfinished:{u}\")\n    \n    w, b, d, u = results['1-0'], results['0-1'], results['1/2-1/2'], results['*']\n    print(f\"\\nâœ… Training complete! Final: W:{w} B:{b} D:{d} Unfinished:{u}\")\n    return results\n\ndef play_game(agent: ChessRLAgent):\n    \"\"\"Interactive game loop\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"â™Ÿï¸  CHESS RL AGENT - INTERACTIVE PLAY\")\n    print(\"=\"*60)\n    \n    # Choose difficulty\n    while True:\n        try:\n            level = int(input(\"\\nSelect difficulty level (1-10, 1=easiest, 10=hardest): \"))\n            if 1 <= level <= 10:\n                break\n            print(\"Please enter a number between 1 and 10.\")\n        except ValueError:\n            print(\"Please enter a valid number.\")\n    \n    # Choose color\n    while True:\n        color = input(\"Play as White or Black? (w/b): \").strip().lower()\n        if color in ['w', 'b', 'white', 'black']:\n            player_white = color.startswith('w')\n            break\n        print(\"Please enter 'w' or 'b'.\")\n    \n    board = chess.Board()\n    move_number = 1\n    \n    print(f\"\\nðŸŽ® You are playing as {'White' if player_white else 'Black'}\")\n    print(f\"ðŸ¤– Agent difficulty: Level {level}/10\")\n    print(\"\\nCommands: type move in UCI format (e.g., e2e4), 'hint' for suggestion, 'quit' to exit\\n\")\n    \n    while not board.is_game_over():\n        print(f\"\\n{'='*60}\")\n        print(f\"Move {move_number}\")\n        print(f\"{'='*60}\")\n        print(board)\n        print()\n        \n        is_player_turn = (board.turn == chess.WHITE and player_white) or \\\n                        (board.turn == chess.BLACK and not player_white)\n        \n        if is_player_turn:\n            # Player's turn\n            color_name = \"white\" if board.turn == chess.WHITE else \"black\"\n            while True:\n                user_input = input(f\"Your move ({color_name}): \").strip().lower()\n                \n                if user_input in ['quit', 'exit', 'q']:\n                    print(\"\\nðŸ‘‹ Thanks for playing!\")\n                    return\n                \n                if user_input == 'hint':\n                    hint_move = agent.select_move_level(board, level=10)\n                    print(f\"ðŸ’¡ Hint: {hint_move.uci()} (Q={agent.get_q_value(board, hint_move):.3f})\")\n                    continue\n                \n                try:\n                    move = chess.Move.from_uci(user_input)\n                    if move in board.legal_moves:\n                        board.push(move)\n                        board.turn = not board.turn\n                        break\n                    else:\n                        print(\"âŒ Illegal move. Try again.\")\n                except:\n                    print(\"âŒ Invalid format. Use UCI notation (e.g., e2e4)\")\n        \n        else:\n            # Agent's turn\n            print(f\"ðŸ¤– Agent is thinking (level {level})...\")\n            move = agent.select_move_level(board, level)\n            q_val = agent.get_q_value(board, move)\n            print(f\"ðŸ¤– Agent plays: {move.uci()} (Q-value: {q_val:.3f})\")\n            board.push(move)\n        \n        if board.turn == chess.WHITE:\n            move_number += 1\n    \n    # Game over\n    print(f\"\\n{'='*60}\")\n    print(\"ðŸ GAME OVER\")\n    print(f\"{'='*60}\")\n    print(board)\n    print(f\"\\nResult: {board.result()}\")\n    \n    result = board.result()\n    if result == '1/2-1/2':\n        print(\"ðŸ¤ It's a draw!\")\n    elif (result == '1-0' and player_white) or (result == '0-1' and not player_white):\n        print(\"ðŸŽ‰ You win! Congratulations!\")\n    else:\n        print(\"ðŸ¤– Agent wins! Better luck next time!\")\n\ndef main():\n    print(\"\\n\" + \"=\"*60)\n    print(\"â™Ÿï¸  ADVANCED CHESS RL AGENT\")\n    print(\"=\"*60)\n    \n    agent = ChessRLAgent()\n    \n    # Try to load existing model\n    if agent.load_model(MODEL_FILE):\n        print(\"âœ… Existing model loaded!\")\n        action = input(\"\\nTrain more? (y/n): \").strip().lower()\n        if action == 'y':\n            train_agent(agent, NUM_TRAINING_GAMES)\n            agent.save_model(MODEL_FILE)\n    else:\n        print(\"ðŸ†• No existing model found. Training new agent...\")\n        train_agent(agent, NUM_TRAINING_GAMES)\n        agent.save_model(MODEL_FILE)\n    \n    # Play loop\n    while True:\n        play_game(agent)\n        again = input(\"\\nðŸ”„ Play again? (y/n): \").strip().lower()\n        if again != 'y':\n            break\n    \n    print(\"\\nðŸ‘‹ Thanks for playing!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:49:04.340781Z","iopub.execute_input":"2025-10-13T14:49:04.341041Z","execution_failed":"2025-10-13T14:57:15.644Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nâ™Ÿï¸  ADVANCED CHESS RL AGENT\n============================================================\nðŸ†• No existing model found. Training new agent...\n\nðŸŽ¯ Training agent with 500 self-play games...\n  Game 50/500 - W:3 B:7 D:7 Unfinished:33\n  Game 100/500 - W:8 B:10 D:11 Unfinished:71\n  Game 150/500 - W:13 B:18 D:18 Unfinished:101\n  Game 200/500 - W:18 B:21 D:26 Unfinished:135\n  Game 250/500 - W:19 B:28 D:38 Unfinished:165\n  Game 300/500 - W:24 B:33 D:47 Unfinished:196\n  Game 350/500 - W:27 B:36 D:53 Unfinished:234\n  Game 400/500 - W:30 B:38 D:60 Unfinished:272\n  Game 450/500 - W:34 B:38 D:70 Unfinished:308\n  Game 500/500 - W:39 B:43 D:80 Unfinished:338\n\nâœ… Training complete! Final: W:39 B:43 D:80 Unfinished:338\nModel saved to chess_rl_model.json\n\n============================================================\nâ™Ÿï¸  CHESS RL AGENT - INTERACTIVE PLAY\n============================================================\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nSelect difficulty level (1-10, 1=easiest, 10=hardest):  10\nPlay as White or Black? (w/b):  w\n"},{"name":"stdout","text":"\nðŸŽ® You are playing as White\nðŸ¤– Agent difficulty: Level 10/10\n\nCommands: type move in UCI format (e.g., e2e4), 'hint' for suggestion, 'quit' to exit\n\n\n============================================================\nMove 1\n============================================================\nr n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\nP P P P P P P P\nR N B Q K B N R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  e2e4\n"},{"name":"stdout","text":"\n============================================================\nMove 2\n============================================================\nr n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . . .\n. . . . P . . .\n. . . . . . . .\nP P P P . P P P\nR N B Q K B N R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  f2f4\n"},{"name":"stdout","text":"\n============================================================\nMove 3\n============================================================\nr n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . . .\n. . . . P P . .\n. . . . . . . .\nP P P P . . P P\nR N B Q K B N R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  hint\n"},{"name":"stdout","text":"ðŸ’¡ Hint: g1h3 (Q=0.000)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  g1h3\n"},{"name":"stdout","text":"\n============================================================\nMove 4\n============================================================\nr n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . . .\n. . . . P P . .\n. . . . . . . N\nP P P P . . P P\nR N B Q K B . R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  hint\n"},{"name":"stdout","text":"ðŸ’¡ Hint: h3g5 (Q=0.000)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  h3g5\n"},{"name":"stdout","text":"\n============================================================\nMove 5\n============================================================\nr n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . N .\n. . . . P P . .\n. . . . . . . .\nP P P P . . P P\nR N B Q K B . R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  hint\n"},{"name":"stdout","text":"ðŸ’¡ Hint: g5h7 (Q=0.000)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  g5h7\n"},{"name":"stdout","text":"\n============================================================\nMove 6\n============================================================\nr n b q k b n r\np p p p p p p N\n. . . . . . . .\n. . . . . . . .\n. . . . P P . .\n. . . . . . . .\nP P P P . . P P\nR N B Q K B . R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  hint\n"},{"name":"stdout","text":"ðŸ’¡ Hint: h7f8 (Q=0.000)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  h7h8\n"},{"name":"stdout","text":"âŒ Illegal move. Try again.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  h7f8\n"},{"name":"stdout","text":"\n============================================================\nMove 7\n============================================================\nr n b q k N n r\np p p p p p p .\n. . . . . . . .\n. . . . . . . .\n. . . . P P . .\n. . . . . . . .\nP P P P . . P P\nR N B Q K B . R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  hint\n"},{"name":"stdout","text":"ðŸ’¡ Hint: f8h7 (Q=0.000)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  f8h7\n"},{"name":"stdout","text":"\n============================================================\nMove 8\n============================================================\nr n b q k . n r\np p p p p p p N\n. . . . . . . .\n. . . . . . . .\n. . . . P P . .\n. . . . . . . .\nP P P P . . P P\nR N B Q K B . R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  hint\n"},{"name":"stdout","text":"ðŸ’¡ Hint: h7f8 (Q=0.000)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  h7f8\n"},{"name":"stdout","text":"\n============================================================\nMove 9\n============================================================\nr n b q k N n r\np p p p p p p .\n. . . . . . . .\n. . . . . . . .\n. . . . P P . .\n. . . . . . . .\nP P P P . . P P\nR N B Q K B . R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  hint\n"},{"name":"stdout","text":"ðŸ’¡ Hint: f8h7 (Q=0.000)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  f8h7\n"},{"name":"stdout","text":"\n============================================================\nMove 10\n============================================================\nr n b q k . n r\np p p p p p p N\n. . . . . . . .\n. . . . . . . .\n. . . . P P . .\n. . . . . . . .\nP P P P . . P P\nR N B Q K B . R\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (white):  e2e5\n"},{"name":"stdout","text":"âŒ Illegal move. Try again.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}