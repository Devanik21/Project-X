{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIh6f1pDkeSa",
        "outputId": "6c43ed3a-2f47-47c7-bae9-cf11456cbb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU DETECTED: HRF v26.0 'Holo-Fractal Universe' Active\n",
            "✅ Titan-21 Safety Protocol Engaged. System is stable.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import warnings\n",
        "from sklearn.utils import check_X_y, check_array\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.fft import fft\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Sklearn Core & Metrics\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import (\n",
        "    LinearDiscriminantAnalysis,\n",
        "    QuadraticDiscriminantAnalysis,\n",
        ")\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesClassifier,\n",
        "    RandomForestClassifier,\n",
        "    HistGradientBoostingClassifier,\n",
        ")\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import (\n",
        "    StratifiedKFold,\n",
        "    train_test_split,\n",
        "    cross_val_predict,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import (\n",
        "    PowerTransformer,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        "    MinMaxScaler,\n",
        ")\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "\n",
        "# Gradient Boosting\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# GPU CHECK\n",
        "try:\n",
        "    import cupy as cp\n",
        "\n",
        "    GPU_AVAILABLE = True\n",
        "    print(\"✅ GPU DETECTED: HRF v26.0 'Holo-Fractal Universe' Active\")\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"⚠️ GPU NOT FOUND: Running in Slow Mode\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# --- 1. THE HOLOGRAPHIC SOUL (Unit 3 - Multiverse Edition - VRAM PINNED) ---\n",
        "class HolographicSoulUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=15):\n",
        "        self.k = k\n",
        "        self.dna_ = {\n",
        "            \"freq\": 2.0, \"gamma\": 0.5, \"power\": 2.0,\n",
        "            \"metric\": \"minkowski\", \"p\": 2.0, \"phase\": 0.0,\n",
        "            \"dim_reduction\": \"none\",\n",
        "        }\n",
        "        self.projector_ = None\n",
        "        self.X_raw_source_ = None\n",
        "        # GPU Cache\n",
        "        self._X_train_gpu = None\n",
        "        self._y_train_gpu = None\n",
        "        # Pre-calculated norms for fast Euclidean\n",
        "        self._X_train_sq_norm = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self._apply_projection(X)\n",
        "        self.y_train_ = y\n",
        "\n",
        "        # [TITAN OPTIMIZATION] Upload to GPU ONCE\n",
        "        if GPU_AVAILABLE:\n",
        "            self._X_train_gpu = cp.asarray(self.X_train_, dtype=cp.float32)\n",
        "            self._y_train_gpu = cp.asarray(self.y_train_)\n",
        "            # Pre-calc Squared Norm for Fast Euclidean Path\n",
        "            self._X_train_sq_norm = cp.sum(self._X_train_gpu ** 2, axis=1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _apply_projection(self, X):\n",
        "        if self.dna_[\"dim_reduction\"] == \"holo\":\n",
        "            n_components = max(2, int(np.sqrt(X.shape[1])))\n",
        "            self.projector_ = GaussianRandomProjection(n_components=n_components, random_state=42)\n",
        "            self.X_train_ = self.projector_.fit_transform(X)\n",
        "        elif self.dna_[\"dim_reduction\"] == \"pca\":\n",
        "            n_components = max(2, int(np.sqrt(X.shape[1])))\n",
        "            self.projector_ = PCA(n_components=n_components, random_state=42)\n",
        "            self.X_train_ = self.projector_.fit_transform(X)\n",
        "        else:\n",
        "            self.projector_ = None\n",
        "            self.X_train_ = X\n",
        "\n",
        "    def set_raw_source(self, X):\n",
        "        self.X_raw_source_ = X\n",
        "\n",
        "    def evolve(self, X_val, y_val, generations=10):\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "\n",
        "        # [TITAN OPTIMIZATION] Pre-load Validation Data\n",
        "        X_val_curr = self.projector_.transform(X_val) if self.projector_ else X_val\n",
        "        X_val_g = cp.asarray(X_val_curr, dtype=cp.float32)\n",
        "        y_val_g = cp.asarray(y_val)\n",
        "\n",
        "        # Pre-calc validation norm for Fast Euclidean\n",
        "        val_sq_norm = cp.sum(X_val_g ** 2, axis=1)\n",
        "\n",
        "        n_universes = 8 # Slightly reduced for speed, keeps high diversity\n",
        "        best_dna = self.dna_.copy()\n",
        "\n",
        "        # Smart Init (Fast Sample)\n",
        "        sample_X = self._X_train_gpu[:100]\n",
        "        dists = cp.mean(cp.linalg.norm(sample_X[:, None, :] - sample_X[None, :, :], axis=2))\n",
        "        median_dist = float(cp.asnumpy(dists))\n",
        "        if median_dist > 0: best_dna[\"freq\"] = 3.14159 / median_dist\n",
        "\n",
        "        # Initial Score\n",
        "        best_acc = self._score_on_gpu(X_val_g, y_val_g, val_sq_norm)\n",
        "\n",
        "        patience = 0\n",
        "\n",
        "        for gen in range(generations):\n",
        "            candidates = []\n",
        "            for _ in range(n_universes):\n",
        "                mutant = best_dna.copy()\n",
        "                trait = random.choice(list(mutant.keys()))\n",
        "\n",
        "                if trait == \"freq\": mutant[\"freq\"] *= np.random.uniform(0.8, 1.25)\n",
        "                elif trait == \"gamma\": mutant[\"gamma\"] = np.random.uniform(0.1, 5.0)\n",
        "                elif trait == \"power\": mutant[\"power\"] = random.choice([0.5, 1.0, 2.0, 3.0, 4.0, 6.0])\n",
        "                elif trait == \"p\":\n",
        "                    # 50% chance to snap to 2.0 (Fast Path), 50% random\n",
        "                    if random.random() < 0.5: mutant[\"p\"] = 2.0\n",
        "                    else: mutant[\"p\"] = np.clip(mutant[\"p\"] + np.random.uniform(-0.5, 0.5), 0.5, 8.0)\n",
        "                elif trait == \"phase\": mutant[\"phase\"] = np.random.uniform(0, 3.14159)\n",
        "                candidates.append(mutant)\n",
        "\n",
        "            generation_best_acc = -1\n",
        "            generation_best_dna = None\n",
        "\n",
        "            for mutant_dna in candidates:\n",
        "                self.dna_ = mutant_dna\n",
        "                # Score using fast internal method\n",
        "                acc = self._score_on_gpu(X_val_g, y_val_g, val_sq_norm)\n",
        "\n",
        "                if acc > generation_best_acc:\n",
        "                    generation_best_acc = acc\n",
        "                    generation_best_dna = mutant_dna\n",
        "\n",
        "            if generation_best_acc >= best_acc:\n",
        "                best_acc = generation_best_acc\n",
        "                best_dna = generation_best_dna\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "\n",
        "            # Reset to best\n",
        "            self.dna_ = best_dna\n",
        "\n",
        "            # [TITAN OPTIMIZATION] Early Stopping\n",
        "            # If we don't improve for 8 generations, the soul is mature.\n",
        "            if patience >= 8:\n",
        "                break\n",
        "\n",
        "        self.dna_ = best_dna\n",
        "        del X_val_g, y_val_g, val_sq_norm\n",
        "        cp.get_default_memory_pool().free_all_blocks()\n",
        "\n",
        "        return best_acc\n",
        "\n",
        "    def _score_on_gpu(self, X_val_g, y_val_g, val_sq_norm=None):\n",
        "        probs = self._predict_proba_gpu_internal(X_val_g, val_sq_norm)\n",
        "        preds = cp.argmax(probs, axis=1)\n",
        "        return float(cp.mean(preds == y_val_g))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.projector_ is not None: X_curr = self.projector_.transform(X)\n",
        "        else: X_curr = X\n",
        "\n",
        "        if GPU_AVAILABLE:\n",
        "            X_g = cp.asarray(X_curr, dtype=cp.float32)\n",
        "            # Calc Norm for new data\n",
        "            x_sq_norm = cp.sum(X_g ** 2, axis=1)\n",
        "            probs = self._predict_proba_gpu_internal(X_g, x_sq_norm)\n",
        "            return cp.asnumpy(probs)\n",
        "        else:\n",
        "            return np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "    def _predict_proba_gpu_internal(self, X_te_g, X_te_sq_norm=None):\n",
        "        n_test = len(X_te_g)\n",
        "        n_classes = len(self.classes_)\n",
        "        probas = []\n",
        "        # Increased Batch Size for T4 (Matrix Multiplication can handle it)\n",
        "        #batch_size = 256 # for wide datasets\n",
        "        batch_size = 2048\n",
        "\n",
        "        p_norm = self.dna_.get(\"p\", 2.0)\n",
        "        gamma = self.dna_[\"gamma\"]\n",
        "        freq = self.dna_[\"freq\"]\n",
        "        power = self.dna_[\"power\"]\n",
        "        phase = self.dna_.get(\"phase\", 0.0)\n",
        "\n",
        "        # CHECK: Can we use Fast Euclidean? (p ~= 2.0)\n",
        "        use_fast_path = abs(p_norm - 2.0) < 0.05\n",
        "\n",
        "        for i in range(0, n_test, batch_size):\n",
        "            end = min(i + batch_size, n_test)\n",
        "            batch_te = X_te_g[i:end]\n",
        "\n",
        "            # --- DISTANCE CALCULATION ---\n",
        "            if use_fast_path and self._X_train_sq_norm is not None:\n",
        "                # [FAST PATH] A^2 + B^2 - 2AB\n",
        "                # 50x Speedup using Matrix Multiplication\n",
        "                if X_te_sq_norm is not None:\n",
        "                    batch_sq = X_te_sq_norm[i:end][:, None]\n",
        "                else:\n",
        "                    batch_sq = cp.sum(batch_te**2, axis=1, keepdims=True)\n",
        "\n",
        "                train_sq = self._X_train_sq_norm[None, :]\n",
        "                dot_prod = cp.dot(batch_te, self._X_train_gpu.T)\n",
        "\n",
        "                dists_sq = batch_sq + train_sq - 2 * dot_prod\n",
        "                dists_sq = cp.maximum(dists_sq, 0.0)\n",
        "                dists = cp.sqrt(dists_sq)\n",
        "            else:\n",
        "                # [SLOW PATH] Broadcasting for non-Euclidean metrics (p != 2)\n",
        "                diff = cp.abs(batch_te[:, None, :] - self._X_train_gpu[None, :, :])\n",
        "                dists = cp.sum(cp.power(diff, p_norm), axis=2)\n",
        "                dists = cp.power(dists, 1.0 / p_norm)\n",
        "\n",
        "            # --- WEIGHTING (RESONANCE) ---\n",
        "            # argpartition is faster than argsort for finding Top K\n",
        "            top_k_idx = cp.argsort(dists, axis=1)[:, : self.k]\n",
        "\n",
        "            row_idx = cp.arange(len(batch_te))[:, None]\n",
        "            top_dists = dists[row_idx, top_k_idx]\n",
        "            top_y = self._y_train_gpu[top_k_idx]\n",
        "\n",
        "            cosine_term = 1.0 + cp.cos(freq * top_dists + phase)\n",
        "            cosine_term = cp.maximum(cosine_term, 0.0)\n",
        "            w = cp.exp(-gamma * (top_dists**2)) * cosine_term\n",
        "            w = cp.power(w, power)\n",
        "\n",
        "            batch_probs = cp.zeros((len(batch_te), n_classes))\n",
        "            for c_idx, cls in enumerate(self.classes_):\n",
        "                class_mask = top_y == cls\n",
        "                batch_probs[:, c_idx] = cp.sum(w * class_mask, axis=1)\n",
        "\n",
        "            total_energy = cp.sum(batch_probs, axis=1, keepdims=True)\n",
        "            total_energy[total_energy == 0] = 1.0\n",
        "            batch_probs /= total_energy\n",
        "            probas.append(batch_probs)\n",
        "\n",
        "        return cp.concatenate(probas)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.predict(X))\n",
        "\n",
        "\n",
        "# --- 3. THE QUANTUM FIELD (Unit 4 - Reserve) ---\n",
        "class QuantumFieldUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.rbf_feature_ = RBFSampler(n_components=100, random_state=42)\n",
        "        self.classifier_ = RidgeClassifier(alpha=1.0)\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\"gamma\": 1.0, \"n_components\": 100}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.rbf_feature_.set_params(\n",
        "            gamma=self.dna_[\"gamma\"], n_components=self.dna_[\"n_components\"]\n",
        "        )\n",
        "        X_quantum = self.rbf_feature_.fit_transform(X)\n",
        "        self.classifier_.fit(X_quantum, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_quantum = self.rbf_feature_.transform(X)\n",
        "        d = self.classifier_.decision_function(X_quantum)\n",
        "        if len(self.classes_) == 2:\n",
        "            probs = 1 / (1 + np.exp(-d))\n",
        "            return np.column_stack([1 - probs, probs])\n",
        "        else:\n",
        "            exp_d = np.exp(d - np.max(d, axis=1, keepdims=True))\n",
        "            return exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "# --- 4. THE ENTROPY MAXWELL (Unit 5 - Reserve) ---\n",
        "class EntropyMaxwellUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.models_ = {}\n",
        "        self.classes_ = None\n",
        "        self.priors_ = None\n",
        "        self.dna_ = {\"n_components\": 1}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.models_ = {}\n",
        "        self.priors_ = {}\n",
        "        n_samples = len(y)\n",
        "        for cls in self.classes_:\n",
        "            X_c = X[y == cls]\n",
        "            if len(X_c) < 2:\n",
        "                self.priors_[cls] = 0.0\n",
        "                continue\n",
        "            self.priors_[cls] = len(X_c) / n_samples\n",
        "            n_comp = min(self.dna_[\"n_components\"], len(X_c))\n",
        "            gmm = GaussianMixture(\n",
        "                n_components=n_comp, covariance_type=\"full\", reg_covar=1e-4, random_state=42\n",
        "            )\n",
        "            gmm.fit(X_c)\n",
        "            self.models_[cls] = gmm\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = np.zeros((len(X), len(self.classes_)))\n",
        "        for i, cls in enumerate(self.classes_):\n",
        "            if cls in self.models_:\n",
        "                log_prob = self.models_[cls].score_samples(X)\n",
        "                log_prob = np.clip(log_prob, -100, 100)\n",
        "                probs[:, i] = np.exp(log_prob) * self.priors_[cls]\n",
        "        total = np.sum(probs, axis=1, keepdims=True) + 1e-10\n",
        "        return probs / total\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "# --- 5. THE OMNI-KERNEL NEXUS (Unit 6 - Reserve) ---\n",
        "class OmniKernelUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.model_ = None\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\n",
        "            \"kernel\": \"rbf\",\n",
        "            \"C\": 1.0,\n",
        "            \"gamma\": \"scale\",\n",
        "            \"degree\": 3,\n",
        "            \"coef0\": 0.0,\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.model_ = SVC(\n",
        "            kernel=self.dna_[\"kernel\"],\n",
        "            C=self.dna_[\"C\"],\n",
        "            gamma=self.dna_[\"gamma\"],\n",
        "            degree=self.dna_[\"degree\"],\n",
        "            coef0=self.dna_[\"coef0\"],\n",
        "            probability=True,\n",
        "            random_state=42,\n",
        "            cache_size=500,\n",
        "        )\n",
        "        self.model_.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model_.predict_proba(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return self.model_.score(X, y)\n",
        "\n",
        "\n",
        "# --- 18. THE GOLDEN SPIRAL (Unit 18 - Nature's Code) ---\n",
        "# --- 18. THE GOLDEN FOREST (GPU T4 - Parallel Ensemble) ---\n",
        "class GoldenSpiralUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=21, n_estimators=100):\n",
        "        # n_estimators=50 ensures 'Forest' power but keeps it sub-second on GPU\n",
        "        self.k = k\n",
        "        self.n_estimators = n_estimators\n",
        "        self.classes_ = None\n",
        "        self.X_train_ = None\n",
        "        self.y_train_ = None\n",
        "        # DNA: The \"Seed\" parameters for the forest\n",
        "        self.dna_ = {\"resonance\": 1.618, \"decay\": 1.618, \"shift\": 137.5}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if GPU_AVAILABLE:\n",
        "            self.X_train_ = cp.asarray(X, dtype=cp.float32)\n",
        "            self.y_train_ = cp.asarray(y)\n",
        "        else:\n",
        "            self.X_train_ = np.array(X, dtype=np.float32)\n",
        "            self.y_train_ = np.array(y)\n",
        "\n",
        "        # [GPU STRATEGY]: We don't train 50 separate trees.\n",
        "        # We store the data ONCE. We will simulate 50 \"viewpoints\" during prediction.\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        n_test = len(X_g)\n",
        "        n_classes = len(self.classes_)\n",
        "\n",
        "        # 1. THE HEAVY LIFT: Calculate Neighbors ONCE (The most expensive part)\n",
        "        # We use a single massive matrix op instead of 50 small ones.\n",
        "\n",
        "        # Euclidean Dist ^ 2 = x^2 + y^2 - 2xy\n",
        "        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n",
        "        Y2 = cp.sum(self.X_train_**2, axis=1)\n",
        "        XY = cp.dot(X_g, self.X_train_.T)\n",
        "        dists_sq = cp.maximum(X2 + Y2 - 2*XY, 0.0)\n",
        "        dists = cp.sqrt(dists_sq)\n",
        "\n",
        "        # Get Top K\n",
        "        top_k_idx = cp.argsort(dists, axis=1)[:, :self.k]\n",
        "        row_idx = cp.arange(n_test)[:, None]\n",
        "        top_dists = dists[row_idx, top_k_idx] # (N, k)\n",
        "        top_y = self.y_train_[top_k_idx]      # (N, k)\n",
        "\n",
        "        # 2. THE FOREST SIMULATION (Vectorized Ensemble)\n",
        "        # We apply 50 different \"Physics Laws\" to the SAME neighbors instantaneously.\n",
        "\n",
        "        total_probs = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "\n",
        "        # Generate random mutations for the ensemble on the fly (Deterministic seed)\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Batch the ensemble calculation\n",
        "        decay_vars = rng.uniform(0.5, 3.0, self.n_estimators)\n",
        "        shift_vars = rng.uniform(0.0, 360.0, self.n_estimators)\n",
        "        res_vars = rng.uniform(1.0, 2.0, self.n_estimators)\n",
        "\n",
        "        # Loop through \"Universes\" (Fast loop)\n",
        "        for i in range(self.n_estimators):\n",
        "            decay = decay_vars[i]\n",
        "            shift = np.deg2rad(shift_vars[i])\n",
        "            res = res_vars[i]\n",
        "\n",
        "            # Physics: Weight = 1/d^decay * Cosine_Resonance\n",
        "            # Add epsilon to dists\n",
        "            w_base = 1.0 / (cp.power(top_dists, decay) + 1e-9)\n",
        "            w_spiral = 1.0 + 0.5 * cp.cos(cp.log(top_dists + 1e-9) * res + shift)\n",
        "            w = w_base * cp.maximum(w_spiral, 0.0)\n",
        "\n",
        "            # Aggregate for this tree\n",
        "            tree_p = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "            for c_idx, cls in enumerate(self.classes_):\n",
        "                mask = (top_y == cls)\n",
        "                tree_p[:, c_idx] = cp.sum(w * mask, axis=1)\n",
        "\n",
        "            # Normalize tree\n",
        "            t_sum = cp.sum(tree_p, axis=1, keepdims=True)\n",
        "            total_probs += tree_p / (t_sum + 1e-9)\n",
        "\n",
        "        # Final Average\n",
        "        final_probs = total_probs / self.n_estimators\n",
        "        return cp.asnumpy(final_probs)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# ---Unit 19. THE ENTROPY FOREST (GPU T4 - Bootstrap Thermodynamics) ---\n",
        "class EntropyMaxwellUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=100):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.forest_stats_ = [] # Stores (mean, var) for 50 bootstraps\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\"n_components\": 100} # Placeholder\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if not GPU_AVAILABLE: return self\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        y_g = cp.asarray(y)\n",
        "        n_samples = len(X)\n",
        "\n",
        "        self.forest_stats_ = []\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Train 50 Universes instantly using GPU Bootstrap\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Bootstrap indices\n",
        "            indices = rng.choice(n_samples, n_samples, replace=True)\n",
        "            X_boot = X_g[indices]\n",
        "            y_boot = y_g[indices]\n",
        "\n",
        "            universe_stats = {}\n",
        "            for cls in self.classes_:\n",
        "                X_c = X_boot[y_boot == cls]\n",
        "                if len(X_c) < 2:\n",
        "                    # Fallback to global if class missing in bootstrap\n",
        "                    X_c = X_g[y_g == cls]\n",
        "\n",
        "                # We simply store Mean and Var (Gaussian Approximation)\n",
        "                # This is much faster than GMM and sufficient for Entropy Forest\n",
        "                mu = cp.mean(X_c, axis=0)\n",
        "                sigma = cp.var(X_c, axis=0) + 1e-5 # Stability\n",
        "                prior = len(X_c) / n_samples\n",
        "                universe_stats[cls] = (mu, sigma, prior)\n",
        "\n",
        "            self.forest_stats_.append(universe_stats)\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "        # Ensembling\n",
        "        for stats in self.forest_stats_:\n",
        "            univ_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "            for i, cls in enumerate(self.classes_):\n",
        "                mu, sigma, prior = stats[cls]\n",
        "                # Log-Gaussian PDF\n",
        "                log_p = -0.5 * cp.sum(cp.log(2 * np.pi * sigma), axis=0) - \\\n",
        "                        0.5 * cp.sum((X_g - mu)**2 / sigma, axis=1)\n",
        "                univ_probs[:, i] = log_p + cp.log(prior)\n",
        "\n",
        "            # Softmax this universe\n",
        "            max_p = cp.max(univ_probs, axis=1, keepdims=True)\n",
        "            exp_p = cp.exp(univ_probs - max_p)\n",
        "            univ_probs = exp_p / cp.sum(exp_p, axis=1, keepdims=True)\n",
        "\n",
        "            total_probs += univ_probs\n",
        "\n",
        "        return cp.asnumpy(total_probs / self.n_estimators)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 20. THE QUANTUM FOREST (GPU T4 - Parallel Ridge Fields) ---\n",
        "class QuantumFluxUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=100, gamma=1.5):\n",
        "        # 20 Quantum Realities (Heavy)\n",
        "        self.n_estimators = n_estimators\n",
        "        self.gamma = gamma\n",
        "        self.forest_ = []\n",
        "        self.classes_ = None\n",
        "        # [FIX] Added n_components to DNA so the logger prints correctly\n",
        "        self.dna_ = {\"gamma\": gamma, \"n_components\": 200}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if not GPU_AVAILABLE: return self\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "\n",
        "        # One-hot Y\n",
        "        y_onehot = cp.zeros((len(y), len(self.classes_)), dtype=cp.float32)\n",
        "        y_raw = cp.asarray(y)\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            y_onehot[y_raw == c, i] = 1.0\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        self.forest_ = []\n",
        "\n",
        "        # Train 20 Ridge Models in Parallel Universes\n",
        "        for i in range(self.n_estimators):\n",
        "            # Vary Gamma slightly for diversity\n",
        "            g_var = self.gamma * rng.uniform(0.8, 1.2)\n",
        "            n_comp = self.dna_[\"n_components\"] # Use DNA value\n",
        "\n",
        "            # RBF Weights\n",
        "            W = rng.normal(0, np.sqrt(2*g_var), (n_features, n_comp)).astype(cp.float32)\n",
        "            B = rng.uniform(0, 2*np.pi, n_comp).astype(cp.float32)\n",
        "\n",
        "            # Project X -> Z\n",
        "            Z = cp.cos(cp.dot(X_g, W) + B) * cp.sqrt(2./n_comp)\n",
        "\n",
        "            # Solve Ridge: (Z'Z + aI)^-1 Z'Y\n",
        "            alpha = 1.0\n",
        "            I = cp.eye(n_comp, dtype=cp.float32)\n",
        "\n",
        "            try:\n",
        "                # Cholesky solve (Ultra Fast on T4)\n",
        "                weights = cp.linalg.solve(cp.dot(Z.T, Z) + alpha*I, cp.dot(Z.T, y_onehot))\n",
        "                self.forest_.append((W, B, weights))\n",
        "            except: pass # Skip singular universes\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "        valid = 0\n",
        "        for W, B, weights in self.forest_:\n",
        "            Z = cp.cos(cp.dot(X_g, W) + B) * cp.sqrt(2./len(B))\n",
        "            raw = cp.dot(Z, weights)\n",
        "\n",
        "            # Softmax\n",
        "            max_r = cp.max(raw, axis=1, keepdims=True)\n",
        "            exp_r = cp.exp(raw - max_r)\n",
        "            p = exp_r / cp.sum(exp_r, axis=1, keepdims=True)\n",
        "\n",
        "            total_probs += p\n",
        "            valid += 1\n",
        "\n",
        "        return cp.asnumpy(total_probs / max(1, valid))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# --- 21. THE GRAVITY FOREST (GPU T4 - Many Body Simulation) ---\n",
        "class EventHorizonUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=100):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.centroids_ = None\n",
        "        self.masses_ = None\n",
        "        self.classes_ = None\n",
        "        # [FIX] Added 'decay_power' here to satisfy the printer logic\n",
        "        self.dna_ = {\"horizon_pct\": 10.0, \"decay_power\": 2.0}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if not GPU_AVAILABLE: return self\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        y_g = cp.asarray(y)\n",
        "\n",
        "        # Calculate Base Centers (The Stars)\n",
        "        self.centroids_ = []\n",
        "        self.masses_ = []\n",
        "        for cls in self.classes_:\n",
        "            X_c = X_g[y_g == cls]\n",
        "            if len(X_c) > 0:\n",
        "                self.centroids_.append(cp.mean(X_c, axis=0))\n",
        "                self.masses_.append(cp.log1p(len(X_c)))\n",
        "            else:\n",
        "                self.centroids_.append(cp.zeros(X.shape[1]))\n",
        "                self.masses_.append(0.0)\n",
        "\n",
        "        self.centroids_ = cp.array(self.centroids_) # (C, F)\n",
        "        self.masses_ = cp.array(self.masses_)       # (C,)\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "\n",
        "        # 1. Calculate Base Distances (Matrix: Samples x Classes)\n",
        "        # ||X - C||^2 = X^2 + C^2 - 2XC\n",
        "        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n",
        "        C2 = cp.sum(self.centroids_**2, axis=1)\n",
        "        XC = cp.dot(X_g, self.centroids_.T)\n",
        "        dist_sq = cp.maximum(X2 + C2 - 2*XC, 1e-9) # (N, C)\n",
        "\n",
        "        # 2. Simulate 50 Gravity Variations (The Forest)\n",
        "        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Use the decay power from DNA as the mean for the random variation\n",
        "        base_decay = self.dna_[\"decay_power\"]\n",
        "        decay_vars = rng.uniform(base_decay * 0.25, base_decay * 1.25, self.n_estimators)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            decay = decay_vars[i]\n",
        "\n",
        "            # Force = Mass / Dist^decay\n",
        "            # (Use Log space for stability)\n",
        "            # Log(F) = Log(M) - decay * Log(Dist^2)/2\n",
        "            # Log(Dist^2)/2 = Log(Dist)\n",
        "\n",
        "            log_dist = 0.5 * cp.log(dist_sq)\n",
        "            log_force = cp.log(self.masses_) - (decay * log_dist)\n",
        "\n",
        "            # Softmax forces\n",
        "            max_f = cp.max(log_force, axis=1, keepdims=True)\n",
        "            exp_f = cp.exp(log_force - max_f)\n",
        "            p = exp_f / cp.sum(exp_f, axis=1, keepdims=True)\n",
        "\n",
        "            total_probs += p\n",
        "\n",
        "        return cp.asnumpy(total_probs / self.n_estimators)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------\n",
        "\n",
        "# --- 18. THE FAST GOLDEN SPIRAL (Lite Version) ---\n",
        "class FastGoldenUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=21):\n",
        "        self.k = k\n",
        "        self.classes_ = None\n",
        "        self.X_train_ = None\n",
        "        self.y_train_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.X_train_ = np.array(X, dtype=np.float32)\n",
        "        self.y_train_ = np.array(y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # FAST LOGIC: No ensemble. Just one Golden Ratio weighted KNN.\n",
        "        # We use standard Euclidean distance but weight neighbors by 1/d^Phi\n",
        "        from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "        X_test = np.array(X, dtype=np.float32)\n",
        "        dists = euclidean_distances(X_test, self.X_train_)\n",
        "\n",
        "        # Get Top K neighbors\n",
        "        idx = np.argsort(dists, axis=1)[:, :self.k]\n",
        "        row_idx = np.arange(len(X))[:, None]\n",
        "\n",
        "        top_dists = dists[row_idx, idx]\n",
        "        top_y = self.y_train_[idx]\n",
        "\n",
        "        # PHI PHYSICS: Weight = 1 / (Distance ^ 1.618)\n",
        "        phi = 1.6180339887\n",
        "        weights = 1.0 / (np.power(top_dists, phi) + 1e-9)\n",
        "\n",
        "        probs = np.zeros((len(X), len(self.classes_)))\n",
        "        for c_idx, cls in enumerate(self.classes_):\n",
        "            # Sum weights where neighbor class matches\n",
        "            mask = (top_y == cls)\n",
        "            probs[:, c_idx] = np.sum(weights * mask, axis=1)\n",
        "\n",
        "        # Normalize\n",
        "        sums = np.sum(probs, axis=1, keepdims=True)\n",
        "        return np.nan_to_num(probs / (sums + 1e-9), nan=1.0/len(self.classes_))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# --- 19. THE FAST ENTROPY (Gaussian Thermodynamics) ---\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "class FastEntropyUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        # GaussianNB is literally a probability density calculator (Thermodynamics)\n",
        "        # It is extremely fast (O(n))\n",
        "        self.model = GaussianNB()\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "\n",
        "# --- 20. THE FAST QUANTUM (Single Field Ridge) ---\n",
        "class FastQuantumUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, gamma=1.0, n_components=100):\n",
        "        # No ensemble. Just one mapping to higher dimension + Linear Solver\n",
        "        self.gamma = gamma\n",
        "        self.n_components = n_components\n",
        "        self.rbf = RBFSampler(gamma=gamma, n_components=n_components, random_state=42)\n",
        "        self.solver = RidgeClassifier(alpha=1.0)\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        X_q = self.rbf.fit_transform(X)\n",
        "        self.solver.fit(X_q, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_q = self.rbf.transform(X)\n",
        "        d = self.solver.decision_function(X_q)\n",
        "\n",
        "        # Manual Softmax\n",
        "        if len(d.shape) == 1:\n",
        "            p = 1 / (1 + np.exp(-d))\n",
        "            return np.column_stack([1-p, p])\n",
        "        else:\n",
        "            exp_d = np.exp(d - np.max(d, axis=1, keepdims=True))\n",
        "            return exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# --- 21. THE FAST GRAVITY (Newtonian Centers) ---\n",
        "class FastGravityUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.centroids_ = []\n",
        "        self.masses_ = []\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.centroids_ = []\n",
        "        self.masses_ = []\n",
        "\n",
        "        # Calculate Center of Mass for each class once\n",
        "        for cls in self.classes_:\n",
        "            X_c = X[y == cls]\n",
        "            if len(X_c) > 0:\n",
        "                self.centroids_.append(np.mean(X_c, axis=0))\n",
        "                # Mass = log(count) to prevent huge class imbalance bias\n",
        "                self.masses_.append(np.log1p(len(X_c)))\n",
        "            else:\n",
        "                self.centroids_.append(np.zeros(X.shape[1]))\n",
        "                self.masses_.append(0)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "        # Vectorized Gravity Calculation\n",
        "        for i, (center, mass) in enumerate(zip(self.centroids_, self.masses_)):\n",
        "            # Distance squared (Newtonian)\n",
        "            d2 = np.sum((X - center)**2, axis=1)\n",
        "            # Force = Mass / Distance^2\n",
        "            force = mass / (d2 + 1e-9)\n",
        "            probs[:, i] = force\n",
        "\n",
        "        # Normalize\n",
        "        sums = np.sum(probs, axis=1, keepdims=True)\n",
        "        return np.nan_to_num(probs / (sums + 1e-9), nan=1.0/len(self.classes_))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# --- 22. THE OMEGA POINT (The Hidden Infinity Engine - Tensor Core) ---\n",
        "class TheOmegaPoint_Unit22(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.classes_ = None\n",
        "        self.model_ = None\n",
        "        self.pca_vector_ = None  # To store the \"Principal Vibration\"\n",
        "        self.scaler_ = StandardScaler()\n",
        "\n",
        "    def _apply_theoretical_transforms(self, X, is_training=False):\n",
        "        # 1. Standardize Reality\n",
        "        if is_training:\n",
        "            X_geo = self.scaler_.fit_transform(X)\n",
        "        else:\n",
        "            X_geo = self.scaler_.transform(X)\n",
        "\n",
        "        n_samples, n_features = X_geo.shape\n",
        "\n",
        "        # --- THEORY 1: THE TENSOR FIELD (Interaction Energy) ---\n",
        "        # Instead of Phase, we calculate the PHYSICAL INTERACTION between forces.\n",
        "        # This creates a \"Force Field\" of all possible pairings (x1*x2, x1*x3...)\n",
        "        # Mathematics: Outer Product -> Upper Triangle\n",
        "        tensor_list = []\n",
        "        for i in range(n_features):\n",
        "            for j in range(i, n_features):\n",
        "                tensor_list.append(X_geo[:, i] * X_geo[:, j])\n",
        "        tensor_field = np.column_stack(tensor_list)\n",
        "\n",
        "        # --- THEORY 2: SCHRODINGER KINETIC ENERGY ---\n",
        "        # Kinetic Energy = 1/2 * mass * velocity^2\n",
        "        # We treat the value as velocity.\n",
        "        kinetic = 0.5 * (X_geo ** 2)\n",
        "\n",
        "        # --- THEORY 3: SHANNON ENTROPY (Information Density) ---\n",
        "        # How \"surprising\" is this data point?\n",
        "        # We transform to probabilities first (Softmax-ish)\n",
        "        p = np.abs(X_geo) / (np.sum(np.abs(X_geo), axis=1, keepdims=True) + 1e-9)\n",
        "        entropy = -np.sum(p * np.log(p + 1e-9), axis=1, keepdims=True)\n",
        "\n",
        "        # --- THEORY 4: THE GOD ALEPH (EIGEN-RESONANCE) ---\n",
        "        # We project the entire reality onto its \"Principal Vibration\" (First Eigenvector).\n",
        "        # This is the \"Main Frequency\" of the universe (Dataset).\n",
        "        if is_training:\n",
        "            cov_mat = np.cov(X_geo.T)\n",
        "            eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n",
        "            self.pca_vector_ = eig_vecs[:, -1]\n",
        "\n",
        "        aleph = np.dot(X_geo, self.pca_vector_).reshape(-1, 1)\n",
        "\n",
        "        # FINAL STACKING\n",
        "        omega_features = np.hstack(\n",
        "            [\n",
        "                X_geo,  # Base\n",
        "                kinetic,  # Physics\n",
        "                entropy,  # Info\n",
        "                tensor_field,  # Geometry (High Dim)\n",
        "                aleph,  # Divinity\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return np.nan_to_num(omega_features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "    def _benchmark_divinity(self, X_omega, y, n_orig):\n",
        "        \"\"\"\n",
        "        Benchmarks the new Tensor Reality.\n",
        "        \"\"\"\n",
        "        from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 65)\n",
        "        print(\" | THE DIVINE INSPECTION: TENSOR DIMENSION ACCURACIES |\")\n",
        "        print(\"-\" * 65)\n",
        "        print(f\" {'THEORETICAL LAYER':<25} | {'ACCURACY':<10} | {'STATUS':<10}\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "        n = n_orig\n",
        "        layers = [\n",
        "            (\"Base Reality (Norm)\", 0, n),\n",
        "            (\"Kinetic Energy\", n, 2 * n),\n",
        "            (\"Shannon Entropy\", 2 * n, 2 * n + 1),\n",
        "            (\"The Tensor Field\", 2 * n + 1, X_omega.shape[1] - 1),\n",
        "            (\"THE GOD ALEPH (Eigen)\", X_omega.shape[1] - 1, X_omega.shape[1]),\n",
        "        ]\n",
        "\n",
        "        for name, start, end in layers:\n",
        "            X_subset = X_omega[:, start:end]\n",
        "            probe = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "            probe.fit(X_subset, y)\n",
        "            acc = probe.score(X_subset, y)\n",
        "            print(f\" {name:<25} | {acc:.2%}    | Active\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if hasattr(self, \"verbose\") and self.verbose:\n",
        "            print(\" [OMEGA] TRANSCODING REALITY INTO TENSOR FIELDS...\")\n",
        "\n",
        "        X_omega = self._apply_theoretical_transforms(X, is_training=True)\n",
        "        self._benchmark_divinity(X_omega, y, X.shape[1])\n",
        "\n",
        "        self.model_ = ExtraTreesClassifier(\n",
        "            n_estimators=1000,\n",
        "            max_depth=None,\n",
        "            max_features=\"sqrt\",\n",
        "            bootstrap=False,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        self.model_.fit(X_omega, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_omega = self._apply_theoretical_transforms(X, is_training=False)\n",
        "        return self.model_.predict_proba(X_omega)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "# --- 23. THE FRACTAL MIRROR (Unit 23 - Dynamic Elite Sync) ---\n",
        "class FractalMirrorUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, top_3_models):\n",
        "        \"\"\"\n",
        "        DYNAMIC ARCHITECTURE:\n",
        "        Accepts the 'Top 3 Elite' models found by the Council.\n",
        "        These change for every dataset (e.g., Logic+Soul+Gravity vs. Quantum+Gradient+Bio).\n",
        "        \"\"\"\n",
        "        self.top_3_models = top_3_models\n",
        "        self.classes_ = None\n",
        "\n",
        "        # HYBRID META-LEARNERS\n",
        "        # 1. The Conservative Judge (Ridge): Prevents overfitting, handles linear corrections.\n",
        "        self.judge_linear_ = RidgeClassifier(alpha=10.0, class_weight=\"balanced\")\n",
        "        # 2. The Creative Judge (Boosting): Finds complex non-linear patches in the elites' logic.\n",
        "        self.judge_boost_ = HistGradientBoostingClassifier(\n",
        "            max_iter=100,\n",
        "            max_depth=4,\n",
        "            max_leaf_nodes=15,       # <--- NEW: Restricts complexity\n",
        "            l2_regularization=20.0,  # <--- NEW: Prevents overfitting\n",
        "            learning_rate=0.02,\n",
        "            early_stopping=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def _get_council_opinions(self, X, y=None, is_training=False):\n",
        "        \"\"\"\n",
        "        Generates the Council's input.\n",
        "        - Training: Uses Cross-Validation (Blindfolding) to see REAL errors.\n",
        "        - Prediction: Uses standard prediction.\n",
        "        \"\"\"\n",
        "        meta_features = []\n",
        "        for model in self.top_3_models:\n",
        "            # A: TRAINING PHASE (Blindfolded CV)\n",
        "            if is_training and y is not None:\n",
        "                try:\n",
        "                    # We use 5-fold CV to get a robust \"out-of-sample\" view\n",
        "                    if hasattr(model, \"predict_proba\"):\n",
        "                        p = cross_val_predict(\n",
        "                            model, X, y, cv=5, method=\"predict_proba\", n_jobs=-1\n",
        "                        )\n",
        "                    else:\n",
        "                        d = cross_val_predict(\n",
        "                            model, X, y, cv=5, method=\"decision_function\", n_jobs=-1\n",
        "                        )\n",
        "                        # Softmax normalization for decision functions\n",
        "                        p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n",
        "                except:\n",
        "                    # Fallback (Safety Net): Standard fit if CV crashes\n",
        "                    model.fit(X, y)\n",
        "                    if hasattr(model, \"predict_proba\"):\n",
        "                        p = model.predict_proba(X)\n",
        "                    else:\n",
        "                        p = np.ones((len(X), len(np.unique(y)))) / len(np.unique(y))\n",
        "\n",
        "            # B: PREDICTION PHASE (Standard)\n",
        "            else:\n",
        "                if hasattr(model, \"predict_proba\"):\n",
        "                    p = model.predict_proba(X)\n",
        "                else:\n",
        "                    d = model.decision_function(X)\n",
        "                    p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n",
        "\n",
        "            # Clean NaNs (Safety)\n",
        "            p = np.nan_to_num(p, 0.0)\n",
        "            meta_features.append(p)\n",
        "\n",
        "        return np.hstack(meta_features)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        # STEP 1: CROSS-VALIDATION (The Truth Serum)\n",
        "        # We extract features BEFORE retraining the models, so we capture their true mistakes.\n",
        "        X_council = self._get_council_opinions(X, y, is_training=True)\n",
        "\n",
        "        # STEP 2: DYNAMIC SYNC (The Power Up)\n",
        "        # Now we retrain the Top 3 Elites on 100% of this data.\n",
        "        # This guarantees they are fully adapted to this specific dataset.\n",
        "        for model in self.top_3_models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        # STEP 3: STACKING (The Mirror)\n",
        "        # Input = Original Data + Elite Opinions\n",
        "        X_stack = X_council\n",
        "\n",
        "        # STEP 4: TRAIN THE META-JUDGES\n",
        "        # Ridge ensures we don't hallucinate.\n",
        "        self.judge_linear_.fit(X_council, y)\n",
        "        # Boosting fixes the hard edge cases.\n",
        "        self.judge_boost_.fit(X_stack, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # 1. Ask the Synced Elites\n",
        "        X_council = self._get_council_opinions(X, is_training=False)\n",
        "        X_stack = X_council\n",
        "\n",
        "        # 2. Get Conservative Opinion (Linear)\n",
        "        d_linear = self.judge_linear_.decision_function(X_council)\n",
        "        if len(d_linear.shape) == 1: # Binary handling\n",
        "            p_linear = 1 / (1 + np.exp(-d_linear))\n",
        "            p_linear = np.column_stack([1-p_linear, p_linear])\n",
        "        else: # Multi-class\n",
        "            exp_d = np.exp(d_linear - np.max(d_linear, axis=1, keepdims=True))\n",
        "            p_linear = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "        # 3. Get Corrective Opinion (Boosting)\n",
        "        p_boost = self.judge_boost_.predict_proba(X_stack)\n",
        "\n",
        "        # 4. The Final Balanced Verdict\n",
        "        # 60% Boosting (Intelligence) + 40% Linear (Stability)\n",
        "        # This ratio provides the \"Tie or Win\" guarantee.\n",
        "        return 0.7 * p_linear + 0.3 * p_boost\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "\n",
        "# --- 24. DIMENSION Z (The Infinite Alien - Balanced) ---\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- 24. DIMENSION Z (The Final Sniper - Sharpened Ace) ---\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.base import clone\n",
        "\n",
        "# --- 24. DIMENSION Z (The Universal Geometric Corrector) ---\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "class AlienDimensionZ(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    THE UNIVERSAL WHETSTONE.\n",
        "    Role: Wakes up AFTER Phase 4.\n",
        "    Operation: Takes the WINNING PROBABILITIES (Council or Ace) and\n",
        "               bends them to match the local geometry of the universe.\n",
        "    \"\"\"\n",
        "    def __init__(self, impact_factor=0.15):\n",
        "        # impact_factor: How much we trust geometry over logic (0.15 = 15%)\n",
        "        self.impact_factor = impact_factor\n",
        "        self.geometry_lock_ = None\n",
        "        self.y_train_ = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.y_train_ = y\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        # MEMORIZE THE GEOMETRY (The Reality Check)\n",
        "        # We use a K-Tree to find exactly what the neighbors say\n",
        "        self.geometry_lock_ = NearestNeighbors(n_neighbors=33, metric='minkowski', p=2, n_jobs=-1)\n",
        "        self.geometry_lock_.fit(X)\n",
        "        return self\n",
        "\n",
        "    def sharpen_probabilities(self, input_probs, X_new):\n",
        "        \"\"\"\n",
        "        Takes the Logic's opinion (input_probs) and blends it with\n",
        "        Physical Reality (Neighbor Consensus).\n",
        "        \"\"\"\n",
        "        if self.geometry_lock_ is None:\n",
        "            return input_probs\n",
        "\n",
        "        # 1. Ask the Universe: \"Who is near this point?\"\n",
        "        dists, indices = self.geometry_lock_.kneighbors(X_new)\n",
        "\n",
        "        # 2. Calculate Geometric Gravity\n",
        "        # (Weighted vote of neighbors based on distance)\n",
        "        p_geom = np.zeros_like(input_probs)\n",
        "        n_samples = len(X_new)\n",
        "\n",
        "        # Vectorized neighbor voting for speed\n",
        "        neighbor_votes = self.y_train_[indices] # (N, k)\n",
        "\n",
        "        # Distance weights (Inverse distance)\n",
        "        weights = 1.0 / (dists + 1e-9)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Weighted bin count for this sample\n",
        "            for k_idx, class_label in enumerate(neighbor_votes[i]):\n",
        "                # Find column index for this class\n",
        "                col_idx = np.where(self.classes_ == class_label)[0][0]\n",
        "                p_geom[i, col_idx] += weights[i, k_idx]\n",
        "\n",
        "        # Normalize Geometry Probabilities\n",
        "        row_sums = p_geom.sum(axis=1, keepdims=True)\n",
        "        p_geom = np.divide(p_geom, row_sums, out=np.zeros_like(p_geom), where=row_sums!=0)\n",
        "\n",
        "        # 3. The Fusion (Logic + Geometry)\n",
        "        # We blend the Input (Council/Ace) with the Geometry\n",
        "        final_probs = ((1.0 - self.impact_factor) * input_probs) + (self.impact_factor * p_geom)\n",
        "\n",
        "        return final_probs\n",
        "\n",
        "    def predict(self, input_probs, X_new):\n",
        "        final_p = self.sharpen_probabilities(input_probs, X_new)\n",
        "        return self.classes_[np.argmax(final_p, axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "# --- 25. THE NEURAL-MANIFOLD ENGINE (Unit 25 - The Universal Solver) ---\n",
        "# --- 25. THE OMEGA NEURAL ENGINE (Unit 25 - True Infinite Freedom) ---\n",
        "from scipy.linalg import pinv\n",
        "from scipy.special import expit, erf\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- 25. THE OMEGA NEURAL ENGINE (Unit 25 - GPU ACCELERATED) ---\n",
        "try:\n",
        "    import cupy as cp\n",
        "    import cupyx.scipy.special as cpx  # For erf/expit on GPU\n",
        "    GPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    import numpy as cp\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"⚠️ GPU NOT FOUND: Neural Engine running on CPU (Slow Mode)\")\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class NeuralManifoldUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_hidden=100, activation=\"tanh\",\n",
        "                 alpha=0.5, beta=1.0,\n",
        "                 gamma=1.0, bias_scale=1.0, power=1.0):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.activation = activation\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.bias_scale = bias_scale\n",
        "        self.power = power\n",
        "\n",
        "        self.input_weights_ = None\n",
        "        self.bias_ = None\n",
        "        self.output_weights_ = None\n",
        "        self.classes_ = None\n",
        "        self._X_train_gpu = None # GPU Cache\n",
        "        self._y_train_gpu = None # GPU Cache\n",
        "        self._rng_seed = 42\n",
        "\n",
        "    def _get_gpu_rng(self, seed):\n",
        "        return cp.random.RandomState(seed)\n",
        "\n",
        "    def _activate(self, X, dna=None):\n",
        "        # Unpack DNA\n",
        "        d = dna if dna else self.__dict__\n",
        "        act_name = d.get('activation', self.activation)\n",
        "        b = d.get('beta', self.beta)\n",
        "        g = d.get('gamma', self.gamma)\n",
        "        bs = d.get('bias_scale', self.bias_scale)\n",
        "        p = d.get('power', self.power)\n",
        "        n_h = d.get('n_hidden', self.n_hidden)\n",
        "\n",
        "        # Slice weights (Virtual Resizing on GPU)\n",
        "        W = self.input_weights_[:X.shape[1], :n_h]\n",
        "        B = self.bias_[:n_h]\n",
        "\n",
        "        # Projection (Chaos Injection)\n",
        "        # X is already on GPU here\n",
        "        H = cp.dot(X * g, W) + (B * bs)\n",
        "\n",
        "        # Infinite Library (GPU Optimized)\n",
        "        if act_name == \"tanh\": H = cp.tanh(b * H)\n",
        "        elif act_name == \"sine\": H = cp.sin(b * H)\n",
        "        elif act_name == \"sigmoid\": H = 1.0 / (1.0 + cp.exp(-b * H))\n",
        "        elif act_name == \"relu\": H = cp.maximum(0, H)\n",
        "        elif act_name == \"swish\": H = H * (1.0 / (1.0 + cp.exp(-b * H)))\n",
        "        elif act_name == \"mish\": H = H * cp.tanh(cp.log1p(cp.exp(H)))\n",
        "        elif act_name == \"gaussian\": H = cp.exp(-1.0 * (b * H)**2)\n",
        "        elif act_name == \"sinc\": H = cp.sinc(b * H)\n",
        "        elif act_name == \"elu\": H = cp.where(H > 0, H, b * (cp.exp(H) - 1))\n",
        "        elif act_name == \"softsign\": H = H / (1 + cp.abs(H))\n",
        "        elif act_name == \"cosine\": H = cp.cos(b * H)\n",
        "        elif act_name == \"bent_id\": H = (cp.sqrt(H**2 + 1) - 1)/2 + H\n",
        "        # Fallback\n",
        "        else: H = cp.tanh(b * H)\n",
        "\n",
        "        # Polynomial Manifold\n",
        "        if p != 1.0:\n",
        "            H = cp.sign(H) * cp.abs(H) ** p\n",
        "\n",
        "        return H\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Move Data to GPU ONCE (Crucial for Speed)\n",
        "        if GPU_AVAILABLE:\n",
        "            self._X_train_gpu = cp.asarray(X, dtype=cp.float32)\n",
        "            # One-hot encode on GPU\n",
        "            y_encoded = cp.zeros((n_samples, len(self.classes_)))\n",
        "            y_gpu = cp.asarray(y)\n",
        "            for i, c in enumerate(self.classes_):\n",
        "                y_encoded[y_gpu == c, i] = 1\n",
        "            self._y_train_gpu = y_encoded\n",
        "            self._y_labels_gpu = y_gpu # For scoring\n",
        "        else:\n",
        "            # CPU Fallback\n",
        "            self._X_train_gpu = X\n",
        "            y_encoded = np.zeros((n_samples, len(self.classes_)))\n",
        "            for i, c in enumerate(self.classes_):\n",
        "                y_encoded[y == c, i] = 1\n",
        "            self._y_train_gpu = y_encoded\n",
        "            self._y_labels_gpu = y\n",
        "\n",
        "        # Initialize Weights in VRAM\n",
        "        max_hidden = 5000\n",
        "        rng = self._get_gpu_rng(self._rng_seed)\n",
        "\n",
        "        if self.input_weights_ is None:\n",
        "            self.input_weights_ = rng.normal(size=(n_features, max_hidden), dtype=cp.float32)\n",
        "            self.bias_ = rng.normal(size=(max_hidden,), dtype=cp.float32)\n",
        "\n",
        "        # Solve (GPU Pinv is 50x faster)\n",
        "        self._solve_weights(self.__dict__)\n",
        "        return self\n",
        "\n",
        "    def _solve_weights(self, dna):\n",
        "        H = self._activate(self._X_train_gpu, dna)\n",
        "        n_h = dna.get('n_hidden', self.n_hidden)\n",
        "        I = cp.eye(n_h, dtype=cp.float32)\n",
        "\n",
        "        # The Heavy Lifting: Matrix Inversion on Tensor Core\n",
        "        # Ridge: (H^T H + alpha*I)^-1 H^T Y\n",
        "        # Using pseudo-inverse for maximum stability\n",
        "        H_inv = cp.linalg.pinv(cp.dot(H.T, H) + dna['alpha'] * I)\n",
        "        self.output_weights_ = cp.dot(cp.dot(H_inv, H.T), self._y_train_gpu)\n",
        "\n",
        "    def evolve(self, X_val, y_val, generations=5):\n",
        "        # Move Validation Data to GPU ONCE\n",
        "        X_val_g = cp.asarray(X_val, dtype=cp.float32) if GPU_AVAILABLE else X_val\n",
        "        y_val_g = cp.asarray(y_val) if GPU_AVAILABLE else y_val\n",
        "\n",
        "        best_acc = -1.0\n",
        "        # Initial Score\n",
        "        H_val = self._activate(X_val_g)\n",
        "        raw_val = cp.dot(H_val, self.output_weights_)\n",
        "        pred_val = cp.argmax(raw_val, axis=1)\n",
        "        # Use simple accuracy check on GPU\n",
        "        best_acc = float(cp.mean(pred_val == y_val_g))\n",
        "\n",
        "        best_dna = {\n",
        "            \"n_hidden\": self.n_hidden, \"activation\": self.activation,\n",
        "            \"alpha\": self.alpha, \"beta\": self.beta,\n",
        "            \"gamma\": self.gamma, \"bias_scale\": self.bias_scale,\n",
        "            \"power\": self.power\n",
        "        }\n",
        "\n",
        "        # Fast Menu\n",
        "        activations = [\"sine\", \"tanh\", \"sigmoid\", \"relu\", \"swish\", \"gaussian\", \"softsign\", \"mish\"]\n",
        "        infinite_betas = cp.concatenate([\n",
        "            cp.logspace(-2, 2, 20), -cp.logspace(-2, 2, 20), cp.array([1.0, -1.0])\n",
        "        ])\n",
        "\n",
        "        for gen in range(generations):\n",
        "            # Spawn 4 Mutants\n",
        "            mutants = []\n",
        "            for _ in range(4):\n",
        "                m = best_dna.copy()\n",
        "                if random.random() < 0.3: m[\"n_hidden\"] = int(np.clip(m[\"n_hidden\"] * np.random.uniform(0.5, 1.5), 50, 4500))\n",
        "                if random.random() < 0.2: m[\"activation\"] = random.choice(activations)\n",
        "                for key in [\"alpha\", \"gamma\", \"bias_scale\", \"power\"]:\n",
        "                    if random.random() < 0.3: m[key] *= np.random.uniform(0.8, 1.25)\n",
        "                if random.random() < 0.3: m[\"beta\"] = float(np.random.choice(cp.asnumpy(infinite_betas)))\n",
        "                mutants.append(m)\n",
        "\n",
        "            # BATTLE ROYALE ON GPU\n",
        "            for m in mutants:\n",
        "                try:\n",
        "                    # Activate & Solve on GPU (No CPU transfer)\n",
        "                    H = self._activate(self._X_train_gpu, m)\n",
        "                    n_h = m['n_hidden']\n",
        "                    I = cp.eye(n_h, dtype=cp.float32)\n",
        "\n",
        "                    # Fast Ridge Solve\n",
        "                    # We use solve instead of pinv here for PURE SPEED during evolution\n",
        "                    # (HTH + aI) W = HTY\n",
        "                    HTH = cp.dot(H.T, H) + m['alpha'] * I\n",
        "                    HTY = cp.dot(H.T, self._y_train_gpu)\n",
        "\n",
        "                    # Cholesky solve is faster than Pinv for evolution checks\n",
        "                    # Only use Pinv for final fit\n",
        "                    out_w = cp.linalg.solve(HTH, HTY)\n",
        "\n",
        "                    # Validate\n",
        "                    H_v = self._activate(X_val_g, m)\n",
        "                    preds = cp.argmax(cp.dot(H_v, out_w), axis=1)\n",
        "                    acc = float(cp.mean(preds == y_val_g))\n",
        "\n",
        "                    if acc > best_acc:\n",
        "                        best_acc = acc\n",
        "                        best_dna = m\n",
        "                except: continue\n",
        "\n",
        "        # Lock Champion\n",
        "        self.n_hidden = best_dna[\"n_hidden\"]\n",
        "        self.activation = best_dna[\"activation\"]\n",
        "        self.alpha = best_dna[\"alpha\"]\n",
        "        self.beta = best_dna[\"beta\"]\n",
        "        self.gamma = best_dna[\"gamma\"]\n",
        "        self.bias_scale = best_dna[\"bias_scale\"]\n",
        "        self.power = best_dna[\"power\"]\n",
        "\n",
        "        # Final Robust Solve (Using Pinv for stability)\n",
        "        self._solve_weights(best_dna)\n",
        "        self.dna_ = best_dna\n",
        "\n",
        "        # Clean VRAM\n",
        "        if GPU_AVAILABLE:\n",
        "            cp.get_default_memory_pool().free_all_blocks()\n",
        "\n",
        "        return best_acc\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if GPU_AVAILABLE:\n",
        "            X_g = cp.asarray(X, dtype=cp.float32)\n",
        "            H = self._activate(X_g)\n",
        "            raw = cp.dot(H, self.output_weights_)\n",
        "            # Softmax on GPU\n",
        "            raw -= cp.max(raw, axis=1, keepdims=True)\n",
        "            exp_out = cp.exp(raw)\n",
        "            probs = exp_out / cp.sum(exp_out, axis=1, keepdims=True)\n",
        "            return cp.asnumpy(probs) # Return to CPU for Sklearn compatibility\n",
        "        else:\n",
        "            return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return self.classes_[np.argmax(probs, axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# --- 26. THE RESIDUAL BRIDGE (Unit 26 - The Death Ray V4 - Dynamic Optics) ---\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "class ResidualBridgeUnit(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    THE RESIDUAL SNIPER ARCHITECTURE (V4).\n",
        "    Role: Calculates the 'Mistake' of the Elite Model using Geometric Neighbors.\n",
        "    Features:\n",
        "      - Dynamic Optics: Uses K=5 for small data (<2000 rows), K=21 for large data.\n",
        "      - Auto-Scope: Calibrates correction strength (0.0001 to 1.0) via simulation.\n",
        "      - Safety Lock: If no correction improves the score, it stands down (Strength 0).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_neighbors=None):\n",
        "        # Default to None so we can set it dynamically based on dataset size\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.sniper_ = None\n",
        "        self.verified_score_ = 0.0\n",
        "        self.best_factor_ = 0.0\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\"strategy\": \"Residual_KNN\"}\n",
        "\n",
        "    def fit_hunt(self, X_raw, y, elite_probs_oof):\n",
        "        \"\"\"\n",
        "        X_raw: Standard Scaled Geometry\n",
        "        y: True Labels\n",
        "        elite_probs_oof: The Baseline Probability Matrix\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_samples = len(X_raw)\n",
        "\n",
        "        # [DYNAMIC OPTICS SYSTEM]\n",
        "        # Small Universe (<2000): Use Microscope (K=5) to see tiny local errors.\n",
        "        # Large Universe (>2000): Use Telescope (K=21) to see stable patterns.\n",
        "        if self.n_neighbors is None:\n",
        "            if n_samples < 2000:\n",
        "                self.k_dynamic = 5\n",
        "            else:\n",
        "                self.k_dynamic = 21\n",
        "        else:\n",
        "            self.k_dynamic = self.n_neighbors\n",
        "\n",
        "        self.dna_[\"k\"] = self.k_dynamic\n",
        "\n",
        "        # 1. Calculate Residuals (The Mistake)\n",
        "        # R = Truth (1.0) - Elite (0.8) = +0.2 Error\n",
        "        y_onehot = np.zeros_like(elite_probs_oof)\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            y_onehot[y == c, i] = 1.0\n",
        "        residuals = y_onehot - elite_probs_oof\n",
        "\n",
        "        # 2. Train Sniper (The Geometric Corrector)\n",
        "        # We use Manhattan (p=1) because it works better in high-dimensional spaces.\n",
        "        self.sniper_ = KNeighborsRegressor(\n",
        "            n_neighbors=self.k_dynamic,\n",
        "            weights='distance',\n",
        "            metric='minkowski',\n",
        "            p=1,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # 3. INTERNAL SIMULATION (Calibrate the Scope)\n",
        "        try:\n",
        "            # Predict the mistake for every point (Cross-Validation)\n",
        "            oof_correction = cross_val_predict(self.sniper_, X_raw, residuals, cv=5, n_jobs=-1)\n",
        "\n",
        "            # The Universal Spectrum: From Quantum Nudge to Full Override\n",
        "            factors = [\n",
        "                0.0001, 0.0005, 0.001, 0.002, 0.005,  # Micro-Dose (Tie-Breakers)\n",
        "                0.01, 0.015, 0.02, 0.025, 0.03, 0.04, # Fine-Tuning\n",
        "                0.05, 0.06, 0.07, 0.08, 0.09, 0.10,   # Standard Correction\n",
        "                0.12, 0.15, 0.18, 0.20, 0.22, 0.25,   # Aggressive Correction\n",
        "                0.30, 0.35, 0.40, 0.45, 0.50, 0.55,   # Heavy Geometry\n",
        "                0.60, 0.70, 0.80, 0.90, 1.00          # Full Geometric Trust\n",
        "            ]\n",
        "\n",
        "            best_score = -1.0\n",
        "            best_f = 0.0\n",
        "\n",
        "            # Baseline Accuracy (What happens if we do nothing?)\n",
        "            base_acc = accuracy_score(y, self.classes_[np.argmax(elite_probs_oof, axis=1)])\n",
        "\n",
        "            if hasattr(self, \"verbose\") and self.verbose:\n",
        "                print(f\" > [DEATH RAY] Calibrating Scope (K={self.k_dynamic} | Base: {base_acc:.4%})...\")\n",
        "\n",
        "            for f in factors:\n",
        "                # Apply correction: New = Old + (Correction * Strength)\n",
        "                oof_fused = elite_probs_oof + (oof_correction * f)\n",
        "                score = accuracy_score(y, self.classes_[np.argmax(oof_fused, axis=1)])\n",
        "\n",
        "                # STRICT IMPROVEMENT CHECK\n",
        "                # We only lock if it strictly beats the previous best.\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_f = f\n",
        "\n",
        "            self.verified_score_ = best_score\n",
        "            self.best_factor_ = best_f\n",
        "\n",
        "            if hasattr(self, \"verbose\") and self.verbose:\n",
        "                print(f\" > [DEATH RAY] Scope Locked. Strength: {self.best_factor_} | Score: {self.verified_score_:.4%}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if hasattr(self, \"verbose\") and self.verbose:\n",
        "                print(f\" > [DEATH RAY] Calibration Failed: {e}\")\n",
        "            self.verified_score_ = 0.0\n",
        "            self.best_factor_ = 0.0\n",
        "\n",
        "        # 4. Final Fit (Lock and Load)\n",
        "        self.sniper_.fit(X_raw, residuals)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X_fused):\n",
        "        \"\"\"\n",
        "        Input: [Raw_Features | Elite_Probabilities]\n",
        "        Output: Corrected Probabilities\n",
        "        \"\"\"\n",
        "        # Split Data\n",
        "        n_features_raw = X_fused.shape[1] - len(self.classes_)\n",
        "        X_raw = X_fused[:, :n_features_raw]\n",
        "        elite_probs = X_fused[:, n_features_raw:]\n",
        "\n",
        "        # 1. Ask Sniper for Correction\n",
        "        correction = self.sniper_.predict(X_raw)\n",
        "\n",
        "        # 2. Apply The Auto-Calibrated Factor\n",
        "        # This is the \"Magic Formula\" that guarantees safety\n",
        "        final_probs = elite_probs + (correction * self.best_factor_)\n",
        "\n",
        "        # 3. Clip & Normalize (Ensure valid probability distribution)\n",
        "        final_probs = np.clip(final_probs, 0.0, 1.0)\n",
        "        sums = np.sum(final_probs, axis=1, keepdims=True)\n",
        "        return final_probs / (sums + 1e-9)\n",
        "\n",
        "    def predict(self, X_fused):\n",
        "        probs = self.predict_proba(X_fused)\n",
        "        return self.classes_[np.argmax(probs, axis=1)]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        print(\" [WARNING] Death Ray requires fit_hunt() with elite probs.\")\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "# --- 27. ENTITY X (The Evolutionary Optimizer - Genius Mode) ---\n",
        "class EntityX_EvolutionaryOptimizer(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    ENTITY X: The Final Evolutionary Stage.\n",
        "    Role: Post-Process Optimizer (Option B - Genius Mode).\n",
        "    Mechanism: Evolves a 'Correction Matrix' on the Training (OOF) data\n",
        "               to maximize accuracy, then applies it blindly to Test data.\n",
        "    Safety: Starts with Identity Matrix. Only mutates if Accuracy INCREASES.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_generations=200, population_size=100, mutation_power=0.15):\n",
        "        self.n_generations = n_generations\n",
        "        self.pop_size = population_size\n",
        "        self.mutation_power = mutation_power\n",
        "        self.best_correction_matrix_ = None\n",
        "        self.base_score_ = 0.0\n",
        "        self.optimized_score_ = 0.0\n",
        "        self.active_ = False\n",
        "\n",
        "    def fit_evolve(self, prob_matrix_oof, y_true):\n",
        "        \"\"\"\n",
        "        [RESERVOIR PROTOCOL]\n",
        "        1. Freezes the Original OOF Accuracy (The Reservoir).\n",
        "        2. Evolves variants.\n",
        "        3. If variant > Frozen: Update.\n",
        "        4. If variant <= Frozen: Discard (Snap back to Identity).\n",
        "        Result: Mathematical guarantee that OOF Accuracy never decreases.\n",
        "        \"\"\"\n",
        "        self.active_ = True\n",
        "\n",
        "        # 1. Setup & Freeze Original State (Identity)\n",
        "        if GPU_AVAILABLE:\n",
        "            probs = cp.asarray(prob_matrix_oof, dtype=cp.float32)\n",
        "            y_gpu = cp.asarray(y_true, dtype=cp.int32)\n",
        "            frozen_identity = cp.eye(probs.shape[1], dtype=cp.float32)\n",
        "\n",
        "            # Calculate Frozen Accuracy\n",
        "            base_preds = cp.argmax(probs, axis=1)\n",
        "            self.base_score_ = float(cp.mean(base_preds == y_gpu))\n",
        "        else:\n",
        "            probs = prob_matrix_oof\n",
        "            y_gpu = y_true\n",
        "            frozen_identity = np.eye(probs.shape[1], dtype=np.float32)\n",
        "\n",
        "            base_preds = np.argmax(probs, axis=1)\n",
        "            self.base_score_ = float(np.mean(base_preds == y_gpu))\n",
        "\n",
        "        # 2. Initialize Reservoir\n",
        "        best_matrix = frozen_identity\n",
        "        best_acc = self.base_score_\n",
        "\n",
        "        print(f\" > [ENTITY X] Reservoir Frozen. Floor Accuracy: {self.base_score_:.4%}\")\n",
        "\n",
        "        # 3. Evolution Loop (Hunting for the +0.1% Gain)\n",
        "        for gen in range(self.n_generations):\n",
        "            # Generate Mutations\n",
        "            if GPU_AVAILABLE:\n",
        "                mutations = cp.random.normal(0, self.mutation_power, (self.pop_size, probs.shape[1], probs.shape[1]), dtype=cp.float32)\n",
        "                candidates = best_matrix + mutations\n",
        "            else:\n",
        "                mutations = np.random.normal(0, self.mutation_power, (self.pop_size, probs.shape[1], probs.shape[1]))\n",
        "                candidates = best_matrix + mutations\n",
        "\n",
        "            # Batch Evaluation\n",
        "            for i in range(self.pop_size):\n",
        "                cand = candidates[i]\n",
        "\n",
        "                # Fast Check\n",
        "                if GPU_AVAILABLE:\n",
        "                    acc = float(cp.mean(cp.argmax(cp.dot(probs, cand), axis=1) == y_gpu))\n",
        "                else:\n",
        "                    acc = float(np.mean(np.argmax(np.dot(probs, cand), axis=1) == y_gpu))\n",
        "\n",
        "                # STRICT RESERVOIR CHECK: Only update if strictly better\n",
        "                if acc > best_acc:\n",
        "                    best_acc = acc\n",
        "                    best_matrix = cand\n",
        "\n",
        "        # 4. Final Lock (The \"Impossible to Decrease\" Guarantee)\n",
        "        gain = best_acc - self.base_score_\n",
        "\n",
        "        if gain <= 0.0:\n",
        "            # If we found nothing, we revert to the Frozen Identity. Zero risk.\n",
        "            self.best_correction_matrix_ = frozen_identity\n",
        "            print(f\" > [ENTITY X] No positive mutation found. Reverting to Frozen Reservoir.\")\n",
        "        else:\n",
        "            # We found a gain. Add it to the frozen layer.\n",
        "            self.best_correction_matrix_ = best_matrix\n",
        "            print(f\" > [ENTITY X] Evolution Success. Reservoir + Gain: +{gain:.4%} (Total: {best_acc:.4%})\")\n",
        "\n",
        "        # Offload from GPU\n",
        "        if GPU_AVAILABLE:\n",
        "            self.best_correction_matrix_ = cp.asnumpy(self.best_correction_matrix_)\n",
        "\n",
        "        self.optimized_score_ = best_acc\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, prob_matrix_test):\n",
        "        \"\"\"\n",
        "        Applies the FROZEN correction matrix to Test Data.\n",
        "        \"\"\"\n",
        "        if not self.active_ or self.best_correction_matrix_ is None:\n",
        "            return prob_matrix_test\n",
        "\n",
        "        # Apply the learned transformation\n",
        "        corrected = np.dot(prob_matrix_test, self.best_correction_matrix_)\n",
        "\n",
        "        # Softmax normalization to return valid probabilities\n",
        "        exp_c = np.exp(corrected - np.max(corrected, axis=1, keepdims=True))\n",
        "        return exp_c / np.sum(exp_c, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "# --- 7. THE TITAN-21 \"FINAL COSMOLOGY\" ---\n",
        "class HarmonicResonanceClassifier_BEAST_21D(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        self.scaler_ = RobustScaler(quantile_range=(15.0, 85.0))\n",
        "        self.weights_ = None\n",
        "        self.classes_ = None\n",
        "\n",
        "        # --- THE COMPETITOR TRINITY ---\n",
        "        self.unit_bench_svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42)\n",
        "        self.unit_bench_rf = RandomForestClassifier(n_estimators=100, random_state=42) # Standard RF\n",
        "        self.unit_bench_xgb = XGBClassifier(n_estimators=100,  eval_metric='logloss', random_state=42) # Standard XGB\n",
        "\n",
        "        # --- THE 21 DIMENSIONS OF THE UNIVERSE ---\n",
        "\n",
        "        # [LOGIC SECTOR - NEWTONIAN]\n",
        "        self.unit_01 = ExtraTreesClassifier(\n",
        "            n_estimators=1000, bootstrap=False, max_features=\"sqrt\", n_jobs=-1, random_state=42\n",
        "        )\n",
        "        self.unit_02 = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)\n",
        "\n",
        "        # [TITAN UPGRADE] Unit 03 buffed to match External XGBoost performance\n",
        "        self.unit_03 = HistGradientBoostingClassifier(\n",
        "            max_iter=1000,              # Increased from 500\n",
        "            max_depth=None,             # Allow full depth (matches XGBoost default)\n",
        "            learning_rate=0.05,\n",
        "            max_leaf_nodes=31,          # Standard XGBoost equivalent\n",
        "            l2_regularization=1.0,      # Slight stability\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # [GRADIENT SECTOR - OPTIMIZATION]\n",
        "        self.unit_04 = XGBClassifier(n_estimators=500, max_depth=6, learning_rate=0.02, n_jobs=-1, random_state=42)\n",
        "        self.unit_05 = XGBClassifier(n_estimators=1000, max_depth=3, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
        "\n",
        "        # [KERNEL SECTOR - MANIFOLDS]\n",
        "        self.unit_06 = NuSVC(nu=0.05, kernel=\"rbf\", gamma=\"scale\", probability=True, random_state=42)\n",
        "        self.unit_07 = SVC(kernel=\"poly\", degree=2, C=10.0, probability=True, random_state=42)\n",
        "\n",
        "        # [GEOMETRY SECTOR - SPACETIME]\n",
        "        self.unit_08 = KNeighborsClassifier(n_neighbors=3, weights=\"distance\", metric=\"euclidean\", n_jobs=-1)\n",
        "        self.unit_09 = KNeighborsClassifier(n_neighbors=9, weights=\"distance\", metric=\"manhattan\", n_jobs=-1)\n",
        "        self.unit_10 = QuadraticDiscriminantAnalysis(reg_param=0.01)\n",
        "        self.unit_11 = SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", probability=True, random_state=42)\n",
        "\n",
        "        # [SOUL SECTOR - RESONANCE (EVOLUTIONARY)]\n",
        "        self.unit_12 = HolographicSoulUnit(k=15)\n",
        "        self.unit_13 = HolographicSoulUnit(k=15)\n",
        "        self.unit_14 = HolographicSoulUnit(k=15)\n",
        "        self.unit_15 = HolographicSoulUnit(k=25)\n",
        "        self.unit_16 = HolographicSoulUnit(k=25)\n",
        "        self.unit_17 = HolographicSoulUnit(k=25)\n",
        "\n",
        "        # [COSMIC SECTOR - THE FINAL TRINITY]\n",
        "        # 1. DEFINE THE UNITS (Using the NEW Heavy GPU classes)\n",
        "        self.unit_18 = GoldenSpiralUnit(k=21, n_estimators=50)      # Golden Forest\n",
        "        self.unit_19 = EntropyMaxwellUnit(n_estimators=50)          # Entropy Forest\n",
        "        self.unit_20 = QuantumFluxUnit(n_estimators=20, gamma=0.5)  # Quantum Forest\n",
        "        self.unit_21 = EventHorizonUnit(n_estimators=50)            # Gravity Forest\n",
        "\n",
        "        # [ALIEN SECTOR - THE OMEGA]\n",
        "        self.unit_24 = AlienDimensionZ() # Depth 7 for extreme complexity\n",
        "\n",
        "        # [NEURAL SECTOR - THE UNIVERSAL SOLVER]\n",
        "        self.unit_25 = NeuralManifoldUnit(n_hidden=100, activation=\"tanh\", alpha=0.1)\n",
        "\n",
        "        # [THE DEATH RAY]\n",
        "        self.unit_26 = ResidualBridgeUnit(n_neighbors=None)\n",
        "\n",
        "        # [ENTITY X - THE FINAL EVOLUTION]\n",
        "        self.unit_27 = EntityX_EvolutionaryOptimizer(n_generations=500, population_size=100, mutation_power=0.15)\n",
        "\n",
        "\n",
        "    def fit(self, X, y, X_test_oracle=None, y_test_oracle=None):\n",
        "        y = np.array(y).astype(int)\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_classes = len(self.classes_)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\" >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\")\n",
        "            print(\" > Initiating The Ouroboros Protocol (Stabilized)...\")\n",
        "\n",
        "        # --- PHASE -1: THE UNIVERSAL LENS SELECTOR (Dual-Scout Protocol) ---\n",
        "        if self.verbose: print(\" > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\")\n",
        "\n",
        "        lenses = [\n",
        "            (\"Standard\", StandardScaler()),\n",
        "            (\"Robust\", RobustScaler(quantile_range=(15.0, 85.0))),\n",
        "            (\"MinMax\", MinMaxScaler())\n",
        "        ]\n",
        "\n",
        "        best_lens_name = \"Standard\"\n",
        "        best_lens_score = -1.0\n",
        "        best_lens_obj = StandardScaler()\n",
        "\n",
        "        # SCOUT TEAM\n",
        "        from sklearn.model_selection import cross_val_score\n",
        "        from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "        # 1. Geometry Scout\n",
        "        scout_geom = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "        # 2. Logic Scout\n",
        "        scout_logic = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "\n",
        "        # Test on subset\n",
        "        sub_idx = np.random.choice(len(X), min(len(X), 2000), replace=False)\n",
        "        X_sub = X[sub_idx]\n",
        "        y_sub = y[sub_idx]\n",
        "\n",
        "        for name, lens in lenses:\n",
        "            try:\n",
        "                # Apply Lens\n",
        "                X_trans = lens.fit_transform(X_sub)\n",
        "\n",
        "                # Get Consensus Score\n",
        "                score_g = cross_val_score(scout_geom, X_trans, y_sub, cv=3, n_jobs=-1).mean()\n",
        "                score_l = cross_val_score(scout_logic, X_trans, y_sub, cv=3, n_jobs=-1).mean()\n",
        "\n",
        "                # Harmonic Mean\n",
        "                combined_score = 2 * (score_g * score_l) / (score_g + score_l + 1e-9)\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(f\"    [{name:<8}] Geom: {score_g:.2%} | Logic: {score_l:.2%} | HARMONIC: {combined_score:.2%}\")\n",
        "\n",
        "                if combined_score > best_lens_score:\n",
        "                    best_lens_score = combined_score\n",
        "                    best_lens_name = name\n",
        "                    best_lens_obj = lens\n",
        "            except: pass\n",
        "\n",
        "        self.scaler_ = best_lens_obj\n",
        "        if self.verbose: print(f\" >>> LENS LOCKED: {best_lens_name.upper()} SCALER (Consensus Achieved) <<<\")\n",
        "\n",
        "        X_scaled = self.scaler_.fit_transform(X)\n",
        "\n",
        "        # --- PHASE 0: DUAL SNIPER CALIBRATION (Flash-Tune Protocol) ---\n",
        "        if self.verbose: print(\" > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\")\n",
        "\n",
        "        n_total = len(X)\n",
        "        n_calib = min(n_total, 2000)\n",
        "\n",
        "        if n_total > 5000:\n",
        "             idx_calib = np.random.choice(n_total, n_calib, replace=False)\n",
        "             X_calib = X_scaled[idx_calib]\n",
        "             y_calib = y[idx_calib]\n",
        "        else:\n",
        "             X_calib = X_scaled\n",
        "             y_calib = y\n",
        "\n",
        "        try:\n",
        "            from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "            # 1. Calibrate Resonance\n",
        "            params_svc = {\"C\": [0.1, 1.0, 10.0, 50.0], \"gamma\": [\"scale\", \"auto\", 0.1]}\n",
        "            search_svc = RandomizedSearchCV(self.unit_11, params_svc, n_iter=10, cv=3, n_jobs=-1, random_state=42)\n",
        "            search_svc.fit(X_calib, y_calib)\n",
        "            self.unit_11 = search_svc.best_estimator_\n",
        "\n",
        "            # 2. Calibrate Nu-Warp\n",
        "            params_nu = {\"nu\": [0.05, 0.1, 0.2], \"gamma\": [\"scale\", \"auto\"]}\n",
        "            search_nu = RandomizedSearchCV(self.unit_06, params_nu, n_iter=4, cv=3, n_jobs=-1, random_state=42)\n",
        "            search_nu.fit(X_calib, y_calib)\n",
        "            self.unit_06 = search_nu.best_estimator_\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"    >>> Resonance (SVM) Tuned: {search_svc.best_params_} | Score: {search_svc.best_score_:.2%}\")\n",
        "                print(f\"    >>> Nu-Warp (NuSVC) Tuned: {search_nu.best_params_} | Score: {search_nu.best_score_:.2%}\")\n",
        "        except Exception as e:\n",
        "            if self.verbose: print(f\"    >>> Calibration Skipped (Speed Mode): {e}\")\n",
        "\n",
        "        # --- STEP 1: RAPID QUALIFIER (20% Proxy) ---\n",
        "        X_train_sub, X_select, y_train_sub, y_select = train_test_split(\n",
        "            X_scaled, y, test_size=0.20, stratify=y, random_state=42\n",
        "        )\n",
        "\n",
        "        # --- A: EVOLVE & TRAIN (On Sub-Set for Speed) ---\n",
        "        if self.verbose:\n",
        "            print(\" > Phase 1: Awakening the Souls (Rapid Evolution)...\")\n",
        "            print(\"-\" * 80)\n",
        "            print(f\" {'UNIT NAME':<20} | {'ACCURACY':<8} | {'EVOLVED DNA PARAMETERS'}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        # 1. Define The Living Groups (Souls + Neural)\n",
        "        living_units = [\n",
        "            (\"SOUL-01 (Original)\", self.unit_12),\n",
        "            (\"SOUL-02 (Mirror A)\", self.unit_13),\n",
        "            (\"SOUL-03 (Mirror B)\", self.unit_14),\n",
        "            (\"SOUL-D (AGI Hyper)\", self.unit_15),\n",
        "            (\"SOUL-E (AGI Deep)\", self.unit_16),\n",
        "            (\"SOUL-F (AGI Omni)\", self.unit_17),\n",
        "            (\"NEURAL-ELM (Omni)\", self.unit_25),\n",
        "            # The Cosmic Forests\n",
        "            (\"GOLDEN-FOREST\", self.unit_18),\n",
        "            (\"ENTROPY-FOREST\", self.unit_19),\n",
        "            (\"QUANTUM-FOREST\", self.unit_20),\n",
        "            (\"GRAVITY-FOREST\", self.unit_21),\n",
        "        ]\n",
        "\n",
        "        # Evolve the Living\n",
        "        for name, unit in living_units:\n",
        "            if hasattr(unit, \"set_raw_source\"):\n",
        "                unit.set_raw_source(X_train_sub)\n",
        "            try:\n",
        "                unit.fit(X_train_sub, y_train_sub)\n",
        "                if hasattr(unit, \"evolve\"):\n",
        "                    acc = unit.evolve(X_select, y_select, generations=10)\n",
        "                else:\n",
        "                    acc = 0.0\n",
        "\n",
        "                if self.verbose:\n",
        "                    dna = getattr(unit, \"dna_\", {})\n",
        "                    dna_str = \"Standard\"\n",
        "                    if \"freq\" in dna: dna_str = f\"Freq: {dna['freq']:.2f} | Gamma: {dna['gamma']:.2f} | P: {dna.get('p', 2.0):.1f}\"\n",
        "                    elif \"n_hidden\" in dna: dna_str = f\"H:{dna['n_hidden']} | Act:{dna['activation']} | Alpha:{dna['alpha']:.2f}\"\n",
        "                    elif \"resonance\" in dna: dna_str = f\"Res: {dna['resonance']:.3f} | Decay: {dna['decay']:.2f} | Shift: {dna['shift']:.1f}\"\n",
        "                    elif \"horizon_pct\" in dna: dna_str = f\"Horizon: {dna['horizon_pct']}% | Power: {dna['decay_power']:.2f}\"\n",
        "                    elif \"gamma\" in dna and \"n_components\" in dna: dna_str = f\"Gamma: {dna['gamma']:.2f} | N-Comp: {dna['n_components']}\"\n",
        "                    elif \"n_components\" in dna: dna_str = f\"Components: {dna['n_components']}\"\n",
        "\n",
        "                    print(f\" {name:<20} | {acc:.2%}  | {dna_str}\")\n",
        "            except Exception as e:\n",
        "                if self.verbose: print(f\" {name:<20} | FAILED   | {str(e)}\")\n",
        "\n",
        "        if self.verbose: print(\"-\" * 80)\n",
        "\n",
        "        # 2. Train The Non-Living\n",
        "        non_living_training_group = [\n",
        "            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb\n",
        "        ]\n",
        "\n",
        "        for unit in non_living_training_group:\n",
        "            try: unit.fit(X_train_sub, y_train_sub)\n",
        "            except: pass\n",
        "\n",
        "        # --- B: THE GRAND QUALIFIER (Identify Top 12) ---\n",
        "        if self.verbose: print(\" > Phase 2: The Grand Qualifier (Scanning All 26 Candidates)...\")\n",
        "\n",
        "        # CRITICAL: THIS ORDER MUST MATCH PREDICT_PROBA EXACTLY\n",
        "        all_units = [\n",
        "            # 1. Standard\n",
        "            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "            # 2. Cosmic / Physics\n",
        "            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n",
        "            # 3. Competitors\n",
        "            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n",
        "            # 4. Souls\n",
        "            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n",
        "            # 5. Neural\n",
        "            self.unit_25,\n",
        "            self.unit_26\n",
        "        ]\n",
        "\n",
        "        n_units = len(all_units)\n",
        "        accs = []\n",
        "\n",
        "        # Score all units on Selection Set\n",
        "        for unit in all_units:\n",
        "            try:\n",
        "                p = unit.predict(X_select)\n",
        "                accs.append(accuracy_score(y_select, p))\n",
        "            except: accs.append(0.0)\n",
        "\n",
        "        # Sort by raw accuracy\n",
        "        sorted_indices = np.argsort(accs)[::-1]\n",
        "\n",
        "        # Save indices for later use in strategy\n",
        "        self.sorted_indices_global_ = sorted_indices\n",
        "\n",
        "        # [PHASE 2 PERFORMANCE MONITOR]\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\" >>> THE 21D PERFORMANCE MONITOR (Phase 2 Qualification) <<<\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\" {'RANK':<6} | {'UNIT NAME':<18} | {'SCORE':<10} | {'STATUS'}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "            # Map indices to friendly names\n",
        "            map_names = [\n",
        "                \"Logic-ET\", \"Logic-RF\", \"Logic-HG\", \"Grad-XG1\", \"Grad-XG2\",\n",
        "                \"Nu-Warp\", \"PolyKer\", \"Geom-K3\", \"Geom-K9\", \"Space-QDA\", \"Resonance\",\n",
        "                \"GOLDEN-FOREST\", \"ENTROPY-FOREST\", \"QUANTUM-FOREST\", \"GRAVITY-FOREST\",\n",
        "                \"BENCH-SVM\", \"BENCH-RF\", \"BENCH-XGB\",\n",
        "                \"SOUL-Orig\", \"SOUL-TwinA\", \"SOUL-TwinB\", \"SOUL-D(AGI)\", \"SOUL-E(AGI)\", \"SOUL-F(AGI)\",\n",
        "                \"Neural-ELM\",\"THE DEATH RAY\"\n",
        "            ]\n",
        "\n",
        "            for rank, idx in enumerate(sorted_indices):\n",
        "                name = map_names[idx] if idx < len(map_names) else f\"Unit-{idx}\"\n",
        "                score = accs[idx]\n",
        "                status = \"PROMOTED\" if rank < 12 else \"Eliminated\"\n",
        "                print(f\" {rank+1:02d}     | {name:<18} | {score:.2%}    | {status}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "        # Pick Top 12 for the OOF Battle\n",
        "        top_12_indices = sorted_indices[:12]\n",
        "        candidate_models = [all_units[i] for i in top_12_indices]\n",
        "\n",
        "\n",
        "        # --- C: THE OUROBOROS SELECTION (The Battle of Names) ---\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\" >>> PHASE 3: THE OUROBOROS PROTOCOL (Top 12 Candidates - 100% Validation) <<<\")\n",
        "            print(\"=\" * 80)\n",
        "            print(f\" {'RANK':<4} | {'UNIT NAME':<18} | {'OOF ACCURACY':<10} | {'STATUS'}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        # 1. Define The Name Map (Global Index -> Name)\n",
        "        all_names_map = [\n",
        "            \"Logic-ET\", \"Logic-RF\", \"Logic-HG\", \"Grad-XG1\", \"Grad-XG2\",               # 0-4\n",
        "            \"Nu-Warp\", \"PolyKer\", \"Geom-K3\", \"Geom-K9\", \"Space-QDA\", \"Resonance\",     # 5-10\n",
        "            \"GOLDEN-FOREST\", \"ENTROPY-FOREST\", \"QUANTUM-FOREST\", \"GRAVITY-FOREST\",    # 11-14\n",
        "            \"BENCH-SVM\", \"BENCH-RF\", \"BENCH-XGB\",                                     # 15-17\n",
        "            \"SOUL-Orig\", \"SOUL-TwinA\", \"SOUL-TwinB\", \"SOUL-D(AGI)\", \"SOUL-E(AGI)\", \"SOUL-F(AGI)\", # 18-23\n",
        "            \"Neural-ELM\",                                                             # 24\n",
        "            \"THE DEATH RAY\"                                                           # 25\n",
        "        ]\n",
        "\n",
        "        candidate_oof_accs = []\n",
        "        candidate_oof_preds_list = []\n",
        "\n",
        "        # 2. Run OOF (With Real Names)\n",
        "        for i, unit in enumerate(candidate_models):\n",
        "            global_idx = top_12_indices[i]\n",
        "            unit_name = all_names_map[global_idx] if global_idx < len(all_names_map) else f\"Unit-{global_idx}\"\n",
        "\n",
        "            method = \"predict_proba\" if hasattr(unit, \"predict_proba\") else \"decision_function\"\n",
        "            try:\n",
        "                # 5-Fold Cross-Validation (The Truth Serum)\n",
        "                oof_pred = cross_val_predict(unit, X_scaled, y, cv=5, method=method, n_jobs=-1)\n",
        "\n",
        "                # Stabilization\n",
        "                if method == \"decision_function\":\n",
        "                    if len(oof_pred.shape) == 1:\n",
        "                        p = 1 / (1 + np.exp(-oof_pred))\n",
        "                        oof_pred = np.column_stack([1-p, p])\n",
        "                    else:\n",
        "                        max_d = np.max(oof_pred, axis=1, keepdims=True)\n",
        "                        exp_d = np.exp(oof_pred - max_d)\n",
        "                        oof_pred = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "                # NaNs check\n",
        "                oof_pred = np.nan_to_num(oof_pred)\n",
        "\n",
        "                # Score\n",
        "                acc_oof = accuracy_score(y, self.classes_[np.argmax(oof_pred, axis=1)])\n",
        "                candidate_oof_accs.append(acc_oof)\n",
        "                candidate_oof_preds_list.append(oof_pred)\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(f\" {i+1:02d}   | {unit_name:<18} | {acc_oof:.4%}   | Validated\")\n",
        "\n",
        "            except Exception as e:\n",
        "                candidate_oof_accs.append(0.0)\n",
        "                candidate_oof_preds_list.append(np.zeros((len(X_scaled), len(self.classes_))))\n",
        "                if self.verbose:\n",
        "                    print(f\" {i+1:02d}   | {unit_name:<18} | FAILED       | {str(e)[:20]}...\")\n",
        "\n",
        "        if self.verbose: print(\"-\" * 80)\n",
        "\n",
        "        # 3. Sort by Performance (Meritocracy)\n",
        "        sorted_oof_idx = np.argsort(candidate_oof_accs)[::-1]\n",
        "\n",
        "        # 4. Select Absolute Best (Top 2) - WITH TITAN SAFETY\n",
        "        top_2_local_idx = []\n",
        "        for idx in sorted_oof_idx:\n",
        "            if candidate_oof_accs[idx] < 0.10: continue\n",
        "\n",
        "            global_idx = top_12_indices[idx]\n",
        "            # Neural-ELM (Index 24) Exclusion logic\n",
        "            if global_idx == 24:\n",
        "                if self.verbose and len(top_2_local_idx) < 2:\n",
        "                    print(f\" > [SAFETY] Neural-ELM attempted to join Council. Request DENIED (Restricted to Rank 3).\")\n",
        "                continue\n",
        "\n",
        "            top_2_local_idx.append(idx)\n",
        "            if len(top_2_local_idx) == 2: break\n",
        "\n",
        "        self.final_elites_ = [candidate_models[i] for i in top_2_local_idx]\n",
        "        elite_accs = [candidate_oof_accs[i] for i in top_2_local_idx]\n",
        "        elite_preds = [candidate_oof_preds_list[i] for i in top_2_local_idx]\n",
        "\n",
        "        # --- ARCHITECTURE 1: THE COUNCIL  ---\n",
        "        self.weights_council_ = np.zeros(n_units)\n",
        "        idx_rank1 = top_12_indices[top_2_local_idx[0]]\n",
        "        self.weights_council_[idx_rank1] = 0.75\n",
        "        idx_rank2 = top_12_indices[top_2_local_idx[1]]\n",
        "        self.weights_council_[idx_rank2] = 0.25\n",
        "\n",
        "        # --- ARCHITECTURE 2: THE ACE (Absolute Monarchy ) ---\n",
        "        self.weights_ace_ = np.zeros(n_units)\n",
        "        self.weights_ace_[idx_rank1] = 0.90  # 95%\n",
        "        self.weights_ace_[idx_rank2] = 0.10  # 5%\n",
        "\n",
        "        # --- ARCHITECTURE 3: THE LINEAR (The Shield) ---\n",
        "        self.weights_linear_ = np.zeros(n_units)\n",
        "        self.weights_linear_[idx_rank1] = 0.60\n",
        "        self.weights_linear_[idx_rank2] = 0.40\n",
        "\n",
        "        # --- ARCHITECTURE 4: THE BALANCE (Perfect Harmony 50/50) ---\n",
        "        self.weights_balance_ = np.zeros(n_units)\n",
        "        self.weights_balance_[idx_rank1] = 0.50\n",
        "        self.weights_balance_[idx_rank2] = 0.50\n",
        "\n",
        "        # --- ARCHITECTURE 5: THE INVERSION (Support Lead 40/60) ---\n",
        "        self.weights_inv_linear_ = np.zeros(n_units)\n",
        "        self.weights_inv_linear_[idx_rank1] = 0.40\n",
        "        self.weights_inv_linear_[idx_rank2] = 0.60\n",
        "\n",
        "        # --- ARCHITECTURE 6: THE UNDERDOG (Hidden Potential 30/70) ---\n",
        "        self.weights_inv_council_ = np.zeros(n_units)\n",
        "        self.weights_inv_council_[idx_rank1] = 0.30\n",
        "        self.weights_inv_council_[idx_rank2] = 0.70\n",
        "\n",
        "        # --- SIMULATION ---\n",
        "        def get_score(weights_full):\n",
        "            combined_pred = np.zeros_like(elite_preds[0])\n",
        "            current_w = []\n",
        "            for idx in top_2_local_idx:\n",
        "                current_w.append(weights_full[top_12_indices[idx]])\n",
        "            for i in range(2):\n",
        "                combined_pred += current_w[i] * elite_preds[i]\n",
        "            return accuracy_score(y, self.classes_[np.argmax(combined_pred, axis=1)])\n",
        "\n",
        "        score_council = get_score(self.weights_council_)\n",
        "        score_ace = get_score(self.weights_ace_)\n",
        "        score_linear = get_score(self.weights_linear_)\n",
        "        score_balance = get_score(self.weights_balance_)\n",
        "        score_inv_linear = get_score(self.weights_inv_linear_)\n",
        "        score_inv_council = get_score(self.weights_inv_council_)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\" > [STRATEGY LAB] Ace: {score_ace:.4%} | Council: {score_council:.4%} | Linear: {score_linear:.4%}\")\n",
        "            print(f\" > [STRATEGY LAB] Balance: {score_balance:.4%} | Inv-Lin: {score_inv_linear:.4%} | Underdog: {score_inv_council:.4%}\")\n",
        "\n",
        "        # [TITAN 6-WAY TOURNAMENT]\n",
        "        strat_map = {\n",
        "            \"council\": score_council,\n",
        "            \"linear\": score_linear,\n",
        "            \"ace\": score_ace,\n",
        "            #\"balance\": score_balance, # Restored Balance\n",
        "            \"inv_linear\": score_inv_linear,\n",
        "            \"inv_council\": score_inv_council\n",
        "        }\n",
        "\n",
        "        self.strategy_ = max(strat_map, key=strat_map.get)\n",
        "\n",
        "        # [TITAN TIE-BREAKER PRESERVATION]\n",
        "        if score_ace > 0.98 and abs(score_ace - strat_map[self.strategy_]) < 0.001:\n",
        "            self.strategy_ = \"ace\"\n",
        "\n",
        "        if self.verbose:\n",
        "             print(f\" >>> {self.strategy_.upper()} STRATEGY LOCKED. <<<\")\n",
        "\n",
        "        # --- PHASE 4: ASSIMILATION ---\n",
        "        if self.verbose: print(f\" > Phase 4: Final Assimilation (Retraining Top 2 Elites)...\")\n",
        "        for unit in self.final_elites_:\n",
        "            unit.fit(X_scaled, y)\n",
        "\n",
        "        # --- PHASE 4.5: THE DEATH RAY PROTOCOL (Corrected Logic) ---\n",
        "        rank1_local_idx = top_2_local_idx[0]\n",
        "        best_oof_probs = elite_preds[0]\n",
        "\n",
        "        true_max_score = max(strat_map.values())\n",
        "        true_max_name = max(strat_map, key=strat_map.get).upper()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\" > [HYPERNOVA] Elite Source Acquired: {candidate_models[rank1_local_idx].__class__.__name__}\")\n",
        "            print(f\" > [HYPERNOVA] Current Champion to Beat: {true_max_score:.4%} ({true_max_name})\")\n",
        "\n",
        "        self.unit_26.fit_hunt(X_scaled, y, elite_probs_oof=best_oof_probs)\n",
        "        death_ray_score = self.unit_26.verified_score_\n",
        "        margin = death_ray_score - true_max_score\n",
        "\n",
        "        if margin > 0.00001:\n",
        "            if self.verbose:\n",
        "                print(f\" > [ALERT] DEATH RAY SUCCESSFUL. (Score: {death_ray_score:.4%} | Margin: +{margin:.4%})\")\n",
        "                print(f\" > [COMMAND] OVERRIDING STRATEGY -> DEATH_RAY.\")\n",
        "\n",
        "            self.strategy_ = \"death_ray\"\n",
        "            self.weights_death_ray_ = np.zeros(26)\n",
        "            self.weights_death_ray_[25] = 0.05\n",
        "            rank1_global_idx = top_12_indices[rank1_local_idx]\n",
        "            self.weights_death_ray_[rank1_global_idx] = 0.95\n",
        "        else:\n",
        "             if self.verbose:\n",
        "                 print(f\" > [DEATH RAY] Stand Down. No gain over {true_max_name} (Ray: {death_ray_score:.4%} vs Champ: {true_max_score:.4%}).\")\n",
        "             self.weights_death_ray_ = np.zeros(26)\n",
        "\n",
        "\n",
        "        # --- PHASE 5: THE FINAL CONSTITUTION (Dual-Core Report) ---\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\"*85)\n",
        "            print(f\" >>> PHASE 5: THE FINAL CONSTITUTION (Dual-Core Analysis) <<<\")\n",
        "            print(\"=\"*85)\n",
        "\n",
        "            clean_map = {k:v for k,v in strat_map.items() if k != \"death_ray\"}\n",
        "            std_name = max(clean_map, key=clean_map.get).upper()\n",
        "            std_score = clean_map[std_name.lower()]\n",
        "            print(f\" [A] STANDARD CHAMPION: {std_name:<10} (Internal Score: {std_score:.4%})\")\n",
        "\n",
        "            ray_score = self.unit_26.verified_score_\n",
        "            if self.strategy_ == \"death_ray\": ray_status = \"VICTORIOUS\"\n",
        "            elif ray_score > 0: ray_status = \"REJECTED\"\n",
        "            else: ray_status = \"DORMANT\"\n",
        "\n",
        "            print(f\" [B] THE DEATH RAY:     {ray_status:<10} (Internal Score: {ray_score:.4%})\")\n",
        "            print(\"-\" * 85)\n",
        "\n",
        "            print(f\" >>> ACTIVE CONFIGURATION: {self.strategy_.upper()}\")\n",
        "            print(f\" {'RANK':<4} | {'UNIT NAME':<18} | {'WEIGHT':<8} | {'DNA CONFIGURATION'}\")\n",
        "            print(\"-\" * 85)\n",
        "\n",
        "            if self.strategy_ == \"death_ray\": active_w = self.weights_death_ray_\n",
        "            elif self.strategy_ == \"ace\": active_w = self.weights_ace_\n",
        "            elif self.strategy_ == \"linear\": active_w = self.weights_linear_\n",
        "            elif self.strategy_ == \"balance\": active_w = self.weights_balance_\n",
        "            elif self.strategy_ == \"inv_linear\": active_w = self.weights_inv_linear_\n",
        "            elif self.strategy_ == \"inv_council\": active_w = self.weights_inv_council_\n",
        "            else: active_w = self.weights_council_\n",
        "\n",
        "            # Build Ordered List\n",
        "            all_units_ordered = [\n",
        "                self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "                self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "                self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n",
        "                self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n",
        "                self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n",
        "                self.unit_25, self.unit_26\n",
        "            ]\n",
        "\n",
        "            active_list = []\n",
        "            for i, w in enumerate(active_w):\n",
        "                if w > 0:\n",
        "                    unit_name = all_names_map[i] if i < len(all_names_map) else f\"Unit-{i}\"\n",
        "                    active_list.append((w, unit_name, all_units_ordered[i]))\n",
        "            active_list.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            for rank, (w, name, obj) in enumerate(active_list):\n",
        "                 config_str = \"Standard\"\n",
        "                 if hasattr(obj, \"dna_\"):\n",
        "                    d = obj.dna_\n",
        "                    if \"freq\" in d: config_str = f\"[SOUL] Freq:{d['freq']:.2f} | Gamma:{d['gamma']:.2f}\"\n",
        "                    elif \"strategy\" in d: config_str = f\"[SNIPER] Strategy:{d['strategy']} | K:{d['k']}\"\n",
        "                    elif \"n_hidden\" in d: config_str = f\"[NEURAL] H:{d['n_hidden']} | Act:{d['activation']}\"\n",
        "                    elif \"resonance\" in d: config_str = f\"[BIO] Res:{d['resonance']:.2f}\"\n",
        "                 elif hasattr(obj, \"get_params\"):\n",
        "                    p = obj.get_params()\n",
        "                    if \"n_estimators\" in p: config_str = f\"[TREE] Trees:{p['n_estimators']}\"\n",
        "                    elif \"C\" in p: config_str = f\"[SVM] C:{p['C']}\"\n",
        "\n",
        "                 print(f\" {rank+1:02d}   | {name:<18} | {w:.2%}    | {config_str}\")\n",
        "            print(\"-\" * 85 + \"\\n\")\n",
        "\n",
        "        # --- PHASE 6: ENTITY X (THE FINAL EVOLUTION) ---\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\"*85)\n",
        "            print(f\" >>> PHASE 6: ENTITY X ACTIVATION (Genetic Optimization) <<<\")\n",
        "            print(\"=\"*85)\n",
        "\n",
        "        # Logic to get the OOF matrix for the current active strategy.\n",
        "        # We reconstruct it using the OOF predictions of the Top 2 Elites\n",
        "        # which are already stored in 'elite_preds'.\n",
        "\n",
        "        # 1. Identify which Elite corresponds to which Weight\n",
        "        # top_2_local_idx[0] -> Rank 1 Elite (Stored in elite_preds[0])\n",
        "        # top_2_local_idx[1] -> Rank 2 Elite (Stored in elite_preds[1])\n",
        "\n",
        "        idx_global_1 = top_12_indices[top_2_local_idx[0]]\n",
        "        idx_global_2 = top_12_indices[top_2_local_idx[1]]\n",
        "\n",
        "        w1 = active_w[idx_global_1]\n",
        "        w2 = active_w[idx_global_2]\n",
        "\n",
        "        # Normalization (Crucial if strategy uses sum != 1, though mostly they are 1.0)\n",
        "        norm_w = w1 + w2 + 1e-9\n",
        "\n",
        "        # Construct the \"Foundation Matrix\" that Entity X will optimize\n",
        "        foundation_oof = (w1 * elite_preds[0] + w2 * elite_preds[1]) / norm_w\n",
        "\n",
        "        # 2. Train Entity X\n",
        "        self.unit_27.fit_evolve(foundation_oof, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_scaled = self.scaler_.transform(X)\n",
        "\n",
        "        # 1. Select the Locked Weights (6-Way Support)\n",
        "        if hasattr(self, \"strategy_\"):\n",
        "            if self.strategy_ == \"death_ray\": active_weights = self.weights_death_ray_\n",
        "            elif self.strategy_ == \"ace\": active_weights = self.weights_ace_\n",
        "            elif self.strategy_ == \"linear\": active_weights = self.weights_linear_\n",
        "            elif self.strategy_ == \"balance\": active_weights = self.weights_balance_\n",
        "            elif self.strategy_ == \"inv_linear\": active_weights = self.weights_inv_linear_\n",
        "            elif self.strategy_ == \"inv_council\": active_weights = self.weights_inv_council_\n",
        "            else: active_weights = self.weights_council_\n",
        "        else:\n",
        "            active_weights = self.weights_council_ # Default Fallback\n",
        "\n",
        "        # 2. Vectorized Weighted Prediction\n",
        "        final_pred = None\n",
        "\n",
        "        # CRITICAL: THIS ORDER MUST MATCH FIT PHASE 2 EXACTLY\n",
        "        all_units = [\n",
        "            # 1. Standard (01-11)\n",
        "            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "\n",
        "            # 2. Cosmic / Physics (18-21)\n",
        "            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n",
        "\n",
        "            # 3. Competitors\n",
        "            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n",
        "\n",
        "            # 4. Souls (12-17)\n",
        "            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n",
        "\n",
        "            # 5. Neural (25)\n",
        "            self.unit_25,\n",
        "            self.unit_26\n",
        "        ]\n",
        "\n",
        "        # Safety Check\n",
        "        if len(all_units) != len(active_weights):\n",
        "            if self.verbose: print(f\"CRITICAL ERROR: Weight Mismatch. Units: {len(all_units)} vs Weights: {len(active_weights)}\")\n",
        "            return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        for i, unit in enumerate(all_units):\n",
        "            # Only predict if this unit is active (Weight > 0)\n",
        "            if active_weights[i] > 0:\n",
        "                try:\n",
        "                    if i == 25: # Unit 26 (Death Ray) Special Logic\n",
        "                         # Needs Rank 1 Elite prediction on THIS new data\n",
        "                         # We approximate Rank 1 by finding the highest weighted unit in the active set (excluding 26)\n",
        "                         temp_w = np.copy(active_weights)\n",
        "                         temp_w[25] = -1\n",
        "                         r1_idx = np.argmax(temp_w)\n",
        "\n",
        "                         r1_unit = all_units[r1_idx]\n",
        "                         if hasattr(r1_unit, \"predict_proba\"): r1_p = r1_unit.predict_proba(X_scaled)\n",
        "                         else:\n",
        "                             d = r1_unit.decision_function(X_scaled)\n",
        "                             r1_p = np.exp(d)/np.sum(np.exp(d), axis=1, keepdims=True)\n",
        "\n",
        "                         X_fused = np.hstack([X_scaled, r1_p])\n",
        "                         p = unit.predict_proba(X_fused)\n",
        "\n",
        "                    else:\n",
        "                        if hasattr(unit, \"predict_proba\"):\n",
        "                            p = unit.predict_proba(X_scaled)\n",
        "                        else:\n",
        "                            d = unit.decision_function(X_scaled)\n",
        "                            # Softmax\n",
        "                            max_d = np.max(d, axis=1, keepdims=True)\n",
        "                            exp_d = np.exp(d - max_d)\n",
        "                            p = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "                    if final_pred is None:\n",
        "                        final_pred = active_weights[i] * p\n",
        "                    else:\n",
        "                        final_pred += active_weights[i] * p\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if final_pred is None: return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        # Normalization\n",
        "        base_pred = final_pred / np.sum(final_pred, axis=1, keepdims=True)\n",
        "\n",
        "        # 3. APPLY ENTITY X (The Final Transformation)\n",
        "        return self.unit_27.predict_proba(base_pred)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "def HarmonicResonanceForest_Ultimate(n_estimators=None):\n",
        "    return HarmonicResonanceClassifier_BEAST_21D(verbose=True)\n",
        "\n",
        "# --- ADD THIS AT THE ABSOLUTE BOTTOM ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Put your data loading here\n",
        "    # X, y = load_your_data()\n",
        "\n",
        "    # 2. Put your model execution here\n",
        "    # model = HarmonicResonanceForest_Ultimate()\n",
        "    # model.fit(X, y)\n",
        "\n",
        "    print(\"✅ Titan-21 Safety Protocol Engaged. System is stable.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils import resample\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Updated to accept custom_X and custom_y\n",
        "def run_comparative_benchmark(dataset_name, openml_id, sample_limit=3000, custom_X=None, custom_y=None):\n",
        "    print(f\"\\n[DATASET] Loading {dataset_name} (ID: {openml_id})...\")\n",
        "\n",
        "    try:\n",
        "        # --- PATH A: Custom Data Provided (Pre-cleaned) ---\n",
        "        if custom_X is not None and custom_y is not None:\n",
        "            print(\"  > Using provided Custom Data...\")\n",
        "            X = custom_X\n",
        "            y = custom_y\n",
        "\n",
        "            # Ensure X is numpy (in case a DF was passed)\n",
        "            if hasattr(X, 'values'):\n",
        "                X = X.values\n",
        "\n",
        "        # --- PATH B: Fetch from OpenML ---\n",
        "        else:\n",
        "            # Fetch as DataFrame to handle types better\n",
        "            X_df, y = fetch_openml(data_id=openml_id, return_X_y=True, as_frame=True, parser='auto')\n",
        "\n",
        "            # 1. AUTO-CLEANER: Convert Objects/Strings to Numbers (Only for DataFrames)\n",
        "            for col in X_df.columns:\n",
        "                if X_df[col].dtype == 'object' or X_df[col].dtype.name == 'category':\n",
        "                    le = LabelEncoder()\n",
        "                    X_df[col] = le.fit_transform(X_df[col].astype(str))\n",
        "\n",
        "            X = X_df.values # Convert to Numpy for HRF\n",
        "\n",
        "        # --- COMMON PIPELINE (NaN Handling) ---\n",
        "        # Even if custom data is passed, we double-check for NaNs to be safe\n",
        "        if np.isnan(X).any():\n",
        "            print(\"  > NaNs detected. Imputing with Mean strategy...\")\n",
        "            imp = SimpleImputer(strategy='mean')\n",
        "            X = imp.fit_transform(X)\n",
        "\n",
        "        le_y = LabelEncoder()\n",
        "        y = le_y.fit_transform(y)\n",
        "\n",
        "        # 3. GPU Limit Check\n",
        "        if len(X) > sample_limit:\n",
        "            print(f\"  ...Downsampling from {len(X)} to {sample_limit} (GPU Limit)...\")\n",
        "            X, y = resample(X, y, n_samples=sample_limit, random_state=42, stratify=y)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "        print(f\"  Shape: {X.shape} | Classes: {len(np.unique(y))}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    competitors = {\n",
        "        \"SVM (RBF)\": make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "        \"XGBoost (GPU)\": XGBClassifier(\n",
        "            device='cuda',\n",
        "            tree_method='hist',\n",
        "            #use_label_encoder=False,\n",
        "            eval_metric='logloss',\n",
        "            random_state=42\n",
        "        ),\n",
        "        # Ensure your HRF class is defined in the notebook before running this\n",
        "        \"HRF Ultimate (GPU)\": HarmonicResonanceForest_Ultimate(n_estimators=60)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    print(f\"\\n[BENCHMARK] Executing comparisons on {dataset_name}...\")\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"{'Model Name':<25} | {'Accuracy':<10} | {'Status'}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    hrf_acc = 0\n",
        "\n",
        "    for name, model in competitors.items():\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, preds)\n",
        "            results[name] = acc\n",
        "            print(f\"{name:<25} | {acc:.4%}    | Done\")\n",
        "\n",
        "            if \"HRF\" in name:\n",
        "                hrf_acc = acc\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{name:<25} | FAILED      | {e}\")\n",
        "\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    best_competitor = 0\n",
        "    for k, v in results.items():\n",
        "        if \"HRF\" not in k and v > best_competitor:\n",
        "            best_competitor = v\n",
        "\n",
        "    margin = hrf_acc - best_competitor\n",
        "\n",
        "    if margin > 0:\n",
        "        print(f\" HRF WINNING MARGIN: +{margin:.4%}\")\n",
        "    else:\n",
        "        print(f\" HRF GAP: {margin:.4%}\")"
      ],
      "metadata": {
        "id": "Rd6G3AlvkiaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 2: Phoneme (Star Noise)\n",
        "# ID: 1489\n",
        "# Type: Audio/Harmonic Time-Series\n",
        "# Though originally for speech, the high-frequency harmonics in this data mimic the acoustic oscillations of stars (Asteroseismology).\n",
        "\n",
        "run_comparative_benchmark(\n",
        "    dataset_name=\"Phoneme\",\n",
        "    openml_id=1489,\n",
        "    sample_limit=5404 #6\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcydO0t-kmNs",
        "outputId": "9df87d4c-540d-4a4c-b201-35a3a8d231db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DATASET] Loading Phoneme (ID: 1489)...\n",
            "  Shape: (5404, 5) | Classes: 2\n",
            "\n",
            "[BENCHMARK] Executing comparisons on Phoneme...\n",
            "-----------------------------------------------------------------\n",
            "Model Name                | Accuracy   | Status\n",
            "-----------------------------------------------------------------\n",
            "SVM (RBF)                 | 83.2562%    | Done\n",
            "Random Forest             | 90.1018%    | Done\n",
            "XGBoost (GPU)             | 87.0490%    | Done\n",
            " >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\n",
            " > Initiating The Ouroboros Protocol (Stabilized)...\n",
            " > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\n",
            "    [Standard] Geom: 83.55% | Logic: 82.65% | HARMONIC: 83.10%\n",
            "    [Robust  ] Geom: 84.25% | Logic: 82.65% | HARMONIC: 83.44%\n",
            "    [MinMax  ] Geom: 83.45% | Logic: 82.65% | HARMONIC: 83.05%\n",
            " >>> LENS LOCKED: ROBUST SCALER (Consensus Achieved) <<<\n",
            " > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\n",
            "    >>> Resonance (SVM) Tuned: {'gamma': 'scale', 'C': 50.0} | Score: 86.70%\n",
            "    >>> Nu-Warp (NuSVC) Tuned: {'nu': 0.2, 'gamma': 'scale'} | Score: 87.53%\n",
            " > Phase 1: Awakening the Souls (Rapid Evolution)...\n",
            "--------------------------------------------------------------------------------\n",
            " UNIT NAME            | ACCURACY | EVOLVED DNA PARAMETERS\n",
            "--------------------------------------------------------------------------------\n",
            " SOUL-01 (Original)   | 89.25%  | Freq: 2.44 | Gamma: 3.34 | P: 2.2\n",
            " SOUL-02 (Mirror A)   | 89.94%  | Freq: 2.24 | Gamma: 2.71 | P: 2.4\n",
            " SOUL-03 (Mirror B)   | 89.94%  | Freq: 2.18 | Gamma: 4.91 | P: 2.1\n",
            " SOUL-D (AGI Hyper)   | 89.71%  | Freq: 2.94 | Gamma: 1.04 | P: 2.0\n",
            " SOUL-E (AGI Deep)    | 89.25%  | Freq: 2.50 | Gamma: 0.50 | P: 2.0\n",
            " SOUL-F (AGI Omni)    | 89.94%  | Freq: 2.24 | Gamma: 1.81 | P: 2.5\n",
            " NEURAL-ELM (Omni)    | 86.47%  | H:150 | Act:tanh | Alpha:0.08\n",
            " GOLDEN-FOREST        | 89.94%  | Res: 1.618 | Decay: 1.62 | Shift: 137.5\n",
            " ENTROPY-FOREST       | 75.26%  | Components: 100\n",
            " QUANTUM-FOREST       | 81.16%  | Gamma: 0.50 | N-Comp: 200\n",
            " GRAVITY-FOREST       | 76.18%  | Horizon: 10.0% | Power: 2.00\n",
            "--------------------------------------------------------------------------------\n",
            " > Phase 2: The Grand Qualifier (Scanning All 26 Candidates)...\n",
            "\n",
            "======================================================================\n",
            " >>> THE 21D PERFORMANCE MONITOR (Phase 2 Qualification) <<<\n",
            "======================================================================\n",
            " RANK   | UNIT NAME          | SCORE      | STATUS\n",
            "----------------------------------------------------------------------\n",
            " 01     | Logic-ET           | 90.75%    | PROMOTED\n",
            " 02     | Logic-RF           | 90.52%    | PROMOTED\n",
            " 03     | BENCH-RF           | 90.29%    | PROMOTED\n",
            " 04     | SOUL-TwinB         | 89.94%    | PROMOTED\n",
            " 05     | SOUL-TwinA         | 89.94%    | PROMOTED\n",
            " 06     | SOUL-F(AGI)        | 89.94%    | PROMOTED\n",
            " 07     | GOLDEN-FOREST      | 89.94%    | PROMOTED\n",
            " 08     | SOUL-D(AGI)        | 89.71%    | PROMOTED\n",
            " 09     | Logic-HG           | 89.48%    | PROMOTED\n",
            " 10     | SOUL-E(AGI)        | 89.25%    | PROMOTED\n",
            " 11     | BENCH-XGB          | 89.25%    | PROMOTED\n",
            " 12     | SOUL-Orig          | 89.25%    | PROMOTED\n",
            " 13     | Geom-K3            | 89.13%    | Eliminated\n",
            " 14     | Geom-K9            | 88.67%    | Eliminated\n",
            " 15     | Grad-XG2           | 88.32%    | Eliminated\n",
            " 16     | Grad-XG1           | 87.98%    | Eliminated\n",
            " 17     | Neural-ELM         | 86.47%    | Eliminated\n",
            " 18     | Nu-Warp            | 85.43%    | Eliminated\n",
            " 19     | BENCH-SVM          | 84.97%    | Eliminated\n",
            " 20     | Resonance          | 84.62%    | Eliminated\n",
            " 21     | QUANTUM-FOREST     | 81.16%    | Eliminated\n",
            " 22     | Space-QDA          | 78.61%    | Eliminated\n",
            " 23     | PolyKer            | 77.92%    | Eliminated\n",
            " 24     | GRAVITY-FOREST     | 76.18%    | Eliminated\n",
            " 25     | ENTROPY-FOREST     | 75.26%    | Eliminated\n",
            " 26     | THE DEATH RAY      | 0.00%    | Eliminated\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            " >>> PHASE 3: THE OUROBOROS PROTOCOL (Top 12 Candidates - 100% Validation) <<<\n",
            "================================================================================\n",
            " RANK | UNIT NAME          | OOF ACCURACY | STATUS\n",
            "--------------------------------------------------------------------------------\n",
            " 01   | Logic-ET           | 91.1173%   | Validated\n",
            " 02   | Logic-RF           | 90.5621%   | Validated\n",
            " 03   | BENCH-RF           | 90.2614%   | Validated\n",
            " 04   | SOUL-TwinB         | 85.4962%   | Validated\n",
            " 05   | SOUL-TwinA         | 85.4962%   | Validated\n",
            " 06   | SOUL-F(AGI)        | 85.2417%   | Validated\n",
            " 07   | GOLDEN-FOREST      | 89.6137%   | Validated\n",
            " 08   | SOUL-D(AGI)        | 85.2417%   | Validated\n",
            " 09   | Logic-HG           | 90.2845%   | Validated\n",
            " 10   | SOUL-E(AGI)        | 85.2417%   | Validated\n",
            " 11   | BENCH-XGB          | 89.3361%   | Validated\n",
            " 12   | SOUL-Orig          | 85.4962%   | Validated\n",
            "--------------------------------------------------------------------------------\n",
            " > [STRATEGY LAB] Ace: 91.0941% | Council: 91.0248% | Linear: 91.1173%\n",
            " > [STRATEGY LAB] Balance: 91.0248% | Inv-Lin: 91.0941% | Underdog: 91.0941%\n",
            " >>> LINEAR STRATEGY LOCKED. <<<\n",
            " > Phase 4: Final Assimilation (Retraining Top 2 Elites)...\n",
            " > [HYPERNOVA] Elite Source Acquired: ExtraTreesClassifier\n",
            " > [HYPERNOVA] Current Champion to Beat: 91.1173% (LINEAR)\n",
            " > [DEATH RAY] Stand Down. No gain over LINEAR (Ray: 91.1173% vs Champ: 91.1173%).\n",
            "\n",
            "=====================================================================================\n",
            " >>> PHASE 5: THE FINAL CONSTITUTION (Dual-Core Analysis) <<<\n",
            "=====================================================================================\n",
            " [A] STANDARD CHAMPION: LINEAR     (Internal Score: 91.1173%)\n",
            " [B] THE DEATH RAY:     REJECTED   (Internal Score: 91.1173%)\n",
            "-------------------------------------------------------------------------------------\n",
            " >>> ACTIVE CONFIGURATION: LINEAR\n",
            " RANK | UNIT NAME          | WEIGHT   | DNA CONFIGURATION\n",
            "-------------------------------------------------------------------------------------\n",
            " 01   | Logic-ET           | 60.00%    | [TREE] Trees:1000\n",
            " 02   | Logic-RF           | 40.00%    | [TREE] Trees:1000\n",
            "-------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "=====================================================================================\n",
            " >>> PHASE 6: ENTITY X ACTIVATION (Genetic Optimization) <<<\n",
            "=====================================================================================\n",
            " > [ENTITY X] Reservoir Frozen. Floor Accuracy: 91.1173%\n",
            " > [ENTITY X] Evolution Success. Reservoir + Gain: +0.1619% (Total: 91.2792%)\n",
            "HRF Ultimate (GPU)        | 90.1018%    | Done\n",
            "-----------------------------------------------------------------\n",
            " HRF GAP: 0.0000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 3: Wall-Following Robot Navigation\n",
        "# ID: 1497\n",
        "# Type: Sensor/Geometric (Ultrasound Waves)\n",
        "\n",
        "run_comparative_benchmark(\n",
        "    dataset_name=\"Wall-Following Robot\",\n",
        "    openml_id=1497,\n",
        "    sample_limit=5456 #25\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "uoATAVbDkoxT",
        "outputId": "6a43870e-8bbd-4f38-f2cd-5fd4aff64563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DATASET] Loading Wall-Following Robot (ID: 1497)...\n",
            "  Shape: (5456, 24) | Classes: 4\n",
            "\n",
            "[BENCHMARK] Executing comparisons on Wall-Following Robot...\n",
            "-----------------------------------------------------------------\n",
            "Model Name                | Accuracy   | Status\n",
            "-----------------------------------------------------------------\n",
            "SVM (RBF)                 | 89.1026%    | Done\n",
            "Random Forest             | 99.2674%    | Done\n",
            "XGBoost (GPU)             | 99.8168%    | Done\n",
            " >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\n",
            " > Initiating The Ouroboros Protocol (Stabilized)...\n",
            " > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\n",
            "    [Standard] Geom: 78.80% | Logic: 96.35% | HARMONIC: 86.70%\n",
            "    [Robust  ] Geom: 80.20% | Logic: 96.35% | HARMONIC: 87.54%\n",
            "    [MinMax  ] Geom: 79.35% | Logic: 96.35% | HARMONIC: 87.03%\n",
            " >>> LENS LOCKED: ROBUST SCALER (Consensus Achieved) <<<\n",
            " > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\n",
            "    >>> Resonance (SVM) Tuned: {'gamma': 0.1, 'C': 50.0} | Score: 90.90%\n",
            "    >>> Nu-Warp (NuSVC) Tuned: {'nu': 0.1, 'gamma': 'scale'} | Score: 90.95%\n",
            " > Phase 1: Awakening the Souls (Rapid Evolution)...\n",
            "--------------------------------------------------------------------------------\n",
            " UNIT NAME            | ACCURACY | EVOLVED DNA PARAMETERS\n",
            "--------------------------------------------------------------------------------\n",
            " SOUL-01 (Original)   | 89.92%  | Freq: 0.62 | Gamma: 0.50 | P: 1.9\n",
            " SOUL-02 (Mirror A)   | 90.61%  | Freq: 0.62 | Gamma: 2.46 | P: 1.6\n",
            " SOUL-03 (Mirror B)   | 89.92%  | Freq: 0.74 | Gamma: 1.55 | P: 1.9\n",
            " SOUL-D (AGI Hyper)   | 90.15%  | Freq: 0.53 | Gamma: 2.36 | P: 1.8\n",
            " SOUL-E (AGI Deep)    | 90.26%  | Freq: 0.51 | Gamma: 0.50 | P: 1.8\n",
            " SOUL-F (AGI Omni)    | 91.18%  | Freq: 0.67 | Gamma: 2.78 | P: 1.6\n",
            " NEURAL-ELM (Omni)    | 85.22%  | H:248 | Act:relu | Alpha:0.12\n",
            " GOLDEN-FOREST        | 88.55%  | Res: 1.618 | Decay: 1.62 | Shift: 137.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1118776749.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Type: Sensor/Geometric (Ultrasound Waves)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m run_comparative_benchmark(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Wall-Following Robot\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mopenml_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1497\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-738022149.py\u001b[0m in \u001b[0;36mrun_comparative_benchmark\u001b[0;34m(dataset_name, openml_id, sample_limit, custom_X, custom_y)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompetitors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2728846572.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_test_oracle, y_test_oracle)\u001b[0m\n\u001b[1;32m   1898\u001b[0m                 \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"evolve\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1900\u001b[0;31m                     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_select\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_select\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m                     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2728846572.py\u001b[0m in \u001b[0;36mevolve\u001b[0;34m(self, X, y, generations)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mGPU_AVAILABLE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2728846572.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2728846572.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    540\u001b[0m                 \u001b[0;31m# Log-Gaussian PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                         \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_g\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m                 \u001b[0muniv_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_p\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cupy/_math/sumprod.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# TODO(okuta): check type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}