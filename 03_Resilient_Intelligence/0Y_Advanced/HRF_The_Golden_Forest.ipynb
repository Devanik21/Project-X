{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6CDNPSwSNn7",
        "outputId": "c2101b13-d378-4a14-9188-574127388cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU DETECTED: 1 device(s) active.\n",
            "üöÄ CuPy Backend Ready for Golden Forest & Entropy Maxwell Units.\n",
            "\n",
            "‚ú® SYSTEM STATUS: Golden Forest & Entropy Units Loaded.\n",
            "‚öîÔ∏è  COMPETITORS READY: SVM, XGBoost, Random Forest, ExtraTrees imported.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- GPU ACCELERATION SETUP ---\n",
        "try:\n",
        "    import cupy as cp\n",
        "    GPU_AVAILABLE = True\n",
        "    print(f\"‚úÖ GPU DETECTED: {cp.cuda.runtime.getDeviceCount()} device(s) active.\")\n",
        "    print(f\"üöÄ CuPy Backend Ready for Golden Forest & Entropy Maxwell Units.\")\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    import numpy as cp  # Fallback alias (though logic inside classes handles this)\n",
        "    print(\"‚ö†Ô∏è GPU NOT DETECTED. Running in CPU Fallback Mode (Slower).\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 18. THE GOLDEN FOREST (GPU T4 - Parallel Ensemble) ---\n",
        "class GoldenSpiralUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=21, n_estimators=100):\n",
        "        # n_estimators=100 ensures 'Forest' power but keeps it sub-second on GPU\n",
        "        self.k = k\n",
        "        self.n_estimators = n_estimators\n",
        "        self.classes_ = None\n",
        "        self.X_train_ = None\n",
        "        self.y_train_ = None\n",
        "        # DNA: The \"Seed\" parameters for the forest\n",
        "        self.dna_ = {\"resonance\": 1.618, \"decay\": 1.618, \"shift\": 137.5}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if GPU_AVAILABLE:\n",
        "            self.X_train_ = cp.asarray(X, dtype=cp.float32)\n",
        "            self.y_train_ = cp.asarray(y)\n",
        "        else:\n",
        "            self.X_train_ = np.array(X, dtype=np.float32)\n",
        "            self.y_train_ = np.array(y)\n",
        "\n",
        "        # [GPU STRATEGY]: We don't train 50 separate trees.\n",
        "        # We store the data ONCE. We will simulate 50 \"viewpoints\" during prediction.\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE:\n",
        "            # CPU Fallback (simplified uniform for safety if CuPy fails completely)\n",
        "            return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        n_test = len(X_g)\n",
        "        n_classes = len(self.classes_)\n",
        "\n",
        "        # 1. THE HEAVY LIFT: Calculate Neighbors ONCE (The most expensive part)\n",
        "        # We use a single massive matrix op instead of 50 small ones.\n",
        "\n",
        "        # Euclidean Dist ^ 2 = x^2 + y^2 - 2xy\n",
        "        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n",
        "        Y2 = cp.sum(self.X_train_**2, axis=1)\n",
        "        XY = cp.dot(X_g, self.X_train_.T)\n",
        "        dists_sq = cp.maximum(X2 + Y2 - 2*XY, 0.0)\n",
        "        dists = cp.sqrt(dists_sq)\n",
        "\n",
        "        # Get Top K\n",
        "        top_k_idx = cp.argsort(dists, axis=1)[:, :self.k]\n",
        "        row_idx = cp.arange(n_test)[:, None]\n",
        "        top_dists = dists[row_idx, top_k_idx] # (N, k)\n",
        "        top_y = self.y_train_[top_k_idx]      # (N, k)\n",
        "\n",
        "        # 2. THE FOREST SIMULATION (Vectorized Ensemble)\n",
        "        # We apply n_estimators different \"Physics Laws\" to the SAME neighbors instantaneously.\n",
        "\n",
        "        total_probs = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "\n",
        "        # Generate random mutations for the ensemble on the fly (Deterministic seed)\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Batch the ensemble calculation\n",
        "        decay_vars = rng.uniform(0.5, 3.0, self.n_estimators)\n",
        "        shift_vars = rng.uniform(0.0, 360.0, self.n_estimators)\n",
        "        res_vars = rng.uniform(1.0, 2.0, self.n_estimators)\n",
        "\n",
        "        # Loop through \"Universes\" (Fast loop)\n",
        "        for i in range(self.n_estimators):\n",
        "            decay = decay_vars[i]\n",
        "            shift = np.deg2rad(shift_vars[i])\n",
        "            res = res_vars[i]\n",
        "\n",
        "            # Physics: Weight = 1/d^decay * Cosine_Resonance\n",
        "            # Add epsilon to dists\n",
        "            w_base = 1.0 / (cp.power(top_dists, decay) + 1e-9)\n",
        "            w_spiral = 1.0 + 0.5 * cp.cos(cp.log(top_dists + 1e-9) * res + shift)\n",
        "            w = w_base * cp.maximum(w_spiral, 0.0)\n",
        "\n",
        "            # Aggregate for this tree\n",
        "            tree_p = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "            for c_idx, cls in enumerate(self.classes_):\n",
        "                mask = (top_y == cls)\n",
        "                tree_p[:, c_idx] = cp.sum(w * mask, axis=1)\n",
        "\n",
        "            # Normalize tree\n",
        "            t_sum = cp.sum(tree_p, axis=1, keepdims=True)\n",
        "            total_probs += tree_p / (t_sum + 1e-9)\n",
        "\n",
        "        # Final Average\n",
        "        final_probs = total_probs / self.n_estimators\n",
        "        return cp.asnumpy(final_probs)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# --- Unit 19. THE ENTROPY FOREST (GPU T4 - Bootstrap Thermodynamics) ---\n",
        "class EntropyMaxwellUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=100):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.forest_stats_ = [] # Stores (mean, var) for 50 bootstraps\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\"n_components\": 100} # Placeholder\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if not GPU_AVAILABLE: return self\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        y_g = cp.asarray(y)\n",
        "        n_samples = len(X)\n",
        "\n",
        "        self.forest_stats_ = []\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Train n_estimators Universes instantly using GPU Bootstrap\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Bootstrap indices\n",
        "            indices = rng.choice(n_samples, n_samples, replace=True)\n",
        "            X_boot = X_g[indices]\n",
        "            y_boot = y_g[indices]\n",
        "\n",
        "            universe_stats = {}\n",
        "            for cls in self.classes_:\n",
        "                X_c = X_boot[y_boot == cls]\n",
        "                if len(X_c) < 2:\n",
        "                    # Fallback to global if class missing in bootstrap\n",
        "                    X_c = X_g[y_g == cls]\n",
        "\n",
        "                # We simply store Mean and Var (Gaussian Approximation)\n",
        "                # This is much faster than GMM and sufficient for Entropy Forest\n",
        "                mu = cp.mean(X_c, axis=0)\n",
        "                sigma = cp.var(X_c, axis=0) + 1e-5 # Stability\n",
        "                prior = len(X_c) / n_samples\n",
        "                universe_stats[cls] = (mu, sigma, prior)\n",
        "\n",
        "            self.forest_stats_.append(universe_stats)\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "        # Ensembling\n",
        "        for stats in self.forest_stats_:\n",
        "            univ_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "            for i, cls in enumerate(self.classes_):\n",
        "                mu, sigma, prior = stats[cls]\n",
        "                # Log-Gaussian PDF\n",
        "                log_p = -0.5 * cp.sum(cp.log(2 * np.pi * sigma), axis=0) - \\\n",
        "                        0.5 * cp.sum((X_g - mu)**2 / sigma, axis=1)\n",
        "                univ_probs[:, i] = log_p + cp.log(prior)\n",
        "\n",
        "            # Softmax this universe\n",
        "            max_p = cp.max(univ_probs, axis=1, keepdims=True)\n",
        "            exp_p = cp.exp(univ_probs - max_p)\n",
        "            univ_probs = exp_p / cp.sum(exp_p, axis=1, keepdims=True)\n",
        "\n",
        "            total_probs += univ_probs\n",
        "\n",
        "        return cp.asnumpy(total_probs / self.n_estimators)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "print(\"\\n‚ú® SYSTEM STATUS: Golden Forest & Entropy Units Loaded.\")\n",
        "print(\"‚öîÔ∏è  COMPETITORS READY: SVM, XGBoost, Random Forest, ExtraTrees imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "print(\"üì• Fetching PHONEME dataset (ID: 1489) from OpenML...\")\n",
        "\n",
        "# 1. Fetch Data\n",
        "# ID 1489 is the standard 'phoneme' benchmark dataset\n",
        "data = fetch_openml(data_id=1489, as_frame=False, parser='auto')\n",
        "X_raw, y_raw = data.data, data.target\n",
        "\n",
        "# 2. Preprocess Target\n",
        "# Ensure targets are integers for our arrays\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_raw)\n",
        "X = X_raw.astype(np.float32)\n",
        "\n",
        "# 3. Split & Scale\n",
        "# 80/20 Split is standard for this size\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 4. Standardization\n",
        "# CRITICAL for GoldenSpiralUnit (Distance-based) and SVM\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"‚úÖ Dataset Loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "print(f\"üîπ Training Shape: {X_train.shape} | Test Shape: {X_test.shape}\")\n",
        "print(f\"üîπ Classes: {np.unique(y)}\")\n",
        "print(\"READY FOR BENCHMARK.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7MEoDCoSXVx",
        "outputId": "b9985be9-48c2-4eec-bbef-9214fe51dee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Fetching PHONEME dataset (ID: 1489) from OpenML...\n",
            "‚úÖ Dataset Loaded: 5404 samples, 5 features\n",
            "üîπ Training Shape: (4323, 5) | Test Shape: (1081, 5)\n",
            "üîπ Classes: [0 1]\n",
            "READY FOR BENCHMARK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from time import time\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- 1. SETUP & GPU CHECK ---\n",
        "try:\n",
        "    import cupy as cp\n",
        "    GPU_AVAILABLE = True\n",
        "    print(f\"‚úÖ GPU WARP DRIVE ACTIVE: {cp.cuda.runtime.getDeviceCount()} Device(s)\")\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    import numpy as cp\n",
        "    print(\"‚ö†Ô∏è RUNNING ON CPU (Slower)\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 2. LOAD & WARP DATA (Phoneme Dataset) ---\n",
        "print(\"\\nüì• Loading Phoneme Dataset & Engaging Warp Drive...\")\n",
        "data = fetch_openml(data_id=1489, as_frame=False, parser='auto')\n",
        "X_raw, y_raw = data.data, data.target\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_raw)\n",
        "\n",
        "# --- WARP DRIVE: POLYNOMIAL FEATURES ---\n",
        "# Essential for the Golden Spiral to find hidden dimensions\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_raw)\n",
        "\n",
        "print(f\"üîπ Dimensionality Expanded: {X_raw.shape[1]} features -> {X_poly.shape[1]} features\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_poly)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# --- 3. THE GOLDEN SPIRAL UNIT (Class Definition) ---\n",
        "class GoldenSpiralUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=21, n_estimators=100, mutation_rate=1.0):\n",
        "        self.k = k\n",
        "        self.n_estimators = n_estimators\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.classes_ = None\n",
        "        self.X_train_ = None\n",
        "        self.y_train_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if GPU_AVAILABLE:\n",
        "            self.X_train_ = cp.asarray(X, dtype=cp.float32)\n",
        "            self.y_train_ = cp.asarray(y)\n",
        "        else:\n",
        "            self.X_train_ = np.array(X, dtype=np.float32)\n",
        "            self.y_train_ = np.array(y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        n_test = len(X_g)\n",
        "        n_classes = len(self.classes_)\n",
        "\n",
        "        # --- STEP 1: HYPER-DIMENSIONAL GEOMETRY ---\n",
        "        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n",
        "        Y2 = cp.sum(self.X_train_**2, axis=1)\n",
        "        XY = cp.dot(X_g, self.X_train_.T)\n",
        "        dists = cp.sqrt(cp.maximum(X2 + Y2 - 2*XY, 0.0))\n",
        "\n",
        "        # Get Top K neighbors\n",
        "        top_k_idx = cp.argsort(dists, axis=1)[:, :self.k]\n",
        "        row_idx = cp.arange(n_test)[:, None]\n",
        "        top_dists = dists[row_idx, top_k_idx]\n",
        "        top_y = self.y_train_[top_k_idx]\n",
        "\n",
        "        # --- STEP 2: CHAOTIC PHYSICS SIMULATION ---\n",
        "        total_probs = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        decay_vars = rng.uniform(0.1, 5.0 * self.mutation_rate, self.n_estimators)\n",
        "        shift_vars = rng.uniform(0.0, 360.0, self.n_estimators)\n",
        "        res_vars = rng.uniform(0.5, 4.0 * self.mutation_rate, self.n_estimators)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            decay = decay_vars[i]\n",
        "            shift = np.deg2rad(shift_vars[i])\n",
        "            res = res_vars[i]\n",
        "\n",
        "            w_base = 1.0 / (cp.power(top_dists, decay) + 1e-9)\n",
        "            w_spiral = 1.0 + 0.5 * cp.cos(cp.log(top_dists + 1e-9) * res + shift)\n",
        "\n",
        "            w = w_base * cp.maximum(w_spiral, 0.0)\n",
        "\n",
        "            tree_p = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "            for c_idx, cls in enumerate(self.classes_):\n",
        "                mask = (top_y == cls)\n",
        "                tree_p[:, c_idx] = cp.sum(w * mask, axis=1)\n",
        "\n",
        "            t_sum = cp.sum(tree_p, axis=1, keepdims=True)\n",
        "            total_probs += tree_p / (t_sum + 1e-9)\n",
        "\n",
        "        return cp.asnumpy(total_probs / self.n_estimators)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "# --- 4. DEFINE THE ARENA ---\n",
        "# The Queen is configured with your WINNING WARP PARAMETERS.\n",
        "models = {\n",
        "    \"Golden Forest (Queen)\": GoldenSpiralUnit(k=160, n_estimators=100, mutation_rate=1.5),\n",
        "    \"XGBoost (Gradient)\": xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"ExtraTrees (Randomized)\": ExtraTreesClassifier(n_estimators=100, n_jobs=-1),\n",
        "    \"RandomForest (Bagging)\": RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
        "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', C=1.0)\n",
        "}\n",
        "\n",
        "results = []\n",
        "print(f\"\\n‚öîÔ∏è  BEGINNING BENCHMARK ON {len(X_train)} SAMPLES...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# --- 5. EXECUTE BATTLES ---\n",
        "for name, model in models.items():\n",
        "    print(f\"üîπ Training {name}...\", end=\" \")\n",
        "\n",
        "    # A. Training Time\n",
        "    start = time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time() - start\n",
        "\n",
        "    # B. Inference Time (on Test set)\n",
        "    start = time()\n",
        "    preds = model.predict(X_test)\n",
        "    infer_time = time() - start\n",
        "\n",
        "    # C. Accuracy\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "\n",
        "    # Store stats\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Train Time (s)\": train_time,\n",
        "        \"Infer Time (s)\": infer_time\n",
        "    })\n",
        "    print(f\"Done. (Acc: {acc:.4f})\")\n",
        "\n",
        "# --- 6. THE LEADERBOARD ---\n",
        "df_results = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÜ FINAL RESULTS: PHONEME DATASET (ID 1489)\")\n",
        "print(\"=\"*60)\n",
        "print(df_results.to_string(index=False, formatters={\n",
        "    \"Accuracy\": \"{:.2%}\".format,\n",
        "    \"Train Time (s)\": \"{:.4f}\".format,\n",
        "    \"Infer Time (s)\": \"{:.4f}\".format\n",
        "}))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Highlights\n",
        "winner = df_results.iloc[0]\n",
        "print(f\"\\nü•á WINNER: {winner['Model']} with {winner['Accuracy']:.2%} accuracy.\")\n",
        "if \"Queen\" in winner['Model']:\n",
        "    print(\"üåü IMPRESSIVE: The Golden Queen reigned supreme!\")\n",
        "else:\n",
        "    print(f\"üí° INSIGHT: {winner['Model']} holds the throne for now.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykka3RSSSkAN",
        "outputId": "6b2dbc64-e8aa-497f-fbc1-ddc94788a7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU WARP DRIVE ACTIVE: 1 Device(s)\n",
            "\n",
            "üì• Loading Phoneme Dataset & Engaging Warp Drive...\n",
            "üîπ Dimensionality Expanded: 5 features -> 20 features\n",
            "\n",
            "‚öîÔ∏è  BEGINNING BENCHMARK ON 4323 SAMPLES...\n",
            "------------------------------------------------------------\n",
            "üîπ Training Golden Forest (Queen)... Done. (Acc: 0.8936)\n",
            "üîπ Training XGBoost (Gradient)... Done. (Acc: 0.8862)\n",
            "üîπ Training ExtraTrees (Randomized)... Done. (Acc: 0.8973)\n",
            "üîπ Training RandomForest (Bagging)... Done. (Acc: 0.8992)\n",
            "üîπ Training SVM (RBF Kernel)... Done. (Acc: 0.8400)\n",
            "\n",
            "============================================================\n",
            "üèÜ FINAL RESULTS: PHONEME DATASET (ID 1489)\n",
            "============================================================\n",
            "                  Model Accuracy Train Time (s) Infer Time (s)\n",
            " RandomForest (Bagging)   89.92%         2.3495         0.0435\n",
            "ExtraTrees (Randomized)   89.73%         0.3888         0.0567\n",
            "  Golden Forest (Queen)   89.36%         0.0005         0.1128\n",
            "     XGBoost (Gradient)   88.62%         0.3172         0.0050\n",
            "       SVM (RBF Kernel)   84.00%         0.3706         0.0985\n",
            "============================================================\n",
            "\n",
            "ü•á WINNER: RandomForest (Bagging) with 89.92% accuracy.\n",
            "üí° INSIGHT: RandomForest (Bagging) holds the throne for now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import numpy as np\n",
        "import warnings\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "\n",
        "# --- 1. SETUP & GPU CHECK ---\n",
        "try:\n",
        "    import cupy as cp\n",
        "    GPU_AVAILABLE = True\n",
        "    print(f\"‚úÖ GPU WARP DRIVE ACTIVE: {cp.cuda.runtime.getDeviceCount()} Device(s)\")\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    import numpy as cp\n",
        "    print(\"‚ö†Ô∏è RUNNING ON CPU (Slower)\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 2. LOAD & WARP DATA ---\n",
        "print(\"\\nüì• Loading Phoneme Dataset & Engaging Warp Drive...\")\n",
        "data = fetch_openml(data_id=1489, as_frame=False, parser='auto')\n",
        "X_raw, y_raw = data.data, data.target\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_raw)\n",
        "\n",
        "# --- WARP DRIVE: POLYNOMIAL FEATURES ---\n",
        "# This creates interaction terms: x1*x2, x1^2, etc.\n",
        "# It explodes the dimensionality to reveal hidden patterns.\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_raw)\n",
        "\n",
        "print(f\"üîπ Dimensionality Expanded: {X_raw.shape[1]} features -> {X_poly.shape[1]} features\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_poly)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# --- 3. THE GOLDEN SPIRAL UNIT (High-Energy Variant) ---\n",
        "class GoldenSpiralUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=21, n_estimators=100, mutation_rate=1.0):\n",
        "        self.k = k\n",
        "        self.n_estimators = n_estimators\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.classes_ = None\n",
        "        self.X_train_ = None\n",
        "        self.y_train_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if GPU_AVAILABLE:\n",
        "            self.X_train_ = cp.asarray(X, dtype=cp.float32)\n",
        "            self.y_train_ = cp.asarray(y)\n",
        "        else:\n",
        "            self.X_train_ = np.array(X, dtype=np.float32)\n",
        "            self.y_train_ = np.array(y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        n_test = len(X_g)\n",
        "        n_classes = len(self.classes_)\n",
        "\n",
        "        # --- STEP 1: HYPER-DIMENSIONAL GEOMETRY ---\n",
        "        # With polynomial features, this distance calc is now \"Curved\"\n",
        "        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n",
        "        Y2 = cp.sum(self.X_train_**2, axis=1)\n",
        "        XY = cp.dot(X_g, self.X_train_.T)\n",
        "        dists = cp.sqrt(cp.maximum(X2 + Y2 - 2*XY, 0.0))\n",
        "\n",
        "        # Get Top K neighbors\n",
        "        top_k_idx = cp.argsort(dists, axis=1)[:, :self.k]\n",
        "        row_idx = cp.arange(n_test)[:, None]\n",
        "        top_dists = dists[row_idx, top_k_idx]\n",
        "        top_y = self.y_train_[top_k_idx]\n",
        "\n",
        "        # --- STEP 2: CHAOTIC PHYSICS SIMULATION ---\n",
        "        total_probs = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Increased Mutation Range for \"Warped\" Space\n",
        "        decay_vars = rng.uniform(0.1, 5.0 * self.mutation_rate, self.n_estimators)\n",
        "        shift_vars = rng.uniform(0.0, 360.0, self.n_estimators)\n",
        "        res_vars = rng.uniform(0.5, 4.0 * self.mutation_rate, self.n_estimators)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            decay = decay_vars[i]\n",
        "            shift = np.deg2rad(shift_vars[i])\n",
        "            res = res_vars[i]\n",
        "\n",
        "            w_base = 1.0 / (cp.power(top_dists, decay) + 1e-9)\n",
        "            w_spiral = 1.0 + 0.5 * cp.cos(cp.log(top_dists + 1e-9) * res + shift)\n",
        "\n",
        "            w = w_base * cp.maximum(w_spiral, 0.0)\n",
        "\n",
        "            tree_p = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "            for c_idx, cls in enumerate(self.classes_):\n",
        "                mask = (top_y == cls)\n",
        "                tree_p[:, c_idx] = cp.sum(w * mask, axis=1)\n",
        "\n",
        "            t_sum = cp.sum(tree_p, axis=1, keepdims=True)\n",
        "            total_probs += tree_p / (t_sum + 1e-9)\n",
        "\n",
        "        return cp.asnumpy(total_probs / self.n_estimators)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "# --- 4. EXECUTE WARP SPEED SEARCH ---\n",
        "print(\"\\nüöÄ ENGAGING WARP DRIVE TUNING...\")\n",
        "print(f\"{'K (Neighbors)':<15} {'Estimators':<15} {'Mutation':<15} {'ACCURACY':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# Strategy: Go Bigger on K, Higher on Mutation\n",
        "k_options = [30, 50, 80, 120, 160]\n",
        "# Note: With Poly features, we might need slightly fewer neighbors or many more. Let's test high.\n",
        "n_est_options = [100]\n",
        "mutation_options = [1.5, 2.0, 3.0] # Testing extreme variance\n",
        "\n",
        "best_acc = 0.0\n",
        "best_config = {}\n",
        "\n",
        "for k in k_options:\n",
        "    for n_est in n_est_options:\n",
        "        for mut in mutation_options:\n",
        "            model = GoldenSpiralUnit(k=k, n_estimators=n_est, mutation_rate=mut)\n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, preds)\n",
        "\n",
        "            print(f\"{k:<15} {n_est:<15} {mut:<15} {acc:.2%}\")\n",
        "\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                best_config = {'k': k, 'n_estimators': n_est, 'mutation': mut}\n",
        "\n",
        "print(\"-\" * 65)\n",
        "print(f\"üèÜ WARP DRIVE RESULTS: {best_acc:.2%}\")\n",
        "print(f\"‚öôÔ∏è BEST CONFIG: {best_config}\")\n",
        "\n",
        "if best_acc > 0.9029:\n",
        "    print(\"üî• VICTORY: RANDOM FOREST HAS FALLEN. THE QUEEN REIGNS.\")\n",
        "    print(f\"Gap to 95%: {95.0 - best_acc*100:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ö° STATUS: CLOSE. If we are stuck at 89-90%, we might need 'Power=3' polynomials.\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGn5xVBrT74I",
        "outputId": "c17dedac-15e1-46d9-f22c-ac1a5b58df06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU WARP DRIVE ACTIVE: 1 Device(s)\n",
            "\n",
            "üì• Loading Phoneme Dataset & Engaging Warp Drive...\n",
            "üîπ Dimensionality Expanded: 5 features -> 20 features\n",
            "\n",
            "üöÄ ENGAGING WARP DRIVE TUNING...\n",
            "K (Neighbors)   Estimators      Mutation        ACCURACY  \n",
            "-----------------------------------------------------------------\n",
            "30              100             1.5             88.44%\n",
            "30              100             2.0             88.53%\n",
            "30              100             3.0             88.53%\n",
            "50              100             1.5             88.71%\n",
            "50              100             2.0             88.81%\n",
            "50              100             3.0             88.71%\n",
            "80              100             1.5             88.99%\n",
            "80              100             2.0             88.90%\n",
            "80              100             3.0             88.71%\n",
            "120             100             1.5             89.27%\n",
            "120             100             2.0             89.18%\n",
            "120             100             3.0             88.71%\n",
            "160             100             1.5             89.36%\n",
            "160             100             2.0             89.27%\n",
            "160             100             3.0             88.81%\n",
            "-----------------------------------------------------------------\n",
            "üèÜ WARP DRIVE RESULTS: 89.36%\n",
            "‚öôÔ∏è BEST CONFIG: {'k': 160, 'n_estimators': 100, 'mutation': 1.5}\n",
            "‚ö° STATUS: CLOSE. If we are stuck at 89-90%, we might need 'Power=3' polynomials.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkAUI1N2WjfD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}