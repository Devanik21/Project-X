{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejp8yzO6FlDh",
        "outputId": "03a0c714-2513-4731-ffb4-1ecfe0ceeeb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU DETECTED: HRF v26.0 'Holo-Fractal Universe' Active\n",
            "✅ Titan-21 Safety Protocol Engaged. System is stable.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import warnings\n",
        "from sklearn.utils import check_X_y, check_array\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.fft import fft\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Sklearn Core & Metrics\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import (\n",
        "    LinearDiscriminantAnalysis,\n",
        "    QuadraticDiscriminantAnalysis,\n",
        ")\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesClassifier,\n",
        "    RandomForestClassifier,\n",
        "    HistGradientBoostingClassifier,\n",
        ")\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import (\n",
        "    StratifiedKFold,\n",
        "    train_test_split,\n",
        "    cross_val_predict,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import (\n",
        "    PowerTransformer,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        "    MinMaxScaler,\n",
        ")\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "\n",
        "# Gradient Boosting\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# GPU CHECK\n",
        "try:\n",
        "    import cupy as cp\n",
        "\n",
        "    GPU_AVAILABLE = True\n",
        "    print(\"✅ GPU DETECTED: HRF v26.0 'Holo-Fractal Universe' Active\")\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"⚠️ GPU NOT FOUND: Running in Slow Mode\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# --- 1. THE HOLOGRAPHIC SOUL (Unit 3 - Multiverse Edition - VRAM PINNED) ---\n",
        "class HolographicSoulUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=15):\n",
        "        self.k = k\n",
        "        self.dna_ = {\n",
        "            \"freq\": 2.0, \"gamma\": 0.5, \"power\": 2.0,\n",
        "            \"metric\": \"minkowski\", \"p\": 2.0, \"phase\": 0.0,\n",
        "            \"dim_reduction\": \"none\",\n",
        "        }\n",
        "        self.projector_ = None\n",
        "        self.X_raw_source_ = None\n",
        "        # GPU Cache\n",
        "        self._X_train_gpu = None\n",
        "        self._y_train_gpu = None\n",
        "        # Pre-calculated norms for fast Euclidean\n",
        "        self._X_train_sq_norm = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self._apply_projection(X)\n",
        "        self.y_train_ = y\n",
        "\n",
        "        # [TITAN OPTIMIZATION] Upload to GPU ONCE\n",
        "        if GPU_AVAILABLE:\n",
        "            self._X_train_gpu = cp.asarray(self.X_train_, dtype=cp.float32)\n",
        "            self._y_train_gpu = cp.asarray(self.y_train_)\n",
        "            # Pre-calc Squared Norm for Fast Euclidean Path\n",
        "            self._X_train_sq_norm = cp.sum(self._X_train_gpu ** 2, axis=1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _apply_projection(self, X):\n",
        "        if self.dna_[\"dim_reduction\"] == \"holo\":\n",
        "            n_components = max(2, int(np.sqrt(X.shape[1])))\n",
        "            self.projector_ = GaussianRandomProjection(n_components=n_components, random_state=42)\n",
        "            self.X_train_ = self.projector_.fit_transform(X)\n",
        "        elif self.dna_[\"dim_reduction\"] == \"pca\":\n",
        "            n_components = max(2, int(np.sqrt(X.shape[1])))\n",
        "            self.projector_ = PCA(n_components=n_components, random_state=42)\n",
        "            self.X_train_ = self.projector_.fit_transform(X)\n",
        "        else:\n",
        "            self.projector_ = None\n",
        "            self.X_train_ = X\n",
        "\n",
        "    def set_raw_source(self, X):\n",
        "        self.X_raw_source_ = X\n",
        "\n",
        "    def evolve(self, X_val, y_val, generations=10):\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "\n",
        "        # [TITAN OPTIMIZATION] Pre-load Validation Data\n",
        "        X_val_curr = self.projector_.transform(X_val) if self.projector_ else X_val\n",
        "        X_val_g = cp.asarray(X_val_curr, dtype=cp.float32)\n",
        "        y_val_g = cp.asarray(y_val)\n",
        "\n",
        "        # Pre-calc validation norm for Fast Euclidean\n",
        "        val_sq_norm = cp.sum(X_val_g ** 2, axis=1)\n",
        "\n",
        "        n_universes = 8 # Slightly reduced for speed, keeps high diversity\n",
        "        best_dna = self.dna_.copy()\n",
        "\n",
        "        # Smart Init (Fast Sample)\n",
        "        sample_X = self._X_train_gpu[:100]\n",
        "        dists = cp.mean(cp.linalg.norm(sample_X[:, None, :] - sample_X[None, :, :], axis=2))\n",
        "        median_dist = float(cp.asnumpy(dists))\n",
        "        if median_dist > 0: best_dna[\"freq\"] = 3.14159 / median_dist\n",
        "\n",
        "        # Initial Score\n",
        "        best_acc = self._score_on_gpu(X_val_g, y_val_g, val_sq_norm)\n",
        "\n",
        "        patience = 0\n",
        "\n",
        "        for gen in range(generations):\n",
        "            candidates = []\n",
        "            for _ in range(n_universes):\n",
        "                mutant = best_dna.copy()\n",
        "                trait = random.choice(list(mutant.keys()))\n",
        "\n",
        "                if trait == \"freq\": mutant[\"freq\"] *= np.random.uniform(0.8, 1.25)\n",
        "                elif trait == \"gamma\": mutant[\"gamma\"] = np.random.uniform(0.1, 5.0)\n",
        "                elif trait == \"power\": mutant[\"power\"] = random.choice([0.5, 1.0, 2.0, 3.0, 4.0, 6.0])\n",
        "                elif trait == \"p\":\n",
        "                    # 50% chance to snap to 2.0 (Fast Path), 50% random\n",
        "                    if random.random() < 0.5: mutant[\"p\"] = 2.0\n",
        "                    else: mutant[\"p\"] = np.clip(mutant[\"p\"] + np.random.uniform(-0.5, 0.5), 0.5, 8.0)\n",
        "                elif trait == \"phase\": mutant[\"phase\"] = np.random.uniform(0, 3.14159)\n",
        "                candidates.append(mutant)\n",
        "\n",
        "            generation_best_acc = -1\n",
        "            generation_best_dna = None\n",
        "\n",
        "            for mutant_dna in candidates:\n",
        "                self.dna_ = mutant_dna\n",
        "                # Score using fast internal method\n",
        "                acc = self._score_on_gpu(X_val_g, y_val_g, val_sq_norm)\n",
        "\n",
        "                if acc > generation_best_acc:\n",
        "                    generation_best_acc = acc\n",
        "                    generation_best_dna = mutant_dna\n",
        "\n",
        "            if generation_best_acc >= best_acc:\n",
        "                best_acc = generation_best_acc\n",
        "                best_dna = generation_best_dna\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "\n",
        "            # Reset to best\n",
        "            self.dna_ = best_dna\n",
        "\n",
        "            # [TITAN OPTIMIZATION] Early Stopping\n",
        "            # If we don't improve for 8 generations, the soul is mature.\n",
        "            if patience >= 8:\n",
        "                break\n",
        "\n",
        "        self.dna_ = best_dna\n",
        "        del X_val_g, y_val_g, val_sq_norm\n",
        "        cp.get_default_memory_pool().free_all_blocks()\n",
        "\n",
        "        return best_acc\n",
        "\n",
        "    def _score_on_gpu(self, X_val_g, y_val_g, val_sq_norm=None):\n",
        "        probs = self._predict_proba_gpu_internal(X_val_g, val_sq_norm)\n",
        "        preds = cp.argmax(probs, axis=1)\n",
        "        return float(cp.mean(preds == y_val_g))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.projector_ is not None: X_curr = self.projector_.transform(X)\n",
        "        else: X_curr = X\n",
        "\n",
        "        if GPU_AVAILABLE:\n",
        "            X_g = cp.asarray(X_curr, dtype=cp.float32)\n",
        "            # Calc Norm for new data\n",
        "            x_sq_norm = cp.sum(X_g ** 2, axis=1)\n",
        "            probs = self._predict_proba_gpu_internal(X_g, x_sq_norm)\n",
        "            return cp.asnumpy(probs)\n",
        "        else:\n",
        "            return np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "    def _predict_proba_gpu_internal(self, X_te_g, X_te_sq_norm=None):\n",
        "        n_test = len(X_te_g)\n",
        "        n_classes = len(self.classes_)\n",
        "        probas = []\n",
        "        # Increased Batch Size for T4 (Matrix Multiplication can handle it)\n",
        "        #batch_size = 256 # for wide datasets\n",
        "        batch_size = 2048\n",
        "\n",
        "        p_norm = self.dna_.get(\"p\", 2.0)\n",
        "        gamma = self.dna_[\"gamma\"]\n",
        "        freq = self.dna_[\"freq\"]\n",
        "        power = self.dna_[\"power\"]\n",
        "        phase = self.dna_.get(\"phase\", 0.0)\n",
        "\n",
        "        # CHECK: Can we use Fast Euclidean? (p ~= 2.0)\n",
        "        use_fast_path = abs(p_norm - 2.0) < 0.05\n",
        "\n",
        "        for i in range(0, n_test, batch_size):\n",
        "            end = min(i + batch_size, n_test)\n",
        "            batch_te = X_te_g[i:end]\n",
        "\n",
        "            # --- DISTANCE CALCULATION ---\n",
        "            if use_fast_path and self._X_train_sq_norm is not None:\n",
        "                # [FAST PATH] A^2 + B^2 - 2AB\n",
        "                # 50x Speedup using Matrix Multiplication\n",
        "                if X_te_sq_norm is not None:\n",
        "                    batch_sq = X_te_sq_norm[i:end][:, None]\n",
        "                else:\n",
        "                    batch_sq = cp.sum(batch_te**2, axis=1, keepdims=True)\n",
        "\n",
        "                train_sq = self._X_train_sq_norm[None, :]\n",
        "                dot_prod = cp.dot(batch_te, self._X_train_gpu.T)\n",
        "\n",
        "                dists_sq = batch_sq + train_sq - 2 * dot_prod\n",
        "                dists_sq = cp.maximum(dists_sq, 0.0)\n",
        "                dists = cp.sqrt(dists_sq)\n",
        "            else:\n",
        "                # [SLOW PATH] Broadcasting for non-Euclidean metrics (p != 2)\n",
        "                diff = cp.abs(batch_te[:, None, :] - self._X_train_gpu[None, :, :])\n",
        "                dists = cp.sum(cp.power(diff, p_norm), axis=2)\n",
        "                dists = cp.power(dists, 1.0 / p_norm)\n",
        "\n",
        "            # --- WEIGHTING (RESONANCE) ---\n",
        "            # argpartition is faster than argsort for finding Top K\n",
        "            top_k_idx = cp.argsort(dists, axis=1)[:, : self.k]\n",
        "\n",
        "            row_idx = cp.arange(len(batch_te))[:, None]\n",
        "            top_dists = dists[row_idx, top_k_idx]\n",
        "            top_y = self._y_train_gpu[top_k_idx]\n",
        "\n",
        "            cosine_term = 1.0 + cp.cos(freq * top_dists + phase)\n",
        "            cosine_term = cp.maximum(cosine_term, 0.0)\n",
        "            w = cp.exp(-gamma * (top_dists**2)) * cosine_term\n",
        "            w = cp.power(w, power)\n",
        "\n",
        "            batch_probs = cp.zeros((len(batch_te), n_classes))\n",
        "            for c_idx, cls in enumerate(self.classes_):\n",
        "                class_mask = top_y == cls\n",
        "                batch_probs[:, c_idx] = cp.sum(w * class_mask, axis=1)\n",
        "\n",
        "            total_energy = cp.sum(batch_probs, axis=1, keepdims=True)\n",
        "            total_energy[total_energy == 0] = 1.0\n",
        "            batch_probs /= total_energy\n",
        "            probas.append(batch_probs)\n",
        "\n",
        "        return cp.concatenate(probas)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.predict(X))\n",
        "\n",
        "\n",
        "# --- 3. THE QUANTUM FIELD (Unit 4 - Reserve) ---\n",
        "class QuantumFieldUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.rbf_feature_ = RBFSampler(n_components=100, random_state=42)\n",
        "        self.classifier_ = RidgeClassifier(alpha=1.0)\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\"gamma\": 1.0, \"n_components\": 100}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.rbf_feature_.set_params(\n",
        "            gamma=self.dna_[\"gamma\"], n_components=self.dna_[\"n_components\"]\n",
        "        )\n",
        "        X_quantum = self.rbf_feature_.fit_transform(X)\n",
        "        self.classifier_.fit(X_quantum, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_quantum = self.rbf_feature_.transform(X)\n",
        "        d = self.classifier_.decision_function(X_quantum)\n",
        "        if len(self.classes_) == 2:\n",
        "            probs = 1 / (1 + np.exp(-d))\n",
        "            return np.column_stack([1 - probs, probs])\n",
        "        else:\n",
        "            exp_d = np.exp(d - np.max(d, axis=1, keepdims=True))\n",
        "            return exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "# --- 4. THE ENTROPY MAXWELL (Unit 5 - Reserve) ---\n",
        "class EntropyMaxwellUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.models_ = {}\n",
        "        self.classes_ = None\n",
        "        self.priors_ = None\n",
        "        self.dna_ = {\"n_components\": 1}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.models_ = {}\n",
        "        self.priors_ = {}\n",
        "        n_samples = len(y)\n",
        "        for cls in self.classes_:\n",
        "            X_c = X[y == cls]\n",
        "            if len(X_c) < 2:\n",
        "                self.priors_[cls] = 0.0\n",
        "                continue\n",
        "            self.priors_[cls] = len(X_c) / n_samples\n",
        "            n_comp = min(self.dna_[\"n_components\"], len(X_c))\n",
        "            gmm = GaussianMixture(\n",
        "                n_components=n_comp, covariance_type=\"full\", reg_covar=1e-4, random_state=42\n",
        "            )\n",
        "            gmm.fit(X_c)\n",
        "            self.models_[cls] = gmm\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = np.zeros((len(X), len(self.classes_)))\n",
        "        for i, cls in enumerate(self.classes_):\n",
        "            if cls in self.models_:\n",
        "                log_prob = self.models_[cls].score_samples(X)\n",
        "                log_prob = np.clip(log_prob, -100, 100)\n",
        "                probs[:, i] = np.exp(log_prob) * self.priors_[cls]\n",
        "        total = np.sum(probs, axis=1, keepdims=True) + 1e-10\n",
        "        return probs / total\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "# --- 5. THE OMNI-KERNEL NEXUS (Unit 6 - Reserve) ---\n",
        "class OmniKernelUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.model_ = None\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\n",
        "            \"kernel\": \"rbf\",\n",
        "            \"C\": 1.0,\n",
        "            \"gamma\": \"scale\",\n",
        "            \"degree\": 3,\n",
        "            \"coef0\": 0.0,\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.model_ = SVC(\n",
        "            kernel=self.dna_[\"kernel\"],\n",
        "            C=self.dna_[\"C\"],\n",
        "            gamma=self.dna_[\"gamma\"],\n",
        "            degree=self.dna_[\"degree\"],\n",
        "            coef0=self.dna_[\"coef0\"],\n",
        "            probability=True,\n",
        "            random_state=42,\n",
        "            cache_size=500,\n",
        "        )\n",
        "        self.model_.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model_.predict_proba(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return self.model_.score(X, y)\n",
        "\n",
        "\n",
        "# --- 18. THE GOLDEN SPIRAL (Unit 18 - Nature's Code) ---\n",
        "# --- 18. THE GOLDEN FOREST (GPU T4 - Parallel Ensemble) ---\n",
        "class GoldenSpiralUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=21, n_estimators=100):\n",
        "        # n_estimators=50 ensures 'Forest' power but keeps it sub-second on GPU\n",
        "        self.k = k\n",
        "        self.n_estimators = n_estimators\n",
        "        self.classes_ = None\n",
        "        self.X_train_ = None\n",
        "        self.y_train_ = None\n",
        "        # DNA: The \"Seed\" parameters for the forest\n",
        "        self.dna_ = {\"resonance\": 1.618, \"decay\": 1.618, \"shift\": 137.5}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if GPU_AVAILABLE:\n",
        "            self.X_train_ = cp.asarray(X, dtype=cp.float32)\n",
        "            self.y_train_ = cp.asarray(y)\n",
        "        else:\n",
        "            self.X_train_ = np.array(X, dtype=np.float32)\n",
        "            self.y_train_ = np.array(y)\n",
        "\n",
        "        # [GPU STRATEGY]: We don't train 50 separate trees.\n",
        "        # We store the data ONCE. We will simulate 50 \"viewpoints\" during prediction.\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        n_test = len(X_g)\n",
        "        n_classes = len(self.classes_)\n",
        "\n",
        "        # 1. THE HEAVY LIFT: Calculate Neighbors ONCE (The most expensive part)\n",
        "        # We use a single massive matrix op instead of 50 small ones.\n",
        "\n",
        "        # Euclidean Dist ^ 2 = x^2 + y^2 - 2xy\n",
        "        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n",
        "        Y2 = cp.sum(self.X_train_**2, axis=1)\n",
        "        XY = cp.dot(X_g, self.X_train_.T)\n",
        "        dists_sq = cp.maximum(X2 + Y2 - 2*XY, 0.0)\n",
        "        dists = cp.sqrt(dists_sq)\n",
        "\n",
        "        # Get Top K\n",
        "        top_k_idx = cp.argsort(dists, axis=1)[:, :self.k]\n",
        "        row_idx = cp.arange(n_test)[:, None]\n",
        "        top_dists = dists[row_idx, top_k_idx] # (N, k)\n",
        "        top_y = self.y_train_[top_k_idx]      # (N, k)\n",
        "\n",
        "        # 2. THE FOREST SIMULATION (Vectorized Ensemble)\n",
        "        # We apply 50 different \"Physics Laws\" to the SAME neighbors instantaneously.\n",
        "\n",
        "        total_probs = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "\n",
        "        # Generate random mutations for the ensemble on the fly (Deterministic seed)\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Batch the ensemble calculation\n",
        "        decay_vars = rng.uniform(0.5, 3.0, self.n_estimators)\n",
        "        shift_vars = rng.uniform(0.0, 360.0, self.n_estimators)\n",
        "        res_vars = rng.uniform(1.0, 2.0, self.n_estimators)\n",
        "\n",
        "        # Loop through \"Universes\" (Fast loop)\n",
        "        for i in range(self.n_estimators):\n",
        "            decay = decay_vars[i]\n",
        "            shift = np.deg2rad(shift_vars[i])\n",
        "            res = res_vars[i]\n",
        "\n",
        "            # Physics: Weight = 1/d^decay * Cosine_Resonance\n",
        "            # Add epsilon to dists\n",
        "            w_base = 1.0 / (cp.power(top_dists, decay) + 1e-9)\n",
        "            w_spiral = 1.0 + 0.5 * cp.cos(cp.log(top_dists + 1e-9) * res + shift)\n",
        "            w = w_base * cp.maximum(w_spiral, 0.0)\n",
        "\n",
        "            # Aggregate for this tree\n",
        "            tree_p = cp.zeros((n_test, n_classes), dtype=cp.float32)\n",
        "            for c_idx, cls in enumerate(self.classes_):\n",
        "                mask = (top_y == cls)\n",
        "                tree_p[:, c_idx] = cp.sum(w * mask, axis=1)\n",
        "\n",
        "            # Normalize tree\n",
        "            t_sum = cp.sum(tree_p, axis=1, keepdims=True)\n",
        "            total_probs += tree_p / (t_sum + 1e-9)\n",
        "\n",
        "        # Final Average\n",
        "        final_probs = total_probs / self.n_estimators\n",
        "        return cp.asnumpy(final_probs)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# ---Unit 19. THE ENTROPY FOREST (GPU T4 - Bootstrap Thermodynamics) ---\n",
        "class EntropyMaxwellUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=100):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.forest_stats_ = [] # Stores (mean, var) for 50 bootstraps\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\"n_components\": 100} # Placeholder\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if not GPU_AVAILABLE: return self\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        y_g = cp.asarray(y)\n",
        "        n_samples = len(X)\n",
        "\n",
        "        self.forest_stats_ = []\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Train 50 Universes instantly using GPU Bootstrap\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Bootstrap indices\n",
        "            indices = rng.choice(n_samples, n_samples, replace=True)\n",
        "            X_boot = X_g[indices]\n",
        "            y_boot = y_g[indices]\n",
        "\n",
        "            universe_stats = {}\n",
        "            for cls in self.classes_:\n",
        "                X_c = X_boot[y_boot == cls]\n",
        "                if len(X_c) < 2:\n",
        "                    # Fallback to global if class missing in bootstrap\n",
        "                    X_c = X_g[y_g == cls]\n",
        "\n",
        "                # We simply store Mean and Var (Gaussian Approximation)\n",
        "                # This is much faster than GMM and sufficient for Entropy Forest\n",
        "                mu = cp.mean(X_c, axis=0)\n",
        "                sigma = cp.var(X_c, axis=0) + 1e-5 # Stability\n",
        "                prior = len(X_c) / n_samples\n",
        "                universe_stats[cls] = (mu, sigma, prior)\n",
        "\n",
        "            self.forest_stats_.append(universe_stats)\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "        # Ensembling\n",
        "        for stats in self.forest_stats_:\n",
        "            univ_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "            for i, cls in enumerate(self.classes_):\n",
        "                mu, sigma, prior = stats[cls]\n",
        "                # Log-Gaussian PDF\n",
        "                log_p = -0.5 * cp.sum(cp.log(2 * np.pi * sigma), axis=0) - \\\n",
        "                        0.5 * cp.sum((X_g - mu)**2 / sigma, axis=1)\n",
        "                univ_probs[:, i] = log_p + cp.log(prior)\n",
        "\n",
        "            # Softmax this universe\n",
        "            max_p = cp.max(univ_probs, axis=1, keepdims=True)\n",
        "            exp_p = cp.exp(univ_probs - max_p)\n",
        "            univ_probs = exp_p / cp.sum(exp_p, axis=1, keepdims=True)\n",
        "\n",
        "            total_probs += univ_probs\n",
        "\n",
        "        return cp.asnumpy(total_probs / self.n_estimators)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 20. THE QUANTUM FOREST (GPU T4 - Parallel Ridge Fields) ---\n",
        "class QuantumFluxUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=100, gamma=1.5):\n",
        "        # 20 Quantum Realities (Heavy)\n",
        "        self.n_estimators = n_estimators\n",
        "        self.gamma = gamma\n",
        "        self.forest_ = []\n",
        "        self.classes_ = None\n",
        "        # [FIX] Added n_components to DNA so the logger prints correctly\n",
        "        self.dna_ = {\"gamma\": gamma, \"n_components\": 200}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if not GPU_AVAILABLE: return self\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "\n",
        "        # One-hot Y\n",
        "        y_onehot = cp.zeros((len(y), len(self.classes_)), dtype=cp.float32)\n",
        "        y_raw = cp.asarray(y)\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            y_onehot[y_raw == c, i] = 1.0\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        self.forest_ = []\n",
        "\n",
        "        # Train 20 Ridge Models in Parallel Universes\n",
        "        for i in range(self.n_estimators):\n",
        "            # Vary Gamma slightly for diversity\n",
        "            g_var = self.gamma * rng.uniform(0.8, 1.2)\n",
        "            n_comp = self.dna_[\"n_components\"] # Use DNA value\n",
        "\n",
        "            # RBF Weights\n",
        "            W = rng.normal(0, np.sqrt(2*g_var), (n_features, n_comp)).astype(cp.float32)\n",
        "            B = rng.uniform(0, 2*np.pi, n_comp).astype(cp.float32)\n",
        "\n",
        "            # Project X -> Z\n",
        "            Z = cp.cos(cp.dot(X_g, W) + B) * cp.sqrt(2./n_comp)\n",
        "\n",
        "            # Solve Ridge: (Z'Z + aI)^-1 Z'Y\n",
        "            alpha = 1.0\n",
        "            I = cp.eye(n_comp, dtype=cp.float32)\n",
        "\n",
        "            try:\n",
        "                # Cholesky solve (Ultra Fast on T4)\n",
        "                weights = cp.linalg.solve(cp.dot(Z.T, Z) + alpha*I, cp.dot(Z.T, y_onehot))\n",
        "                self.forest_.append((W, B, weights))\n",
        "            except: pass # Skip singular universes\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "\n",
        "        valid = 0\n",
        "        for W, B, weights in self.forest_:\n",
        "            Z = cp.cos(cp.dot(X_g, W) + B) * cp.sqrt(2./len(B))\n",
        "            raw = cp.dot(Z, weights)\n",
        "\n",
        "            # Softmax\n",
        "            max_r = cp.max(raw, axis=1, keepdims=True)\n",
        "            exp_r = cp.exp(raw - max_r)\n",
        "            p = exp_r / cp.sum(exp_r, axis=1, keepdims=True)\n",
        "\n",
        "            total_probs += p\n",
        "            valid += 1\n",
        "\n",
        "        return cp.asnumpy(total_probs / max(1, valid))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# --- 21. THE GRAVITY FOREST (GPU T4 - Many Body Simulation) ---\n",
        "class EventHorizonUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=100):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.centroids_ = None\n",
        "        self.masses_ = None\n",
        "        self.classes_ = None\n",
        "        # [FIX] Added 'decay_power' here to satisfy the printer logic\n",
        "        self.dna_ = {\"horizon_pct\": 10.0, \"decay_power\": 2.0}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if not GPU_AVAILABLE: return self\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "        y_g = cp.asarray(y)\n",
        "\n",
        "        # Calculate Base Centers (The Stars)\n",
        "        self.centroids_ = []\n",
        "        self.masses_ = []\n",
        "        for cls in self.classes_:\n",
        "            X_c = X_g[y_g == cls]\n",
        "            if len(X_c) > 0:\n",
        "                self.centroids_.append(cp.mean(X_c, axis=0))\n",
        "                self.masses_.append(cp.log1p(len(X_c)))\n",
        "            else:\n",
        "                self.centroids_.append(cp.zeros(X.shape[1]))\n",
        "                self.masses_.append(0.0)\n",
        "\n",
        "        self.centroids_ = cp.array(self.centroids_) # (C, F)\n",
        "        self.masses_ = cp.array(self.masses_)       # (C,)\n",
        "        return self\n",
        "\n",
        "    def evolve(self, X, y, generations=20):\n",
        "        # [FIX] Actually calculate accuracy instead of returning 0.99 placeholder\n",
        "        if not GPU_AVAILABLE: return 0.0\n",
        "        preds = self.predict(X)\n",
        "        return accuracy_score(y, preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not GPU_AVAILABLE: return np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "        X_g = cp.asarray(X, dtype=cp.float32)\n",
        "\n",
        "        # 1. Calculate Base Distances (Matrix: Samples x Classes)\n",
        "        # ||X - C||^2 = X^2 + C^2 - 2XC\n",
        "        X2 = cp.sum(X_g**2, axis=1, keepdims=True)\n",
        "        C2 = cp.sum(self.centroids_**2, axis=1)\n",
        "        XC = cp.dot(X_g, self.centroids_.T)\n",
        "        dist_sq = cp.maximum(X2 + C2 - 2*XC, 1e-9) # (N, C)\n",
        "\n",
        "        # 2. Simulate 50 Gravity Variations (The Forest)\n",
        "        total_probs = cp.zeros((len(X), len(self.classes_)), dtype=cp.float32)\n",
        "        rng = cp.random.RandomState(42)\n",
        "\n",
        "        # Use the decay power from DNA as the mean for the random variation\n",
        "        base_decay = self.dna_[\"decay_power\"]\n",
        "        decay_vars = rng.uniform(base_decay * 0.25, base_decay * 1.25, self.n_estimators)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            decay = decay_vars[i]\n",
        "\n",
        "            # Force = Mass / Dist^decay\n",
        "            # (Use Log space for stability)\n",
        "            # Log(F) = Log(M) - decay * Log(Dist^2)/2\n",
        "            # Log(Dist^2)/2 = Log(Dist)\n",
        "\n",
        "            log_dist = 0.5 * cp.log(dist_sq)\n",
        "            log_force = cp.log(self.masses_) - (decay * log_dist)\n",
        "\n",
        "            # Softmax forces\n",
        "            max_f = cp.max(log_force, axis=1, keepdims=True)\n",
        "            exp_f = cp.exp(log_force - max_f)\n",
        "            p = exp_f / cp.sum(exp_f, axis=1, keepdims=True)\n",
        "\n",
        "            total_probs += p\n",
        "\n",
        "        return cp.asnumpy(total_probs / self.n_estimators)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------\n",
        "\n",
        "# --- 18. THE FAST GOLDEN SPIRAL (Lite Version) ---\n",
        "class FastGoldenUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=21):\n",
        "        self.k = k\n",
        "        self.classes_ = None\n",
        "        self.X_train_ = None\n",
        "        self.y_train_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.X_train_ = np.array(X, dtype=np.float32)\n",
        "        self.y_train_ = np.array(y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # FAST LOGIC: No ensemble. Just one Golden Ratio weighted KNN.\n",
        "        # We use standard Euclidean distance but weight neighbors by 1/d^Phi\n",
        "        from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "        X_test = np.array(X, dtype=np.float32)\n",
        "        dists = euclidean_distances(X_test, self.X_train_)\n",
        "\n",
        "        # Get Top K neighbors\n",
        "        idx = np.argsort(dists, axis=1)[:, :self.k]\n",
        "        row_idx = np.arange(len(X))[:, None]\n",
        "\n",
        "        top_dists = dists[row_idx, idx]\n",
        "        top_y = self.y_train_[idx]\n",
        "\n",
        "        # PHI PHYSICS: Weight = 1 / (Distance ^ 1.618)\n",
        "        phi = 1.6180339887\n",
        "        weights = 1.0 / (np.power(top_dists, phi) + 1e-9)\n",
        "\n",
        "        probs = np.zeros((len(X), len(self.classes_)))\n",
        "        for c_idx, cls in enumerate(self.classes_):\n",
        "            # Sum weights where neighbor class matches\n",
        "            mask = (top_y == cls)\n",
        "            probs[:, c_idx] = np.sum(weights * mask, axis=1)\n",
        "\n",
        "        # Normalize\n",
        "        sums = np.sum(probs, axis=1, keepdims=True)\n",
        "        return np.nan_to_num(probs / (sums + 1e-9), nan=1.0/len(self.classes_))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# --- 19. THE FAST ENTROPY (Gaussian Thermodynamics) ---\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "class FastEntropyUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        # GaussianNB is literally a probability density calculator (Thermodynamics)\n",
        "        # It is extremely fast (O(n))\n",
        "        self.model = GaussianNB()\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "\n",
        "# --- 20. THE FAST QUANTUM (Single Field Ridge) ---\n",
        "class FastQuantumUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, gamma=1.0, n_components=100):\n",
        "        # No ensemble. Just one mapping to higher dimension + Linear Solver\n",
        "        self.gamma = gamma\n",
        "        self.n_components = n_components\n",
        "        self.rbf = RBFSampler(gamma=gamma, n_components=n_components, random_state=42)\n",
        "        self.solver = RidgeClassifier(alpha=1.0)\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        X_q = self.rbf.fit_transform(X)\n",
        "        self.solver.fit(X_q, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_q = self.rbf.transform(X)\n",
        "        d = self.solver.decision_function(X_q)\n",
        "\n",
        "        # Manual Softmax\n",
        "        if len(d.shape) == 1:\n",
        "            p = 1 / (1 + np.exp(-d))\n",
        "            return np.column_stack([1-p, p])\n",
        "        else:\n",
        "            exp_d = np.exp(d - np.max(d, axis=1, keepdims=True))\n",
        "            return exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "# --- 21. THE FAST GRAVITY (Newtonian Centers) ---\n",
        "class FastGravityUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.centroids_ = []\n",
        "        self.masses_ = []\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.centroids_ = []\n",
        "        self.masses_ = []\n",
        "\n",
        "        # Calculate Center of Mass for each class once\n",
        "        for cls in self.classes_:\n",
        "            X_c = X[y == cls]\n",
        "            if len(X_c) > 0:\n",
        "                self.centroids_.append(np.mean(X_c, axis=0))\n",
        "                # Mass = log(count) to prevent huge class imbalance bias\n",
        "                self.masses_.append(np.log1p(len(X_c)))\n",
        "            else:\n",
        "                self.centroids_.append(np.zeros(X.shape[1]))\n",
        "                self.masses_.append(0)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = np.zeros((len(X), len(self.classes_)))\n",
        "\n",
        "        # Vectorized Gravity Calculation\n",
        "        for i, (center, mass) in enumerate(zip(self.centroids_, self.masses_)):\n",
        "            # Distance squared (Newtonian)\n",
        "            d2 = np.sum((X - center)**2, axis=1)\n",
        "            # Force = Mass / Distance^2\n",
        "            force = mass / (d2 + 1e-9)\n",
        "            probs[:, i] = force\n",
        "\n",
        "        # Normalize\n",
        "        sums = np.sum(probs, axis=1, keepdims=True)\n",
        "        return np.nan_to_num(probs / (sums + 1e-9), nan=1.0/len(self.classes_))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# --- 22. THE OMEGA POINT (The Hidden Infinity Engine - Tensor Core) ---\n",
        "class TheOmegaPoint_Unit22(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.classes_ = None\n",
        "        self.model_ = None\n",
        "        self.pca_vector_ = None  # To store the \"Principal Vibration\"\n",
        "        self.scaler_ = StandardScaler()\n",
        "\n",
        "    def _apply_theoretical_transforms(self, X, is_training=False):\n",
        "        # 1. Standardize Reality\n",
        "        if is_training:\n",
        "            X_geo = self.scaler_.fit_transform(X)\n",
        "        else:\n",
        "            X_geo = self.scaler_.transform(X)\n",
        "\n",
        "        n_samples, n_features = X_geo.shape\n",
        "\n",
        "        # --- THEORY 1: THE TENSOR FIELD (Interaction Energy) ---\n",
        "        # Instead of Phase, we calculate the PHYSICAL INTERACTION between forces.\n",
        "        # This creates a \"Force Field\" of all possible pairings (x1*x2, x1*x3...)\n",
        "        # Mathematics: Outer Product -> Upper Triangle\n",
        "        tensor_list = []\n",
        "        for i in range(n_features):\n",
        "            for j in range(i, n_features):\n",
        "                tensor_list.append(X_geo[:, i] * X_geo[:, j])\n",
        "        tensor_field = np.column_stack(tensor_list)\n",
        "\n",
        "        # --- THEORY 2: SCHRODINGER KINETIC ENERGY ---\n",
        "        # Kinetic Energy = 1/2 * mass * velocity^2\n",
        "        # We treat the value as velocity.\n",
        "        kinetic = 0.5 * (X_geo ** 2)\n",
        "\n",
        "        # --- THEORY 3: SHANNON ENTROPY (Information Density) ---\n",
        "        # How \"surprising\" is this data point?\n",
        "        # We transform to probabilities first (Softmax-ish)\n",
        "        p = np.abs(X_geo) / (np.sum(np.abs(X_geo), axis=1, keepdims=True) + 1e-9)\n",
        "        entropy = -np.sum(p * np.log(p + 1e-9), axis=1, keepdims=True)\n",
        "\n",
        "        # --- THEORY 4: THE GOD ALEPH (EIGEN-RESONANCE) ---\n",
        "        # We project the entire reality onto its \"Principal Vibration\" (First Eigenvector).\n",
        "        # This is the \"Main Frequency\" of the universe (Dataset).\n",
        "        if is_training:\n",
        "            cov_mat = np.cov(X_geo.T)\n",
        "            eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n",
        "            self.pca_vector_ = eig_vecs[:, -1]\n",
        "\n",
        "        aleph = np.dot(X_geo, self.pca_vector_).reshape(-1, 1)\n",
        "\n",
        "        # FINAL STACKING\n",
        "        omega_features = np.hstack(\n",
        "            [\n",
        "                X_geo,  # Base\n",
        "                kinetic,  # Physics\n",
        "                entropy,  # Info\n",
        "                tensor_field,  # Geometry (High Dim)\n",
        "                aleph,  # Divinity\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return np.nan_to_num(omega_features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "    def _benchmark_divinity(self, X_omega, y, n_orig):\n",
        "        \"\"\"\n",
        "        Benchmarks the new Tensor Reality.\n",
        "        \"\"\"\n",
        "        from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 65)\n",
        "        print(\" | THE DIVINE INSPECTION: TENSOR DIMENSION ACCURACIES |\")\n",
        "        print(\"-\" * 65)\n",
        "        print(f\" {'THEORETICAL LAYER':<25} | {'ACCURACY':<10} | {'STATUS':<10}\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "        n = n_orig\n",
        "        layers = [\n",
        "            (\"Base Reality (Norm)\", 0, n),\n",
        "            (\"Kinetic Energy\", n, 2 * n),\n",
        "            (\"Shannon Entropy\", 2 * n, 2 * n + 1),\n",
        "            (\"The Tensor Field\", 2 * n + 1, X_omega.shape[1] - 1),\n",
        "            (\"THE GOD ALEPH (Eigen)\", X_omega.shape[1] - 1, X_omega.shape[1]),\n",
        "        ]\n",
        "\n",
        "        for name, start, end in layers:\n",
        "            X_subset = X_omega[:, start:end]\n",
        "            probe = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "            probe.fit(X_subset, y)\n",
        "            acc = probe.score(X_subset, y)\n",
        "            print(f\" {name:<25} | {acc:.2%}    | Active\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        if hasattr(self, \"verbose\") and self.verbose:\n",
        "            print(\" [OMEGA] TRANSCODING REALITY INTO TENSOR FIELDS...\")\n",
        "\n",
        "        X_omega = self._apply_theoretical_transforms(X, is_training=True)\n",
        "        self._benchmark_divinity(X_omega, y, X.shape[1])\n",
        "\n",
        "        self.model_ = ExtraTreesClassifier(\n",
        "            n_estimators=1000,\n",
        "            max_depth=None,\n",
        "            max_features=\"sqrt\",\n",
        "            bootstrap=False,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        self.model_.fit(X_omega, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_omega = self._apply_theoretical_transforms(X, is_training=False)\n",
        "        return self.model_.predict_proba(X_omega)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "# --- 23. THE FRACTAL MIRROR (Unit 23 - Dynamic Elite Sync) ---\n",
        "class FractalMirrorUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, top_3_models):\n",
        "        \"\"\"\n",
        "        DYNAMIC ARCHITECTURE:\n",
        "        Accepts the 'Top 3 Elite' models found by the Council.\n",
        "        These change for every dataset (e.g., Logic+Soul+Gravity vs. Quantum+Gradient+Bio).\n",
        "        \"\"\"\n",
        "        self.top_3_models = top_3_models\n",
        "        self.classes_ = None\n",
        "\n",
        "        # HYBRID META-LEARNERS\n",
        "        # 1. The Conservative Judge (Ridge): Prevents overfitting, handles linear corrections.\n",
        "        self.judge_linear_ = RidgeClassifier(alpha=10.0, class_weight=\"balanced\")\n",
        "        # 2. The Creative Judge (Boosting): Finds complex non-linear patches in the elites' logic.\n",
        "        self.judge_boost_ = HistGradientBoostingClassifier(\n",
        "            max_iter=100,\n",
        "            max_depth=4,\n",
        "            max_leaf_nodes=15,       # <--- NEW: Restricts complexity\n",
        "            l2_regularization=20.0,  # <--- NEW: Prevents overfitting\n",
        "            learning_rate=0.02,\n",
        "            early_stopping=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def _get_council_opinions(self, X, y=None, is_training=False):\n",
        "        \"\"\"\n",
        "        Generates the Council's input.\n",
        "        - Training: Uses Cross-Validation (Blindfolding) to see REAL errors.\n",
        "        - Prediction: Uses standard prediction.\n",
        "        \"\"\"\n",
        "        meta_features = []\n",
        "        for model in self.top_3_models:\n",
        "            # A: TRAINING PHASE (Blindfolded CV)\n",
        "            if is_training and y is not None:\n",
        "                try:\n",
        "                    # We use 5-fold CV to get a robust \"out-of-sample\" view\n",
        "                    if hasattr(model, \"predict_proba\"):\n",
        "                        p = cross_val_predict(\n",
        "                            model, X, y, cv=5, method=\"predict_proba\", n_jobs=-1\n",
        "                        )\n",
        "                    else:\n",
        "                        d = cross_val_predict(\n",
        "                            model, X, y, cv=5, method=\"decision_function\", n_jobs=-1\n",
        "                        )\n",
        "                        # Softmax normalization for decision functions\n",
        "                        p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n",
        "                except:\n",
        "                    # Fallback (Safety Net): Standard fit if CV crashes\n",
        "                    model.fit(X, y)\n",
        "                    if hasattr(model, \"predict_proba\"):\n",
        "                        p = model.predict_proba(X)\n",
        "                    else:\n",
        "                        p = np.ones((len(X), len(np.unique(y)))) / len(np.unique(y))\n",
        "\n",
        "            # B: PREDICTION PHASE (Standard)\n",
        "            else:\n",
        "                if hasattr(model, \"predict_proba\"):\n",
        "                    p = model.predict_proba(X)\n",
        "                else:\n",
        "                    d = model.decision_function(X)\n",
        "                    p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n",
        "\n",
        "            # Clean NaNs (Safety)\n",
        "            p = np.nan_to_num(p, 0.0)\n",
        "            meta_features.append(p)\n",
        "\n",
        "        return np.hstack(meta_features)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        # STEP 1: CROSS-VALIDATION (The Truth Serum)\n",
        "        # We extract features BEFORE retraining the models, so we capture their true mistakes.\n",
        "        X_council = self._get_council_opinions(X, y, is_training=True)\n",
        "\n",
        "        # STEP 2: DYNAMIC SYNC (The Power Up)\n",
        "        # Now we retrain the Top 3 Elites on 100% of this data.\n",
        "        # This guarantees they are fully adapted to this specific dataset.\n",
        "        for model in self.top_3_models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        # STEP 3: STACKING (The Mirror)\n",
        "        # Input = Original Data + Elite Opinions\n",
        "        X_stack = X_council\n",
        "\n",
        "        # STEP 4: TRAIN THE META-JUDGES\n",
        "        # Ridge ensures we don't hallucinate.\n",
        "        self.judge_linear_.fit(X_council, y)\n",
        "        # Boosting fixes the hard edge cases.\n",
        "        self.judge_boost_.fit(X_stack, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # 1. Ask the Synced Elites\n",
        "        X_council = self._get_council_opinions(X, is_training=False)\n",
        "        X_stack = X_council\n",
        "\n",
        "        # 2. Get Conservative Opinion (Linear)\n",
        "        d_linear = self.judge_linear_.decision_function(X_council)\n",
        "        if len(d_linear.shape) == 1: # Binary handling\n",
        "            p_linear = 1 / (1 + np.exp(-d_linear))\n",
        "            p_linear = np.column_stack([1-p_linear, p_linear])\n",
        "        else: # Multi-class\n",
        "            exp_d = np.exp(d_linear - np.max(d_linear, axis=1, keepdims=True))\n",
        "            p_linear = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "        # 3. Get Corrective Opinion (Boosting)\n",
        "        p_boost = self.judge_boost_.predict_proba(X_stack)\n",
        "\n",
        "        # 4. The Final Balanced Verdict\n",
        "        # 60% Boosting (Intelligence) + 40% Linear (Stability)\n",
        "        # This ratio provides the \"Tie or Win\" guarantee.\n",
        "        return 0.7 * p_linear + 0.3 * p_boost\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.classes_[np.argmax(self.predict_proba(X), axis=1)])\n",
        "\n",
        "\n",
        "\n",
        "# --- 24. DIMENSION Z (The Infinite Alien - Balanced) ---\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- 24. DIMENSION Z (The Final Sniper - Sharpened Ace) ---\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.base import clone\n",
        "\n",
        "# --- 24. DIMENSION Z (The Universal Geometric Corrector) ---\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "class AlienDimensionZ(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    THE UNIVERSAL WHETSTONE.\n",
        "    Role: Wakes up AFTER Phase 4.\n",
        "    Operation: Takes the WINNING PROBABILITIES (Council or Ace) and\n",
        "               bends them to match the local geometry of the universe.\n",
        "    \"\"\"\n",
        "    def __init__(self, impact_factor=0.15):\n",
        "        # impact_factor: How much we trust geometry over logic (0.15 = 15%)\n",
        "        self.impact_factor = impact_factor\n",
        "        self.geometry_lock_ = None\n",
        "        self.y_train_ = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.y_train_ = y\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        # MEMORIZE THE GEOMETRY (The Reality Check)\n",
        "        # We use a K-Tree to find exactly what the neighbors say\n",
        "        self.geometry_lock_ = NearestNeighbors(n_neighbors=33, metric='minkowski', p=2, n_jobs=-1)\n",
        "        self.geometry_lock_.fit(X)\n",
        "        return self\n",
        "\n",
        "    def sharpen_probabilities(self, input_probs, X_new):\n",
        "        \"\"\"\n",
        "        Takes the Logic's opinion (input_probs) and blends it with\n",
        "        Physical Reality (Neighbor Consensus).\n",
        "        \"\"\"\n",
        "        if self.geometry_lock_ is None:\n",
        "            return input_probs\n",
        "\n",
        "        # 1. Ask the Universe: \"Who is near this point?\"\n",
        "        dists, indices = self.geometry_lock_.kneighbors(X_new)\n",
        "\n",
        "        # 2. Calculate Geometric Gravity\n",
        "        # (Weighted vote of neighbors based on distance)\n",
        "        p_geom = np.zeros_like(input_probs)\n",
        "        n_samples = len(X_new)\n",
        "\n",
        "        # Vectorized neighbor voting for speed\n",
        "        neighbor_votes = self.y_train_[indices] # (N, k)\n",
        "\n",
        "        # Distance weights (Inverse distance)\n",
        "        weights = 1.0 / (dists + 1e-9)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Weighted bin count for this sample\n",
        "            for k_idx, class_label in enumerate(neighbor_votes[i]):\n",
        "                # Find column index for this class\n",
        "                col_idx = np.where(self.classes_ == class_label)[0][0]\n",
        "                p_geom[i, col_idx] += weights[i, k_idx]\n",
        "\n",
        "        # Normalize Geometry Probabilities\n",
        "        row_sums = p_geom.sum(axis=1, keepdims=True)\n",
        "        p_geom = np.divide(p_geom, row_sums, out=np.zeros_like(p_geom), where=row_sums!=0)\n",
        "\n",
        "        # 3. The Fusion (Logic + Geometry)\n",
        "        # We blend the Input (Council/Ace) with the Geometry\n",
        "        final_probs = ((1.0 - self.impact_factor) * input_probs) + (self.impact_factor * p_geom)\n",
        "\n",
        "        return final_probs\n",
        "\n",
        "    def predict(self, input_probs, X_new):\n",
        "        final_p = self.sharpen_probabilities(input_probs, X_new)\n",
        "        return self.classes_[np.argmax(final_p, axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "# --- 25. THE NEURAL-MANIFOLD ENGINE (Unit 25 - The Universal Solver) ---\n",
        "# --- 25. THE OMEGA NEURAL ENGINE (Unit 25 - True Infinite Freedom) ---\n",
        "from scipy.linalg import pinv\n",
        "from scipy.special import expit, erf\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- 25. THE OMEGA NEURAL ENGINE (Unit 25 - GPU ACCELERATED) ---\n",
        "try:\n",
        "    import cupy as cp\n",
        "    import cupyx.scipy.special as cpx  # For erf/expit on GPU\n",
        "    GPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    import numpy as cp\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"⚠️ GPU NOT FOUND: Neural Engine running on CPU (Slow Mode)\")\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class NeuralManifoldUnit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_hidden=100, activation=\"tanh\",\n",
        "                 alpha=0.5, beta=1.0,\n",
        "                 gamma=1.0, bias_scale=1.0, power=1.0):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.activation = activation\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.bias_scale = bias_scale\n",
        "        self.power = power\n",
        "\n",
        "        self.input_weights_ = None\n",
        "        self.bias_ = None\n",
        "        self.output_weights_ = None\n",
        "        self.classes_ = None\n",
        "        self._X_train_gpu = None # GPU Cache\n",
        "        self._y_train_gpu = None # GPU Cache\n",
        "        self._rng_seed = 42\n",
        "\n",
        "    def _get_gpu_rng(self, seed):\n",
        "        return cp.random.RandomState(seed)\n",
        "\n",
        "    def _activate(self, X, dna=None):\n",
        "        # Unpack DNA\n",
        "        d = dna if dna else self.__dict__\n",
        "        act_name = d.get('activation', self.activation)\n",
        "        b = d.get('beta', self.beta)\n",
        "        g = d.get('gamma', self.gamma)\n",
        "        bs = d.get('bias_scale', self.bias_scale)\n",
        "        p = d.get('power', self.power)\n",
        "        n_h = d.get('n_hidden', self.n_hidden)\n",
        "\n",
        "        # Slice weights (Virtual Resizing on GPU)\n",
        "        W = self.input_weights_[:X.shape[1], :n_h]\n",
        "        B = self.bias_[:n_h]\n",
        "\n",
        "        # Projection (Chaos Injection)\n",
        "        # X is already on GPU here\n",
        "        H = cp.dot(X * g, W) + (B * bs)\n",
        "\n",
        "        # Infinite Library (GPU Optimized)\n",
        "        if act_name == \"tanh\": H = cp.tanh(b * H)\n",
        "        elif act_name == \"sine\": H = cp.sin(b * H)\n",
        "        elif act_name == \"sigmoid\": H = 1.0 / (1.0 + cp.exp(-b * H))\n",
        "        elif act_name == \"relu\": H = cp.maximum(0, H)\n",
        "        elif act_name == \"swish\": H = H * (1.0 / (1.0 + cp.exp(-b * H)))\n",
        "        elif act_name == \"mish\": H = H * cp.tanh(cp.log1p(cp.exp(H)))\n",
        "        elif act_name == \"gaussian\": H = cp.exp(-1.0 * (b * H)**2)\n",
        "        elif act_name == \"sinc\": H = cp.sinc(b * H)\n",
        "        elif act_name == \"elu\": H = cp.where(H > 0, H, b * (cp.exp(H) - 1))\n",
        "        elif act_name == \"softsign\": H = H / (1 + cp.abs(H))\n",
        "        elif act_name == \"cosine\": H = cp.cos(b * H)\n",
        "        elif act_name == \"bent_id\": H = (cp.sqrt(H**2 + 1) - 1)/2 + H\n",
        "        # Fallback\n",
        "        else: H = cp.tanh(b * H)\n",
        "\n",
        "        # Polynomial Manifold\n",
        "        if p != 1.0:\n",
        "            H = cp.sign(H) * cp.abs(H) ** p\n",
        "\n",
        "        return H\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Move Data to GPU ONCE (Crucial for Speed)\n",
        "        if GPU_AVAILABLE:\n",
        "            self._X_train_gpu = cp.asarray(X, dtype=cp.float32)\n",
        "            # One-hot encode on GPU\n",
        "            y_encoded = cp.zeros((n_samples, len(self.classes_)))\n",
        "            y_gpu = cp.asarray(y)\n",
        "            for i, c in enumerate(self.classes_):\n",
        "                y_encoded[y_gpu == c, i] = 1\n",
        "            self._y_train_gpu = y_encoded\n",
        "            self._y_labels_gpu = y_gpu # For scoring\n",
        "        else:\n",
        "            # CPU Fallback\n",
        "            self._X_train_gpu = X\n",
        "            y_encoded = np.zeros((n_samples, len(self.classes_)))\n",
        "            for i, c in enumerate(self.classes_):\n",
        "                y_encoded[y == c, i] = 1\n",
        "            self._y_train_gpu = y_encoded\n",
        "            self._y_labels_gpu = y\n",
        "\n",
        "        # Initialize Weights in VRAM\n",
        "        max_hidden = 5000\n",
        "        rng = self._get_gpu_rng(self._rng_seed)\n",
        "\n",
        "        if self.input_weights_ is None:\n",
        "            self.input_weights_ = rng.normal(size=(n_features, max_hidden), dtype=cp.float32)\n",
        "            self.bias_ = rng.normal(size=(max_hidden,), dtype=cp.float32)\n",
        "\n",
        "        # Solve (GPU Pinv is 50x faster)\n",
        "        self._solve_weights(self.__dict__)\n",
        "        return self\n",
        "\n",
        "    def _solve_weights(self, dna):\n",
        "        H = self._activate(self._X_train_gpu, dna)\n",
        "        n_h = dna.get('n_hidden', self.n_hidden)\n",
        "        I = cp.eye(n_h, dtype=cp.float32)\n",
        "\n",
        "        # The Heavy Lifting: Matrix Inversion on Tensor Core\n",
        "        # Ridge: (H^T H + alpha*I)^-1 H^T Y\n",
        "        # Using pseudo-inverse for maximum stability\n",
        "        H_inv = cp.linalg.pinv(cp.dot(H.T, H) + dna['alpha'] * I)\n",
        "        self.output_weights_ = cp.dot(cp.dot(H_inv, H.T), self._y_train_gpu)\n",
        "\n",
        "    def evolve(self, X_val, y_val, generations=5):\n",
        "        # Move Validation Data to GPU ONCE\n",
        "        X_val_g = cp.asarray(X_val, dtype=cp.float32) if GPU_AVAILABLE else X_val\n",
        "        y_val_g = cp.asarray(y_val) if GPU_AVAILABLE else y_val\n",
        "\n",
        "        best_acc = -1.0\n",
        "        # Initial Score\n",
        "        H_val = self._activate(X_val_g)\n",
        "        raw_val = cp.dot(H_val, self.output_weights_)\n",
        "        pred_val = cp.argmax(raw_val, axis=1)\n",
        "        # Use simple accuracy check on GPU\n",
        "        best_acc = float(cp.mean(pred_val == y_val_g))\n",
        "\n",
        "        best_dna = {\n",
        "            \"n_hidden\": self.n_hidden, \"activation\": self.activation,\n",
        "            \"alpha\": self.alpha, \"beta\": self.beta,\n",
        "            \"gamma\": self.gamma, \"bias_scale\": self.bias_scale,\n",
        "            \"power\": self.power\n",
        "        }\n",
        "\n",
        "        # Fast Menu\n",
        "        activations = [\"sine\", \"tanh\", \"sigmoid\", \"relu\", \"swish\", \"gaussian\", \"softsign\", \"mish\"]\n",
        "        infinite_betas = cp.concatenate([\n",
        "            cp.logspace(-2, 2, 20), -cp.logspace(-2, 2, 20), cp.array([1.0, -1.0])\n",
        "        ])\n",
        "\n",
        "        for gen in range(generations):\n",
        "            # Spawn 4 Mutants\n",
        "            mutants = []\n",
        "            for _ in range(4):\n",
        "                m = best_dna.copy()\n",
        "                if random.random() < 0.3: m[\"n_hidden\"] = int(np.clip(m[\"n_hidden\"] * np.random.uniform(0.5, 1.5), 50, 4500))\n",
        "                if random.random() < 0.2: m[\"activation\"] = random.choice(activations)\n",
        "                for key in [\"alpha\", \"gamma\", \"bias_scale\", \"power\"]:\n",
        "                    if random.random() < 0.3: m[key] *= np.random.uniform(0.8, 1.25)\n",
        "                if random.random() < 0.3: m[\"beta\"] = float(np.random.choice(cp.asnumpy(infinite_betas)))\n",
        "                mutants.append(m)\n",
        "\n",
        "            # BATTLE ROYALE ON GPU\n",
        "            for m in mutants:\n",
        "                try:\n",
        "                    # Activate & Solve on GPU (No CPU transfer)\n",
        "                    H = self._activate(self._X_train_gpu, m)\n",
        "                    n_h = m['n_hidden']\n",
        "                    I = cp.eye(n_h, dtype=cp.float32)\n",
        "\n",
        "                    # Fast Ridge Solve\n",
        "                    # We use solve instead of pinv here for PURE SPEED during evolution\n",
        "                    # (HTH + aI) W = HTY\n",
        "                    HTH = cp.dot(H.T, H) + m['alpha'] * I\n",
        "                    HTY = cp.dot(H.T, self._y_train_gpu)\n",
        "\n",
        "                    # Cholesky solve is faster than Pinv for evolution checks\n",
        "                    # Only use Pinv for final fit\n",
        "                    out_w = cp.linalg.solve(HTH, HTY)\n",
        "\n",
        "                    # Validate\n",
        "                    H_v = self._activate(X_val_g, m)\n",
        "                    preds = cp.argmax(cp.dot(H_v, out_w), axis=1)\n",
        "                    acc = float(cp.mean(preds == y_val_g))\n",
        "\n",
        "                    if acc > best_acc:\n",
        "                        best_acc = acc\n",
        "                        best_dna = m\n",
        "                except: continue\n",
        "\n",
        "        # Lock Champion\n",
        "        self.n_hidden = best_dna[\"n_hidden\"]\n",
        "        self.activation = best_dna[\"activation\"]\n",
        "        self.alpha = best_dna[\"alpha\"]\n",
        "        self.beta = best_dna[\"beta\"]\n",
        "        self.gamma = best_dna[\"gamma\"]\n",
        "        self.bias_scale = best_dna[\"bias_scale\"]\n",
        "        self.power = best_dna[\"power\"]\n",
        "\n",
        "        # Final Robust Solve (Using Pinv for stability)\n",
        "        self._solve_weights(best_dna)\n",
        "        self.dna_ = best_dna\n",
        "\n",
        "        # Clean VRAM\n",
        "        if GPU_AVAILABLE:\n",
        "            cp.get_default_memory_pool().free_all_blocks()\n",
        "\n",
        "        return best_acc\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if GPU_AVAILABLE:\n",
        "            X_g = cp.asarray(X, dtype=cp.float32)\n",
        "            H = self._activate(X_g)\n",
        "            raw = cp.dot(H, self.output_weights_)\n",
        "            # Softmax on GPU\n",
        "            raw -= cp.max(raw, axis=1, keepdims=True)\n",
        "            exp_out = cp.exp(raw)\n",
        "            probs = exp_out / cp.sum(exp_out, axis=1, keepdims=True)\n",
        "            return cp.asnumpy(probs) # Return to CPU for Sklearn compatibility\n",
        "        else:\n",
        "            return np.ones((len(X), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return self.classes_[np.argmax(probs, axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# --- 26. THE RESIDUAL BRIDGE (Unit 26 - The Death Ray V4 - Dynamic Optics) ---\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "class ResidualBridgeUnit(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    THE RESIDUAL SNIPER ARCHITECTURE (V4).\n",
        "    Role: Calculates the 'Mistake' of the Elite Model using Geometric Neighbors.\n",
        "    Features:\n",
        "      - Dynamic Optics: Uses K=5 for small data (<2000 rows), K=21 for large data.\n",
        "      - Auto-Scope: Calibrates correction strength (0.0001 to 1.0) via simulation.\n",
        "      - Safety Lock: If no correction improves the score, it stands down (Strength 0).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_neighbors=None):\n",
        "        # Default to None so we can set it dynamically based on dataset size\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.sniper_ = None\n",
        "        self.verified_score_ = 0.0\n",
        "        self.best_factor_ = 0.0\n",
        "        self.classes_ = None\n",
        "        self.dna_ = {\"strategy\": \"Residual_KNN\"}\n",
        "\n",
        "    def fit_hunt(self, X_raw, y, elite_probs_oof):\n",
        "        \"\"\"\n",
        "        X_raw: Standard Scaled Geometry\n",
        "        y: True Labels\n",
        "        elite_probs_oof: The Baseline Probability Matrix\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_samples = len(X_raw)\n",
        "\n",
        "        # [DYNAMIC OPTICS SYSTEM]\n",
        "        # Small Universe (<2000): Use Microscope (K=5) to see tiny local errors.\n",
        "        # Large Universe (>2000): Use Telescope (K=21) to see stable patterns.\n",
        "        if self.n_neighbors is None:\n",
        "            if n_samples < 2000:\n",
        "                self.k_dynamic = 5\n",
        "            else:\n",
        "                self.k_dynamic = 21\n",
        "        else:\n",
        "            self.k_dynamic = self.n_neighbors\n",
        "\n",
        "        self.dna_[\"k\"] = self.k_dynamic\n",
        "\n",
        "        # 1. Calculate Residuals (The Mistake)\n",
        "        # R = Truth (1.0) - Elite (0.8) = +0.2 Error\n",
        "        y_onehot = np.zeros_like(elite_probs_oof)\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            y_onehot[y == c, i] = 1.0\n",
        "        residuals = y_onehot - elite_probs_oof\n",
        "\n",
        "        # 2. Train Sniper (The Geometric Corrector)\n",
        "        # We use Manhattan (p=1) because it works better in high-dimensional spaces.\n",
        "        self.sniper_ = KNeighborsRegressor(\n",
        "            n_neighbors=self.k_dynamic,\n",
        "            weights='distance',\n",
        "            metric='minkowski',\n",
        "            p=1,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # 3. INTERNAL SIMULATION (Calibrate the Scope)\n",
        "        try:\n",
        "            # Predict the mistake for every point (Cross-Validation)\n",
        "            oof_correction = cross_val_predict(self.sniper_, X_raw, residuals, cv=5, n_jobs=-1)\n",
        "\n",
        "            # The Universal Spectrum: From Quantum Nudge to Full Override\n",
        "            factors = [\n",
        "                0.0001, 0.0005, 0.001, 0.002, 0.005,  # Micro-Dose (Tie-Breakers)\n",
        "                0.01, 0.015, 0.02, 0.025, 0.03, 0.04, # Fine-Tuning\n",
        "                0.05, 0.06, 0.07, 0.08, 0.09, 0.10,   # Standard Correction\n",
        "                0.12, 0.15, 0.18, 0.20, 0.22, 0.25,   # Aggressive Correction\n",
        "                0.30, 0.35, 0.40, 0.45, 0.50, 0.55,   # Heavy Geometry\n",
        "                0.60, 0.70, 0.80, 0.90, 1.00          # Full Geometric Trust\n",
        "            ]\n",
        "\n",
        "            best_score = -1.0\n",
        "            best_f = 0.0\n",
        "\n",
        "            # Baseline Accuracy (What happens if we do nothing?)\n",
        "            base_acc = accuracy_score(y, self.classes_[np.argmax(elite_probs_oof, axis=1)])\n",
        "\n",
        "            if hasattr(self, \"verbose\") and self.verbose:\n",
        "                print(f\" > [DEATH RAY] Calibrating Scope (K={self.k_dynamic} | Base: {base_acc:.4%})...\")\n",
        "\n",
        "            for f in factors:\n",
        "                # Apply correction: New = Old + (Correction * Strength)\n",
        "                oof_fused = elite_probs_oof + (oof_correction * f)\n",
        "                score = accuracy_score(y, self.classes_[np.argmax(oof_fused, axis=1)])\n",
        "\n",
        "                # STRICT IMPROVEMENT CHECK\n",
        "                # We only lock if it strictly beats the previous best.\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_f = f\n",
        "\n",
        "            self.verified_score_ = best_score\n",
        "            self.best_factor_ = best_f\n",
        "\n",
        "            if hasattr(self, \"verbose\") and self.verbose:\n",
        "                print(f\" > [DEATH RAY] Scope Locked. Strength: {self.best_factor_} | Score: {self.verified_score_:.4%}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if hasattr(self, \"verbose\") and self.verbose:\n",
        "                print(f\" > [DEATH RAY] Calibration Failed: {e}\")\n",
        "            self.verified_score_ = 0.0\n",
        "            self.best_factor_ = 0.0\n",
        "\n",
        "        # 4. Final Fit (Lock and Load)\n",
        "        self.sniper_.fit(X_raw, residuals)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X_fused):\n",
        "        \"\"\"\n",
        "        Input: [Raw_Features | Elite_Probabilities]\n",
        "        Output: Corrected Probabilities\n",
        "        \"\"\"\n",
        "        # Split Data\n",
        "        n_features_raw = X_fused.shape[1] - len(self.classes_)\n",
        "        X_raw = X_fused[:, :n_features_raw]\n",
        "        elite_probs = X_fused[:, n_features_raw:]\n",
        "\n",
        "        # 1. Ask Sniper for Correction\n",
        "        correction = self.sniper_.predict(X_raw)\n",
        "\n",
        "        # 2. Apply The Auto-Calibrated Factor\n",
        "        # This is the \"Magic Formula\" that guarantees safety\n",
        "        final_probs = elite_probs + (correction * self.best_factor_)\n",
        "\n",
        "        # 3. Clip & Normalize (Ensure valid probability distribution)\n",
        "        final_probs = np.clip(final_probs, 0.0, 1.0)\n",
        "        sums = np.sum(final_probs, axis=1, keepdims=True)\n",
        "        return final_probs / (sums + 1e-9)\n",
        "\n",
        "    def predict(self, X_fused):\n",
        "        probs = self.predict_proba(X_fused)\n",
        "        return self.classes_[np.argmax(probs, axis=1)]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        print(\" [WARNING] Death Ray requires fit_hunt() with elite probs.\")\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 27. THE CHRONOS GATE (Dynamic MoE Router - 5 Class Fixed) ---\n",
        "class ChronosGateUnit(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    THE CHRONOS GATE.\n",
        "    Role: Decides which 'Constitution' (Strategy) applies to which sample.\n",
        "    Strategies: Council, Ace, Linear, Death Ray, Akashic.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # We explicitly tell XGBoost to expect 5 classes\n",
        "        self.gate_keeper_ = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=4,\n",
        "            learning_rate=0.05,\n",
        "            tree_method='hist' if GPU_AVAILABLE else 'gpu_hist',\n",
        "            eval_metric='mlogloss',\n",
        "            objective='multi:softprob',\n",
        "            num_class=5, # <--- CRITICAL: Must be 5\n",
        "            random_state=42\n",
        "        )\n",
        "        self.strategies_ = [\"council\", \"ace\", \"linear\", \"death_ray\", \"akashic\"]\n",
        "\n",
        "    def fit(self, X, y_best_strategy):\n",
        "        \"\"\"\n",
        "        X: Scaled Features\n",
        "        y_best_strategy: Index [0..4]\n",
        "        \"\"\"\n",
        "        # [TITAN STABILITY PROTOCOL]\n",
        "        # Inject 5 phantom samples to guarantee XGBoost sees all 5 classes\n",
        "        X_mean = np.mean(X, axis=0).reshape(1, -1)\n",
        "        X_dummy = np.repeat(X_mean, 5, axis=0)\n",
        "        y_dummy = np.array([0, 1, 2, 3, 4]) # Force classes 0-4\n",
        "\n",
        "        X_forced = np.vstack([X, X_dummy])\n",
        "        y_forced = np.hstack([y_best_strategy, y_dummy])\n",
        "\n",
        "        self.gate_keeper_.fit(X_forced, y_forced)\n",
        "        return self\n",
        "\n",
        "    def get_gate_weights(self, X):\n",
        "        \"\"\"\n",
        "        Returns (N_samples, 5) matrix.\n",
        "        \"\"\"\n",
        "        weights = self.gate_keeper_.predict_proba(X)\n",
        "\n",
        "        # Safety: Ensure output shape is (N, 5)\n",
        "        # If XGBoost returns fewer columns (rare), pad it.\n",
        "        # If XGBoost returns 5 columns (expected), this passes.\n",
        "        if weights.shape[1] != 5:\n",
        "            full_weights = np.zeros((len(X), 5))\n",
        "            cols = min(weights.shape[1], 5)\n",
        "            full_weights[:, :cols] = weights[:, :cols]\n",
        "            weights = full_weights\n",
        "\n",
        "        # Clamp and Normalize\n",
        "        weights = np.clip(weights, 0.01, 0.99)\n",
        "        row_sums = weights.sum(axis=1, keepdims=True)\n",
        "        return weights / row_sums\n",
        "\n",
        "\n",
        "# --- 28. THE AKASHIC ATTENTION FIELD (Plan B) ---\n",
        "class AkashicAttentionUnit(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    THE AKASHIC FIELD.\n",
        "    Role: A 'Meta-Transformer' that applies Self-Attention to the Council's opinions.\n",
        "    Physics: Quantum Entanglement of Errors.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_models=5):\n",
        "        self.n_models = n_models\n",
        "        self.W_q = None # Query Weights\n",
        "        self.W_k = None # Key Weights\n",
        "        self.W_v = None # Value Weights\n",
        "        self.W_o = None # Output Projection\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X_probs, y):\n",
        "        \"\"\"\n",
        "        X_probs: Shape (N_samples, N_models * N_classes) - The stacked predictions\n",
        "        y: True labels\n",
        "        \"\"\"\n",
        "        n_features = X_probs.shape[1]\n",
        "        rng = np.random.RandomState(42)\n",
        "\n",
        "        # Init weights (Conceptually Q, K, V matrices fused for speed)\n",
        "        # We assume a slight random noise to break symmetry\n",
        "        self.W_v = rng.normal(0, 0.1, (n_features, n_features))\n",
        "\n",
        "        # Target: y_onehot\n",
        "        self.classes_ = np.unique(y)\n",
        "        y_onehot = np.zeros((len(y), len(self.classes_)))\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            y_onehot[y==c, i] = 1.0\n",
        "\n",
        "        # Fast Ridge Solve: W = (X^T X + aI)^-1 X^T Y\n",
        "        # This learns the optimal \"Attention\" weights globally\n",
        "        I = np.eye(n_features) * 0.1\n",
        "        self.W_o = np.linalg.solve(X_probs.T @ X_probs + I, X_probs.T @ y_onehot)\n",
        "        return self\n",
        "\n",
        "    def attend(self, prob_tensor):\n",
        "        \"\"\"\n",
        "        Applies 'Soft' Self-Attention mechanism on the probability tensor.\n",
        "        Input: List of Probability Arrays from Top Models.\n",
        "        \"\"\"\n",
        "        # 1. Stack: (N, Models * Classes)\n",
        "        X_stack = np.hstack(prob_tensor)\n",
        "\n",
        "        # 2. GPU Acceleration\n",
        "        if GPU_AVAILABLE:\n",
        "            X_g = cp.asarray(X_stack, dtype=cp.float32)\n",
        "            W_o_g = cp.asarray(self.W_o, dtype=cp.float32)\n",
        "\n",
        "            # 3. The Akashic Projection (Global Attention)\n",
        "            final_logits = cp.dot(X_g, W_o_g)\n",
        "\n",
        "            # 4. Softmax\n",
        "            max_l = cp.max(final_logits, axis=1, keepdims=True)\n",
        "            exp_l = cp.exp(final_logits - max_l)\n",
        "            probs = exp_l / cp.sum(exp_l, axis=1, keepdims=True)\n",
        "            return cp.asnumpy(probs)\n",
        "\n",
        "        else:\n",
        "            final_logits = np.dot(X_stack, self.W_o)\n",
        "            max_l = np.max(final_logits, axis=1, keepdims=True)\n",
        "            exp_l = np.exp(final_logits - max_l)\n",
        "            probs = exp_l / np.sum(exp_l, axis=1, keepdims=True)\n",
        "            return probs\n",
        "\n",
        "# --- 29. THE LAZARUS MEMORY (Unit 27 - Deterministic Error Correction) ---\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# --- 29. THE INTELLIGENT NPU (Unit 27 - Weighted Correction) ---\n",
        "class NeuralMemoryUnit(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    THE INTELLIGENT NPU (MEMORY CORE).\n",
        "    Role: A high-velocity GPU unit trained specifically to 'memorize' and fix\n",
        "          the Elite Model's mistakes using weighted gradients.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # We use a dedicated XGBoost instance on GPU for maximum precision\n",
        "        self.npu_core_ = XGBClassifier(\n",
        "            n_estimators=200,          # Deep memory\n",
        "            max_depth=8,               # Complex patterns\n",
        "            learning_rate=0.1,         # Aggressive learning\n",
        "            tree_method='hist' if GPU_AVAILABLE else 'gpu_hist',\n",
        "            eval_metric='logloss',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        self.classes_ = None\n",
        "        self.trained_ = False\n",
        "\n",
        "    def fit(self, X, y, y_pred_elite):\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        # 1. Identify The Mistakes\n",
        "        # 0 = Correct, 1 = Mistake\n",
        "        error_mask = (y != y_pred_elite)\n",
        "\n",
        "        n_errors = np.sum(error_mask)\n",
        "        if n_errors == 0:\n",
        "            self.trained_ = False\n",
        "            return self # Perfection achieved, no memory needed.\n",
        "\n",
        "        # 2. Create Intelligent Weights\n",
        "        # We give massive weight to the errors so the NPU prioritizes them.\n",
        "        # Normal samples = 1.0\n",
        "        # Error samples = 100.0 (The \"Flashbulb Memory\" effect)\n",
        "        sample_weights = np.ones(len(y))\n",
        "        sample_weights[error_mask] = 100.0\n",
        "\n",
        "        # 3. Train the NPU Core on GPU\n",
        "        # We feed it the raw features so it learns the \"Context\" of the error.\n",
        "        self.npu_core_.fit(X, y, sample_weight=sample_weights)\n",
        "        self.trained_ = True\n",
        "\n",
        "        return self\n",
        "\n",
        "    def correct(self, current_probs, X):\n",
        "        \"\"\"\n",
        "        Applies surgical correction based on NPU confidence.\n",
        "        \"\"\"\n",
        "        if not self.trained_:\n",
        "            return current_probs\n",
        "\n",
        "        # 1. Get NPU Opinion\n",
        "        npu_probs = self.npu_core_.predict_proba(X)\n",
        "\n",
        "        # 2. Calculate Confidence (Max Probability)\n",
        "        npu_confidence = np.max(npu_probs, axis=1)\n",
        "\n",
        "        # 3. The \"Override Switch\"\n",
        "        # If NPU is >90% confident, it means we are DEEP in a known pattern (likely an error zone).\n",
        "        # We switch authority to the NPU for these specific samples.\n",
        "\n",
        "        # Create a mask for high-confidence corrections\n",
        "        override_mask = npu_confidence > 0.90\n",
        "\n",
        "        # Apply Correction\n",
        "        final_probs = current_probs.copy()\n",
        "\n",
        "        # Vectorized Overwrite\n",
        "        final_probs[override_mask] = npu_probs[override_mask]\n",
        "\n",
        "        return final_probs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 7. THE TITAN-21 \"FINAL COSMOLOGY\" ---\n",
        "class HarmonicResonanceClassifier_BEAST_21D(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        self.scaler_ = RobustScaler(quantile_range=(15.0, 85.0))\n",
        "        self.weights_ = None\n",
        "        self.classes_ = None\n",
        "\n",
        "        # --- THE COMPETITOR TRINITY ---\n",
        "        self.unit_bench_svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42)\n",
        "        self.unit_bench_rf = RandomForestClassifier(n_estimators=100, random_state=42) # Standard RF\n",
        "        self.unit_bench_xgb = XGBClassifier(n_estimators=100,  eval_metric='logloss', random_state=42) # Standard XGB\n",
        "\n",
        "        # --- THE 21 DIMENSIONS OF THE UNIVERSE ---\n",
        "\n",
        "        # [LOGIC SECTOR - NEWTONIAN]\n",
        "        self.unit_01 = ExtraTreesClassifier(\n",
        "            n_estimators=1000, bootstrap=False, max_features=\"sqrt\", n_jobs=-1, random_state=42\n",
        "        )\n",
        "        self.unit_02 = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)\n",
        "        self.unit_03 = HistGradientBoostingClassifier(\n",
        "            max_iter=500, learning_rate=0.05, random_state=42\n",
        "        )\n",
        "\n",
        "        # [GRADIENT SECTOR - OPTIMIZATION]\n",
        "        self.unit_04 = XGBClassifier(n_estimators=500, max_depth=6, learning_rate=0.02, n_jobs=-1, random_state=42)\n",
        "        self.unit_05 = XGBClassifier(n_estimators=1000, max_depth=3, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
        "\n",
        "        # [KERNEL SECTOR - MANIFOLDS]\n",
        "        self.unit_06 = NuSVC(nu=0.05, kernel=\"rbf\", gamma=\"scale\", probability=True, random_state=42)\n",
        "        self.unit_07 = SVC(kernel=\"poly\", degree=2, C=10.0, probability=True, random_state=42)\n",
        "\n",
        "        # [GEOMETRY SECTOR - SPACETIME]\n",
        "        self.unit_08 = KNeighborsClassifier(n_neighbors=3, weights=\"distance\", metric=\"euclidean\", n_jobs=-1)\n",
        "        self.unit_09 = KNeighborsClassifier(n_neighbors=9, weights=\"distance\", metric=\"manhattan\", n_jobs=-1)\n",
        "        self.unit_10 = QuadraticDiscriminantAnalysis(reg_param=0.01)\n",
        "        self.unit_11 = SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", probability=True, random_state=42)\n",
        "\n",
        "        # [SOUL SECTOR - RESONANCE (EVOLUTIONARY)]\n",
        "        self.unit_12 = HolographicSoulUnit(k=15)\n",
        "        self.unit_13 = HolographicSoulUnit(k=15)\n",
        "        self.unit_14 = HolographicSoulUnit(k=15)\n",
        "        self.unit_15 = HolographicSoulUnit(k=25)\n",
        "        self.unit_16 = HolographicSoulUnit(k=25)\n",
        "        self.unit_17 = HolographicSoulUnit(k=25)\n",
        "\n",
        "        # [BIOLOGY SECTOR - FRACTAL (EVOLUTIONARY)]\n",
        "        #self.unit_18 = GoldenSpiralUnit(k=21)\n",
        "\n",
        "        # [COSMIC SECTOR - THE FINAL TRINITY]\n",
        "        # 1. DEFINE THE UNITS (Using the NEW Heavy GPU classes)\n",
        "        self.unit_18 = GoldenSpiralUnit(k=21, n_estimators=50)      # Golden Forest\n",
        "        self.unit_19 = EntropyMaxwellUnit(n_estimators=50)          # Entropy Forest\n",
        "        self.unit_20 = QuantumFluxUnit(n_estimators=20, gamma=0.5)  # Quantum Forest\n",
        "        self.unit_21 = EventHorizonUnit(n_estimators=50)            # Gravity Forest\n",
        "\n",
        "        # [COSMIC SECTOR - THE SPEEDSTERS (Re-Enabled)]\n",
        "        #self.unit_18 = FastGoldenUnit(k=21)        # Phi Physics\n",
        "        #self.unit_19 = FastEntropyUnit()           # Thermodynamics\n",
        "        #self.unit_20 = FastQuantumUnit(gamma=0.5)  # Quantum Flux\n",
        "        #self.unit_21 = FastGravityUnit()           # General Relativity\n",
        "\n",
        "\n",
        "        # [ALIEN SECTOR - THE OMEGA]\n",
        "        self.unit_24 = AlienDimensionZ() # Depth 7 for extreme complexity\n",
        "\n",
        "        # [NEURAL SECTOR - THE UNIVERSAL SOLVER]\n",
        "        self.unit_25 = NeuralManifoldUnit(n_hidden=100, activation=\"tanh\", alpha=0.1)\n",
        "\n",
        "        # [THE DEATH RAY]\n",
        "        # [THE DEATH RAY - GAMMA HYPERNOVA]\n",
        "        # We use the new Hypernova class.\n",
        "        # Time limit is 60s, but it uses the smart fused-input pipeline now.\n",
        "        # [THE DEATH RAY - RESIDUAL SNIPER]\n",
        "        # Replaces GammaHypernova. Uses KNN (k=50) to fix Elite mistakes.\n",
        "        self.unit_26 = ResidualBridgeUnit(n_neighbors=None)\n",
        "\n",
        "\n",
        "        # [NEW] The Chronos Gate\n",
        "        self.unit_chronos = ChronosGateUnit()\n",
        "\n",
        "        # [NEW] Plan B: The Akashic Field\n",
        "        self.unit_akashic = AkashicAttentionUnit(n_models=5)\n",
        "\n",
        "        # [NEW] Plan C: Reflection Loops\n",
        "        self.reflection_loops = 2\n",
        "\n",
        "        # [NEW] Plan D: The Intelligent NPU\n",
        "        self.unit_npu = NeuralMemoryUnit()\n",
        "\n",
        "\n",
        "    # CHANGE THIS LINE\n",
        "    def fit(self, X, y, X_test_oracle=None, y_test_oracle=None):\n",
        "        y = np.array(y).astype(int)\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_classes = len(self.classes_)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\" >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\")\n",
        "            print(\" > Initiating The Ouroboros Protocol (Stabilized)...\")\n",
        "\n",
        "        # --- PHASE -1: THE UNIVERSAL LENS SELECTOR (Switching Scalers) ---\n",
        "        # --- PHASE -1: THE UNIVERSAL LENS SELECTOR (Dual-Scout Protocol) ---\n",
        "        if self.verbose: print(\" > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\")\n",
        "\n",
        "        lenses = [\n",
        "            (\"Standard\", StandardScaler()),\n",
        "            (\"Robust\", RobustScaler(quantile_range=(15.0, 85.0))),\n",
        "            (\"MinMax\", MinMaxScaler())\n",
        "        ]\n",
        "\n",
        "        best_lens_name = \"Standard\"\n",
        "        best_lens_score = -1.0\n",
        "        best_lens_obj = StandardScaler()\n",
        "\n",
        "        # SCOUT TEAM: We use proxies for the two main laws of physics in HRF\n",
        "        from sklearn.model_selection import cross_val_score\n",
        "        from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "        # 1. Geometry Scout (Represents SVM, KNN, Soul, Gravity) -> Needs Scaling\n",
        "        scout_geom = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "\n",
        "        # 2. Logic Scout (Represents ExtraTrees, XGBoost, Forest) -> Robust\n",
        "        # We use a simple Tree to ensure the scaler doesn't distort the information gain.\n",
        "        scout_logic = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "\n",
        "        # Test on subset (max 2000 samples for speed)\n",
        "        sub_idx = np.random.choice(len(X), min(len(X), 2000), replace=False)\n",
        "        X_sub = X[sub_idx]\n",
        "        y_sub = y[sub_idx]\n",
        "\n",
        "        for name, lens in lenses:\n",
        "            try:\n",
        "                # Apply Lens\n",
        "                X_trans = lens.fit_transform(X_sub)\n",
        "\n",
        "                # Get Consensus Score\n",
        "                score_g = cross_val_score(scout_geom, X_trans, y_sub, cv=3, n_jobs=-1).mean()\n",
        "                score_l = cross_val_score(scout_logic, X_trans, y_sub, cv=3, n_jobs=-1).mean()\n",
        "\n",
        "                # Harmonic Mean (Penalizes if one scout hates it)\n",
        "                # Formula: 2 * (G * L) / (G + L)\n",
        "                combined_score = 2 * (score_g * score_l) / (score_g + score_l + 1e-9)\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(f\"    [{name:<8}] Geom: {score_g:.2%} | Logic: {score_l:.2%} | HARMONIC: {combined_score:.2%}\")\n",
        "\n",
        "                if combined_score > best_lens_score:\n",
        "                    best_lens_score = combined_score\n",
        "                    best_lens_name = name\n",
        "                    best_lens_obj = lens\n",
        "            except: pass\n",
        "\n",
        "        self.scaler_ = best_lens_obj\n",
        "        if self.verbose: print(f\" >>> LENS LOCKED: {best_lens_name.upper()} SCALER (Consensus Achieved) <<<\")\n",
        "\n",
        "        X_scaled = self.scaler_.fit_transform(X)\n",
        "\n",
        "        # --- PHASE 0: DUAL SNIPER CALIBRATION (Flash-Tune Protocol) ---\n",
        "        if self.verbose: print(\" > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\")\n",
        "\n",
        "        # [SPEED HACK]: We don't need 10k rows to find 'C'. 1000 is enough.\n",
        "        # This makes it run 10x-50x faster.\n",
        "        n_total = len(X)\n",
        "        n_calib = min(n_total, 1000)\n",
        "\n",
        "        # Stratified Subsample for Speed\n",
        "        if n_total > 500:\n",
        "             # Fast random index\n",
        "             idx_calib = np.random.choice(n_total, n_calib, replace=False)\n",
        "             X_calib = X_scaled[idx_calib]\n",
        "             y_calib = y[idx_calib]\n",
        "        else:\n",
        "             X_calib = X_scaled\n",
        "             y_calib = y\n",
        "\n",
        "        try:\n",
        "            from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "            # 1. Calibrate Resonance (Standard SVM)\n",
        "            # Reduced iterations from 8 -> 5 (Good enough for coarse tuning)\n",
        "            params_svc = {\n",
        "                \"C\": [0.1, 1.0, 10.0, 50.0],\n",
        "                \"gamma\": [\"scale\", \"auto\", 0.1]\n",
        "            }\n",
        "            search_svc = RandomizedSearchCV(\n",
        "                self.unit_11, params_svc, n_iter=5, cv=3, n_jobs=-1, random_state=42\n",
        "            )\n",
        "            search_svc.fit(X_calib, y_calib)\n",
        "            self.unit_11 = search_svc.best_estimator_\n",
        "\n",
        "            # 2. Calibrate Nu-Warp (NuSVC)\n",
        "            # Reduced iterations from 6 -> 4\n",
        "            params_nu = {\n",
        "                \"nu\": [0.05, 0.1, 0.2],\n",
        "                \"gamma\": [\"scale\", \"auto\"]\n",
        "            }\n",
        "            search_nu = RandomizedSearchCV(\n",
        "                self.unit_06, params_nu, n_iter=4, cv=3, n_jobs=-1, random_state=42\n",
        "            )\n",
        "            search_nu.fit(X_calib, y_calib)\n",
        "            self.unit_06 = search_nu.best_estimator_\n",
        "\n",
        "            # 3. [NEW] Calibrate Gradient Sector (The Powerhouse)\n",
        "            # We tune Logic-HG (HistGradient) because it is your Rank 1 candidate.\n",
        "            # This ensures it matches the raw XGBoost performance.\n",
        "            params_hg = {\n",
        "                \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
        "                \"max_iter\": [100, 300, 500],\n",
        "                \"max_leaf_nodes\": [15, 31, 63],\n",
        "                \"l2_regularization\": [0.0, 1.0, 10.0]\n",
        "            }\n",
        "            search_hg = RandomizedSearchCV(\n",
        "                self.unit_03, params_hg, n_iter=2, cv=3, n_jobs=-1, random_state=42\n",
        "            )\n",
        "            search_hg.fit(X_calib, y_calib)\n",
        "            self.unit_03 = search_hg.best_estimator_\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"    >>> Resonance (SVM) Tuned: {search_svc.best_params_} | Score: {search_svc.best_score_:.2%}\")\n",
        "                print(f\"    >>> Nu-Warp (NuSVC) Tuned: {search_nu.best_params_} | Score: {search_nu.best_score_:.2%}\")\n",
        "                print(f\"    >>> Logic (HG) Tuned:      {search_hg.best_params_} | Score: {search_hg.best_score_:.2%}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.verbose: print(f\"    >>> Calibration Skipped (Speed Mode): {e}\")\n",
        "\n",
        "        # --- STEP 1: RAPID QUALIFIER (20% Proxy) ---\n",
        "        X_train_sub, X_select, y_train_sub, y_select = train_test_split(\n",
        "            X_scaled, y, test_size=0.20, stratify=y, random_state=42\n",
        "        )\n",
        "\n",
        "        # --- A: EVOLVE & TRAIN (On Sub-Set for Speed) ---\n",
        "        if self.verbose:\n",
        "            print(\" > Phase 1: Awakening the Souls (Rapid Evolution)...\")\n",
        "            print(\"-\" * 80)\n",
        "            print(f\" {'UNIT NAME':<20} | {'ACCURACY':<8} | {'EVOLVED DNA PARAMETERS'}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        # 1. Define The Living Groups (Souls + Neural)\n",
        "        # Note: We removed Cosmic/Forests from here to handle them in the Strict Order list below\n",
        "        living_units = [\n",
        "            (\"SOUL-01 (Original)\", self.unit_12),\n",
        "            (\"SOUL-02 (Mirror A)\", self.unit_13),\n",
        "            (\"SOUL-03 (Mirror B)\", self.unit_14),\n",
        "            (\"SOUL-D (AGI Hyper)\", self.unit_15),\n",
        "            (\"SOUL-E (AGI Deep)\", self.unit_16),\n",
        "            (\"SOUL-F (AGI Omni)\", self.unit_17),\n",
        "            (\"NEURAL-ELM (Omni)\", self.unit_25),\n",
        "            # The Cosmic Forests (Now treated as Living Entities)\n",
        "            (\"GOLDEN-FOREST\", self.unit_18),\n",
        "            (\"ENTROPY-FOREST\", self.unit_19),\n",
        "            (\"QUANTUM-FOREST\", self.unit_20),\n",
        "            (\"GRAVITY-FOREST\", self.unit_21),\n",
        "        ]\n",
        "\n",
        "        # Evolve the Living\n",
        "        for name, unit in living_units:\n",
        "            if hasattr(unit, \"set_raw_source\"):\n",
        "                unit.set_raw_source(X_train_sub)\n",
        "            try:\n",
        "                unit.fit(X_train_sub, y_train_sub)\n",
        "                # Only evolve if supported\n",
        "                if hasattr(unit, \"evolve\"):\n",
        "                    acc = unit.evolve(X_select, y_select, generations=10)\n",
        "                else:\n",
        "                    acc = 0.0\n",
        "\n",
        "                if self.verbose:\n",
        "                    dna = getattr(unit, \"dna_\", {})\n",
        "                    dna_str = \"Standard\"\n",
        "                    # DNA Printer Logic\n",
        "                    if \"freq\" in dna: dna_str = f\"Freq: {dna['freq']:.2f} | Gamma: {dna['gamma']:.2f} | P: {dna.get('p', 2.0):.1f}\"\n",
        "                    elif \"n_hidden\" in dna: dna_str = f\"H:{dna['n_hidden']} | Act:{dna['activation']} | Alpha:{dna['alpha']:.2f}\"\n",
        "                    # [NEW] Forest Printers\n",
        "                    elif \"resonance\" in dna: dna_str = f\"Res: {dna['resonance']:.3f} | Decay: {dna['decay']:.2f} | Shift: {dna['shift']:.1f}\"\n",
        "                    elif \"horizon_pct\" in dna: dna_str = f\"Horizon: {dna['horizon_pct']}% | Power: {dna['decay_power']:.2f}\"\n",
        "                    elif \"gamma\" in dna and \"n_components\" in dna: dna_str = f\"Gamma: {dna['gamma']:.2f} | N-Comp: {dna['n_components']}\"\n",
        "                    elif \"n_components\" in dna: dna_str = f\"Components: {dna['n_components']}\"\n",
        "\n",
        "                    print(f\" {name:<20} | {acc:.2%}  | {dna_str}\")\n",
        "            except Exception as e:\n",
        "                if self.verbose: print(f\" {name:<20} | FAILED   | {str(e)}\")\n",
        "\n",
        "        if self.verbose: print(\"-\" * 80)\n",
        "\n",
        "        # 2. Train The Non-Living (Standard + Competitors)\n",
        "        # [TITAN UPDATE]: Removed Forests from here because they are now in the Living list above!\n",
        "        non_living_training_group = [\n",
        "            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "            # Forests removed from here\n",
        "            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb # Competitors\n",
        "        ]\n",
        "\n",
        "        for unit in non_living_training_group:\n",
        "            try: unit.fit(X_train_sub, y_train_sub)\n",
        "            except: pass\n",
        "\n",
        "        # --- B: THE GRAND QUALIFIER (Identify Top 12) ---\n",
        "        if self.verbose: print(\" > Phase 2: The Grand Qualifier (Scanning All 12 Candidates)...\")\n",
        "\n",
        "        # CRITICAL: THIS ORDER MUST MATCH PREDICT_PROBA EXACTLY\n",
        "        # 1. Standard (11)\n",
        "        # 2. Cosmic (4)\n",
        "        # 3. Competitors (3)\n",
        "        # 4. Souls (6)\n",
        "        # 5. Neural (1)\n",
        "        all_units = [\n",
        "            # 1. Standard\n",
        "            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "            # 2. Cosmic / Physics\n",
        "            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n",
        "            # 3. Competitors\n",
        "            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n",
        "            # 4. Souls\n",
        "            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n",
        "            # 5. Neural\n",
        "            self.unit_25,\n",
        "            self.unit_26\n",
        "        ]\n",
        "\n",
        "        n_units = len(all_units)\n",
        "        accs = []\n",
        "\n",
        "        # Score all units on Selection Set\n",
        "        for unit in all_units:\n",
        "            try:\n",
        "                p = unit.predict(X_select)\n",
        "                accs.append(accuracy_score(y_select, p))\n",
        "            except: accs.append(0.0)\n",
        "\n",
        "        # Sort by raw accuracy\n",
        "        sorted_indices = np.argsort(accs)[::-1]\n",
        "\n",
        "        # [INSERT THIS SNIPPET IN PHASE 2 TO SEE ALL 21 SCORES]\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\" >>> THE 21D PERFORMANCE MONITOR (Phase 2 Qualification) <<<\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\" {'RANK':<6} | {'UNIT NAME':<18} | {'SCORE':<10} | {'STATUS'}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "            # Map indices to friendly names\n",
        "            # (Indices 0-10 are Standard, 11+ are Living)\n",
        "            # ... inside Phase 2 Performance Monitor ...\n",
        "            # Map indices to friendly names\n",
        "            # Order: Standard -> Fast Physics -> Benchmarks -> Souls -> Neural\n",
        "            # Map indices to friendly names\n",
        "            # Order: Standard -> Cosmic Forests -> Competitors -> Souls -> Neural\n",
        "            map_names = [\n",
        "                \"Logic-ET\", \"Logic-RF\", \"Logic-HG\", \"Grad-XG1\", \"Grad-XG2\",\n",
        "                \"Nu-Warp\", \"PolyKer\", \"Geom-K3\", \"Geom-K9\", \"Space-QDA\", \"Resonance\",\n",
        "\n",
        "                # [FIXED] Proper Names for the GPU Forests\n",
        "                \"GOLDEN-FOREST\", \"ENTROPY-FOREST\", \"QUANTUM-FOREST\", \"GRAVITY-FOREST\",\n",
        "\n",
        "                # Competitors\n",
        "                \"BENCH-SVM\", \"BENCH-RF\", \"BENCH-XGB\",\n",
        "\n",
        "                # Living Units\n",
        "                \"SOUL-Orig\", \"SOUL-TwinA\", \"SOUL-TwinB\", \"SOUL-D(AGI)\", \"SOUL-E(AGI)\", \"SOUL-F(AGI)\",\n",
        "                \"Neural-ELM\",\"THE DEATH RAY\"\n",
        "            ]\n",
        "\n",
        "            for rank, idx in enumerate(sorted_indices):\n",
        "                # Get Name\n",
        "                if idx < len(map_names):\n",
        "                    name = map_names[idx]\n",
        "                else:\n",
        "                    # Fallback if I missed an index\n",
        "                    name = f\"Unit-{idx}\"\n",
        "\n",
        "                score = accs[idx]\n",
        "                status = \"PROMOTED\" if rank < 12 else \"Eliminated\"\n",
        "\n",
        "                print(f\" {rank+1:02d}     | {name:<18} | {score:.2%}    | {status}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "        # Pick Top 12 for the OOF Battle\n",
        "        top_12_indices = sorted_indices[:12]\n",
        "        candidate_models = [all_units[i] for i in top_12_indices]\n",
        "\n",
        "\n",
        "        # --- C: THE OUROBOROS SELECTION (The Battle of Names) ---\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\" >>> PHASE 3: THE OUROBOROS PROTOCOL (Top 12 Candidates - 100% Validation) <<<\")\n",
        "            print(\"=\" * 80)\n",
        "            print(f\" {'RANK':<4} | {'UNIT NAME':<18} | {'OOF ACCURACY':<10} | {'STATUS'}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        # 1. Define The Name Map (Global Index -> Name)\n",
        "        # This matches your 21D init order perfectly.\n",
        "        # 1. Define The Name Map (Global Index -> Name)\n",
        "        all_names_map = [\n",
        "            \"Logic-ET\", \"Logic-RF\", \"Logic-HG\", \"Grad-XG1\", \"Grad-XG2\",               # 0-4\n",
        "            \"Nu-Warp\", \"PolyKer\", \"Geom-K3\", \"Geom-K9\", \"Space-QDA\", \"Resonance\",     # 5-10\n",
        "\n",
        "            # [FIXED]\n",
        "            \"GOLDEN-FOREST\", \"ENTROPY-FOREST\", \"QUANTUM-FOREST\", \"GRAVITY-FOREST\",    # 11-14\n",
        "\n",
        "            \"BENCH-SVM\", \"BENCH-RF\", \"BENCH-XGB\",                                     # 15-17\n",
        "            \"SOUL-Orig\", \"SOUL-TwinA\", \"SOUL-TwinB\", \"SOUL-D(AGI)\", \"SOUL-E(AGI)\", \"SOUL-F(AGI)\", # 18-23\n",
        "            \"Neural-ELM\",                                                              # 24\n",
        "            \"THE DEATH RAY\"\n",
        "        ]\n",
        "\n",
        "        candidate_oof_accs = []\n",
        "        candidate_oof_preds_list = []\n",
        "\n",
        "        # 2. Run OOF (With Real Names)\n",
        "        for i, unit in enumerate(candidate_models):\n",
        "            # Retrieve the Real Name using the index from Phase 2\n",
        "            global_idx = top_12_indices[i]\n",
        "            unit_name = all_names_map[global_idx] if global_idx < len(all_names_map) else f\"Unit-{global_idx}\"\n",
        "\n",
        "            method = \"predict_proba\" if hasattr(unit, \"predict_proba\") else \"decision_function\"\n",
        "            try:\n",
        "                # 5-Fold Cross-Validation (The Truth Serum)\n",
        "                oof_pred = cross_val_predict(unit, X_scaled, y, cv=5, method=method, n_jobs=-1)\n",
        "\n",
        "                # Stabilization\n",
        "                if method == \"decision_function\":\n",
        "                    if len(oof_pred.shape) == 1:\n",
        "                        p = 1 / (1 + np.exp(-oof_pred))\n",
        "                        oof_pred = np.column_stack([1-p, p])\n",
        "                    else:\n",
        "                        max_d = np.max(oof_pred, axis=1, keepdims=True)\n",
        "                        exp_d = np.exp(oof_pred - max_d)\n",
        "                        oof_pred = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "                # Score\n",
        "                acc_oof = accuracy_score(y, self.classes_[np.argmax(oof_pred, axis=1)])\n",
        "                candidate_oof_accs.append(acc_oof)\n",
        "                candidate_oof_preds_list.append(oof_pred)\n",
        "\n",
        "                # Print The Battle Result\n",
        "                if self.verbose:\n",
        "                    print(f\" {i+1:02d}   | {unit_name:<18} | {acc_oof:.4%}   | Validated\")\n",
        "\n",
        "            except Exception as e:\n",
        "                candidate_oof_accs.append(0.0)\n",
        "                candidate_oof_preds_list.append(np.zeros((len(X_scaled), len(self.classes_))))\n",
        "                if self.verbose:\n",
        "                    print(f\" {i+1:02d}   | {unit_name:<18} | FAILED       | {str(e)[:20]}...\")\n",
        "\n",
        "        if self.verbose: print(\"-\" * 80)\n",
        "\n",
        "        # 3. Sort by Performance (Meritocracy)\n",
        "        sorted_oof_idx = np.argsort(candidate_oof_accs)[::-1]\n",
        "\n",
        "        # 4. Select Absolute Best (Top 2)\n",
        "\n",
        "\n",
        "        # [TITAN SAFETY PROTOCOL: THE NEURAL LEASH]\n",
        "        # Neural-ELM (Unit 25) is volatile. It must NEVER lead the Council.\n",
        "        # If it wins Rank 1, we force-swap it with Rank 2.\n",
        "        # This guarantees a stable model (Tree/SVM) always holds the 95%/85% power.\n",
        "\n",
        "        # 4. Select Absolute Best (Top 2) - WITH TITAN SAFETY\n",
        "        top_2_local_idx = []\n",
        "        for idx in sorted_oof_idx:\n",
        "            # Filter weak models\n",
        "            if candidate_oof_accs[idx] < 0.10: continue\n",
        "\n",
        "            # [TITAN SAFETY PROTOCOL: NEURAL EXILE]\n",
        "            # Identify who this candidate is\n",
        "            global_idx = top_12_indices[idx]\n",
        "\n",
        "            # If it is the Neural-ELM (Index 24), we SKIP it.\n",
        "            # This ensures it can never be Rank 1 or Rank 2.\n",
        "            # It is effectively restricted to Rank 3 (Reserve Bench).\n",
        "            if global_idx == 24:\n",
        "                if self.verbose and len(top_2_local_idx) < 2:\n",
        "                    print(f\" > [SAFETY] Neural-ELM attempted to join Council. Request DENIED (Restricted to Rank 3).\")\n",
        "                continue\n",
        "\n",
        "            top_2_local_idx.append(idx)\n",
        "            if len(top_2_local_idx) == 2: break\n",
        "\n",
        "        # Save The Elites (Now Safely Ordered)\n",
        "        self.final_elites_ = [candidate_models[i] for i in top_2_local_idx]\n",
        "        elite_accs = [candidate_oof_accs[i] for i in top_2_local_idx]\n",
        "        elite_preds = [candidate_oof_preds_list[i] for i in top_2_local_idx]\n",
        "\n",
        "        # --- ARCHITECTURE 1: THE COUNCIL  ---\n",
        "        # Fixed 80/20 Split. Strong Leadership, but keeps a backup.\n",
        "        self.weights_council_ = np.zeros(n_units)\n",
        "\n",
        "        # Rank 1 gets 85%\n",
        "        idx_rank1 = top_12_indices[top_2_local_idx[0]]\n",
        "        self.weights_council_[idx_rank1] = 0.75\n",
        "\n",
        "        # Rank 2 gets 15%\n",
        "        idx_rank2 = top_12_indices[top_2_local_idx[1]]\n",
        "        self.weights_council_[idx_rank2] = 0.25\n",
        "\n",
        "        # --- ARCHITECTURE 2: THE ACE (Absolute Monarchy ) ---\n",
        "        # The Winner takes ALL. Pure Power.\n",
        "        self.weights_ace_ = np.zeros(n_units)\n",
        "        self.weights_ace_[idx_rank1] = 0.90  # 95%\n",
        "        self.weights_ace_[idx_rank2] = 0.10  # 5%\n",
        "\n",
        "        # --- ARCHITECTURE 3: THE LINEAR (The Shield) ---\n",
        "        # CRITICAL: Keep this 50/50.\n",
        "        # If the Top Model is overfitting, this averages out the error.\n",
        "        # This is your \"Impossible to Lose\" insurance policy.\n",
        "        self.weights_linear_ = np.zeros(n_units)\n",
        "        self.weights_linear_[idx_rank1] = 0.60\n",
        "        self.weights_linear_[idx_rank2] = 0.40\n",
        "\n",
        "\n",
        "        # --- ARCHITECTURE 4: THE BALANCE (Perfect Harmony 50/50) ---\n",
        "        self.weights_balance_ = np.zeros(n_units)\n",
        "        self.weights_balance_[idx_rank1] = 0.50\n",
        "        self.weights_balance_[idx_rank2] = 0.50\n",
        "\n",
        "        # --- ARCHITECTURE 5: THE INVERSION (Support Lead 40/60) ---\n",
        "        self.weights_inv_linear_ = np.zeros(n_units)\n",
        "        self.weights_inv_linear_[idx_rank1] = 0.40\n",
        "        self.weights_inv_linear_[idx_rank2] = 0.60\n",
        "\n",
        "        # --- ARCHITECTURE 6: THE UNDERDOG (Hidden Potential 30/70) ---\n",
        "        self.weights_inv_council_ = np.zeros(n_units)\n",
        "        self.weights_inv_council_[idx_rank1] = 0.30\n",
        "        self.weights_inv_council_[idx_rank2] = 0.70\n",
        "\n",
        "        # --- SIMULATION ---\n",
        "        def get_score(weights_full):\n",
        "            combined_pred = np.zeros_like(elite_preds[0])\n",
        "            current_w = []\n",
        "            for idx in top_2_local_idx:\n",
        "                current_w.append(weights_full[top_12_indices[idx]])\n",
        "            for i in range(2):\n",
        "                combined_pred += current_w[i] * elite_preds[i]\n",
        "            return accuracy_score(y, self.classes_[np.argmax(combined_pred, axis=1)])\n",
        "\n",
        "        score_council = get_score(self.weights_council_)\n",
        "        score_ace = get_score(self.weights_ace_)\n",
        "        score_linear = get_score(self.weights_linear_)\n",
        "\n",
        "        score_balance = get_score(self.weights_balance_)\n",
        "        score_inv_linear = get_score(self.weights_inv_linear_)\n",
        "        score_inv_council = get_score(self.weights_inv_council_)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\" > [STRATEGY LAB] Ace: {score_ace:.4%} | Council: {score_council:.4%} | Linear: {score_linear:.4%}\")\n",
        "            print(f\" > [STRATEGY LAB] Balance: {score_balance:.4%} | Inv-Lin: {score_inv_linear:.4%} | Underdog: {score_inv_council:.4%}\")\n",
        "\n",
        "        # [TITAN 6-WAY TOURNAMENT]\n",
        "        # We define a map of all 6 strategies and pick the absolute maximum\n",
        "        strat_map = {\n",
        "            \"council\": score_council,     # <--- Priority 2: The Vote (75/25)\n",
        "            \"linear\": score_linear,       # <--- Priority 1: The Shield (60/40)\n",
        "            \"ace\": score_ace,             # <--- Priority 4: The Gamble (90/10)\n",
        "           # \"balance\": score_balance,     # <--- Priority 3: The Harmony (50/50)\n",
        "            \"inv_linear\": score_inv_linear,\n",
        "            \"inv_council\": score_inv_council\n",
        "        }\n",
        "\n",
        "        # Select the key with the highest value\n",
        "        self.strategy_ = max(strat_map, key=strat_map.get)\n",
        "\n",
        "        # [TITAN TIE-BREAKER PRESERVATION]\n",
        "        # If Ace is essentially tied (>99% accuracy case), we still force Ace for purity\n",
        "        if score_ace > 0.98 and abs(score_ace - strat_map[self.strategy_]) < 0.001:\n",
        "            self.strategy_ = \"ace\"\n",
        "\n",
        "        if self.verbose:\n",
        "             print(f\" >>> {self.strategy_.upper()} STRATEGY LOCKED. <<<\")\n",
        "\n",
        "        # --- PHASE 4: ASSIMILATION & JURY SELECTION ---\n",
        "        if self.verbose:\n",
        "            print(f\" > Phase 4: Final Assimilation (Retraining Elites & Jury)...\")\n",
        "\n",
        "        # 1. Train the 2 Kings (The Elites)\n",
        "        for unit in self.final_elites_:\n",
        "            unit.fit(X_scaled, y)\n",
        "\n",
        "        # 2. Train the Jury of 5 (Ranks 3-5 from Phase 3)\n",
        "        # These are the \"Internal Tests\" that sit in the meeting room\n",
        "        self.jury_ = []\n",
        "        jury_indices = top_12_indices[2:7] # Get the next 5 best models\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\" > [MEETING ROOM] Assembling the Jury of 5 for Conflict Resolution...\")\n",
        "\n",
        "        for idx in jury_indices:\n",
        "            unit = all_units[idx]\n",
        "            # Retrain on full data so their opinion is valid\n",
        "            try:\n",
        "                unit.fit(X_scaled, y)\n",
        "                self.jury_.append(unit)\n",
        "            except: pass # If a juror fails, we proceed with fewer\n",
        "\n",
        "        # --- PHASE 4.5: THE CHERRY ON TOP (Universal Death Ray - Conditional) ---\n",
        "        # 1. Construct the OOF Matrix of the WINNING Strategy (The Foundation)\n",
        "        # We preserve the safety of Linear/Council/Ace, whatever won Phase 3.\n",
        "        oof_champion = np.zeros_like(elite_preds[0])\n",
        "\n",
        "        # Helper to rebuild the exact blend that won Phase 3\n",
        "        current_weights = np.zeros(n_units)\n",
        "        if self.strategy_ == \"council\": current_weights = self.weights_council_\n",
        "        elif self.strategy_ == \"ace\": current_weights = self.weights_ace_\n",
        "        elif self.strategy_ == \"linear\": current_weights = self.weights_linear_\n",
        "        elif self.strategy_ == \"balance\": current_weights = self.weights_balance_\n",
        "        elif self.strategy_ == \"inv_linear\": current_weights = self.weights_inv_linear_\n",
        "        elif self.strategy_ == \"inv_council\": current_weights = self.weights_inv_council_\n",
        "\n",
        "        # Apply weights to OOFs (Reconstructing the Champion's Logic)\n",
        "        w1 = current_weights[top_12_indices[top_2_local_idx[0]]]\n",
        "        w2 = current_weights[top_12_indices[top_2_local_idx[1]]]\n",
        "\n",
        "        if (w1 + w2) > 0:\n",
        "            oof_champion = (w1 * elite_preds[0] + w2 * elite_preds[1]) / (w1 + w2)\n",
        "        else:\n",
        "            oof_champion = elite_preds[0] # Fallback\n",
        "\n",
        "        # Calculate the Baseline Score (The Floor)\n",
        "        champion_score = accuracy_score(y, self.classes_[np.argmax(oof_champion, axis=1)])\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\" > [HYPERNOVA] Locked Source Strategy: {self.strategy_.upper()} (Score: {champion_score:.4%})\")\n",
        "            print(f\" > [HYPERNOVA] Attempting to improve the Champion...\")\n",
        "\n",
        "        # 2. UNLEASH UNIT 26 (Train on the Champion's Mistakes)\n",
        "        self.unit_26.fit_hunt(X_scaled, y, elite_probs_oof=oof_champion)\n",
        "\n",
        "        # 3. VERIFY KILL (Strict Improvement Only)\n",
        "        death_ray_score = self.unit_26.verified_score_\n",
        "\n",
        "        # CRITICAL LOGIC: Only activate if we beat the Champion\n",
        "        # This guarantees Delta >= 0. We effectively 'Delete' the Death Ray if it fails.\n",
        "        if death_ray_score > (champion_score + 0.00001):\n",
        "             if self.verbose:\n",
        "                 gain = death_ray_score - champion_score\n",
        "                 print(f\" > [ALERT] DEATH RAY SUCCESSFUL. (New Score: {death_ray_score:.4%} | Gain: +{gain:.4%})\")\n",
        "                 print(f\" > [COMMAND] ATTACHING DEATH RAY TO {self.strategy_.upper()}.\")\n",
        "             self.death_ray_active_ = True\n",
        "        else:\n",
        "             if self.verbose:\n",
        "                 print(f\" > [DEATH RAY] Stand Down. No gain over {self.strategy_.upper()}. Keeping Safe Score.\")\n",
        "             self.death_ray_active_ = False\n",
        "\n",
        "\n",
        "        # --- PHASE 5: THE CHRONOS GATE ASSIMILATION (Correction: No Early Exit) ---\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\"*85)\n",
        "            print(f\" >>> PHASE 5: THE CHRONOS GATE ASSIMILATION (Training the Router) <<<\")\n",
        "            print(\"=\"*85)\n",
        "\n",
        "        # --- STEP 0: THE SAFETY VETO ---\n",
        "        current_champ_name = self.strategy_\n",
        "        if current_champ_name == \"death_ray\":\n",
        "            current_champ_score = self.unit_26.verified_score_\n",
        "        else:\n",
        "            current_champ_score = strat_map[self.strategy_]\n",
        "\n",
        "        score_logic = elite_accs[0]\n",
        "        score_geom = 0.0\n",
        "        for i, acc in enumerate(candidate_oof_accs):\n",
        "            name = all_names_map[top_12_indices[i]]\n",
        "            if \"Geom\" in name or \"SVM\" in name or \"Resonance\" in name or \"GRAVITY\" in name:\n",
        "                score_geom = max(score_geom, acc)\n",
        "\n",
        "        geometry_is_weak = score_logic > (score_geom + 0.015)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\" > [SAFETY SCAN] Logic Peak: {score_logic:.4%} | Geometry Peak: {score_geom:.4%}\")\n",
        "\n",
        "        # [TITAN FIX] REMOVED \"return self\". We just set flags and continue.\n",
        "        chronos_active = True\n",
        "\n",
        "        if geometry_is_weak:\n",
        "            if self.verbose:\n",
        "                print(f\" > [SAFETY VETO] Geometry is unstable. The Chronos Gate is SUSPENDED.\")\n",
        "                print(f\" > [SAFETY VETO] Locking Strategy to Phase 4 Winner: {current_champ_name.upper()}\")\n",
        "                print(f\" > [SAFETY] Disabling Ouroboros Reflection (Plan C).\")\n",
        "\n",
        "            self.reflection_loops = 0\n",
        "            chronos_active = False # Disable Gate training\n",
        "        else:\n",
        "            # Prepare for Gate Training\n",
        "            pass\n",
        "\n",
        "        # Only train Gate if Veto didn't fire\n",
        "        if chronos_active:\n",
        "            # 1. Generate OOFs\n",
        "            def get_oof_strat(weights_full):\n",
        "                combined = np.zeros_like(elite_preds[0])\n",
        "                w1 = weights_full[top_12_indices[top_2_local_idx[0]]]\n",
        "                w2 = weights_full[top_12_indices[top_2_local_idx[1]]]\n",
        "                if w1 > 0: combined += w1 * elite_preds[0]\n",
        "                if w2 > 0: combined += w2 * elite_preds[1]\n",
        "                sum_w = w1 + w2\n",
        "                if sum_w > 0: return combined / sum_w\n",
        "                return combined\n",
        "\n",
        "            oof_council = get_oof_strat(self.weights_council_)\n",
        "            oof_ace     = get_oof_strat(self.weights_ace_)\n",
        "            oof_linear  = get_oof_strat(self.weights_linear_)\n",
        "\n",
        "            if self.unit_26.verified_score_ > 0:\n",
        "                oof_death_ray = self.unit_26.predict_proba(np.hstack([X_scaled, elite_preds[0]]))\n",
        "            else:\n",
        "                oof_death_ray = oof_council\n",
        "\n",
        "            p1, p2 = elite_preds[0], elite_preds[1]\n",
        "            akashic_stack = np.hstack([p1, p2, p1*p2, p1+p2, np.abs(p1-p2)])\n",
        "            if self.verbose: print(\" > [AKASHIC] Syncing Attention Field...\")\n",
        "            self.unit_akashic.fit(akashic_stack, y)\n",
        "            oof_akashic = self.unit_akashic.attend([p1, p2, p1*p2, p1+p2, np.abs(p1-p2)])\n",
        "\n",
        "            # 4. Determine Winners\n",
        "            n_samples = len(y)\n",
        "            strat_scores = np.zeros((n_samples, 5))\n",
        "            true_class_idx = np.searchsorted(self.classes_, y)\n",
        "            row_idx = np.arange(n_samples)\n",
        "\n",
        "            strat_scores[:, 0] = oof_council[row_idx, true_class_idx]\n",
        "            strat_scores[:, 1] = oof_ace[row_idx, true_class_idx]\n",
        "            strat_scores[:, 2] = oof_linear[row_idx, true_class_idx]\n",
        "            strat_scores[:, 3] = oof_death_ray[row_idx, true_class_idx]\n",
        "            strat_scores[:, 4] = oof_akashic[row_idx, true_class_idx]\n",
        "\n",
        "            best_strat_indices = np.argmax(strat_scores, axis=1)\n",
        "            self.active_strategies_ = np.unique(best_strat_indices)\n",
        "            label_map = {orig: comp for comp, orig in enumerate(self.active_strategies_)}\n",
        "            compressed_labels = np.array([label_map[i] for i in best_strat_indices])\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\" > [CHRONOS] Active Strategies: {self.active_strategies_}\")\n",
        "                print(f\" > [CHRONOS] Training Gatekeeper on GPU T4 (Compressed Mode)...\")\n",
        "\n",
        "            self.unit_chronos.gate_keeper_.set_params(num_class=len(self.active_strategies_))\n",
        "\n",
        "            try:\n",
        "                gate_pred_compressed = cross_val_predict(\n",
        "                    self.unit_chronos.gate_keeper_, X_scaled, compressed_labels, cv=5, n_jobs=-1\n",
        "                )\n",
        "                oof_gate_final = np.zeros_like(elite_preds[0])\n",
        "                all_oofs = [oof_council, oof_ace, oof_linear, oof_death_ray, oof_akashic]\n",
        "                for i in range(n_samples):\n",
        "                    real_strat_idx = self.active_strategies_[gate_pred_compressed[i]]\n",
        "                    oof_gate_final[i] = all_oofs[real_strat_idx][i]\n",
        "                score_gate = accuracy_score(y, self.classes_[np.argmax(oof_gate_final, axis=1)])\n",
        "            except: score_gate = 0.0\n",
        "\n",
        "            if score_gate > (current_champ_score + 0.0015):\n",
        "                if self.verbose:\n",
        "                    print(f\" > [CHRONOS] Gatekeeper SUCCESS. (Gate: {score_gate:.4%} > Champ: {current_champ_score:.4%})\")\n",
        "                    print(\" > [CHRONOS] Constitution is now LIVING.\")\n",
        "                self.unit_chronos.gate_keeper_.fit(X_scaled, compressed_labels)\n",
        "                self.strategy_ = \"chronos_dynamic\"\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\" > [CHRONOS] Gatekeeper FAILED. (Gate: {score_gate:.4%} vs Champ: {current_champ_score:.4%})\")\n",
        "                    print(f\" > [CHRONOS] Reverting to Static Champion: {current_champ_name.upper()}\")\n",
        "                self.strategy_ = current_champ_name\n",
        "\n",
        "            self.reflection_loops = 4\n",
        "\n",
        "\n",
        "        # --- PHASE 6: THE NPU MEMORY ENGRAM ---\n",
        "        if self.verbose:\n",
        "            print(f\" > [NPU] Analyzing Final Strategy Mistakes for Long-Term Memory...\")\n",
        "\n",
        "        # 1. Get Hard Predictions from the Winner\n",
        "        final_train_probs = self.predict_proba(X)\n",
        "        final_train_preds = self.classes_[np.argmax(final_train_probs, axis=1)]\n",
        "\n",
        "        # 2. Train NPU with Weighted Error Focus\n",
        "        self.unit_npu.fit(X_scaled, y, final_train_preds)\n",
        "\n",
        "        if self.unit_npu.trained_:\n",
        "            if self.verbose:\n",
        "                print(f\" > [NPU] Memory Core Active. Error patterns memorized on T4 GPU.\")\n",
        "        else:\n",
        "            if self.verbose: print(f\" > [NPU] System Perfect. Memory Core Dormant.\")\n",
        "\n",
        "        if self.verbose: print(\"-\" * 85)\n",
        "\n",
        "\n",
        "        # --- PHASE 7: THE FINAL CONSTITUTION (Detailed Report) ---\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\"*90)\n",
        "            print(\" >>> PHASE 7: THE FINAL CONSTITUTION (Dual-Core Analysis) <<<\")\n",
        "            print(\"=\"*90)\n",
        "\n",
        "            # Determine Final Status\n",
        "            dr_status = \"VICTORIOUS\" if hasattr(self, 'death_ray_active_') and self.death_ray_active_ else \"REJECTED\"\n",
        "            dr_score_disp = self.unit_26.verified_score_ if hasattr(self.unit_26, 'verified_score_') else 0.0\n",
        "\n",
        "            # Print The Battle Outcome\n",
        "            print(f\" [A] STANDARD CHAMPION: {current_champ_name.upper():<12} (Internal Score: {current_champ_score:.4%})\")\n",
        "            print(f\" [B] THE DEATH RAY:     {dr_status:<12} (Internal Score: {dr_score_disp:.4%})\")\n",
        "            print(\"-\" * 90)\n",
        "\n",
        "            # Print The Elites\n",
        "            final_strat = \"DEATH_RAY\" if hasattr(self, 'death_ray_active_') and self.death_ray_active_ else current_champ_name.upper()\n",
        "            print(f\" >>> ACTIVE CONFIGURATION: {final_strat}\")\n",
        "            print(f\" {'RANK':<4} | {'UNIT CLASS':<25} | {'WEIGHT':<8} | {'FINE DETAILS / DNA'}\")\n",
        "            print(\"-\" * 90)\n",
        "\n",
        "            # Reconstruct Weights for display\n",
        "            disp_weights = [0.0, 0.0]\n",
        "            if final_strat == \"COUNCIL\": disp_weights = [75.0, 25.0]\n",
        "            elif final_strat == \"ACE\":     disp_weights = [90.0, 10.0]\n",
        "            elif final_strat == \"LINEAR\":  disp_weights = [60.0, 40.0]\n",
        "            elif final_strat == \"DEATH_RAY\": disp_weights = [95.0, 5.0] # Approx visual for DR\n",
        "\n",
        "            for i, unit in enumerate(self.final_elites_):\n",
        "                name = unit.__class__.__name__\n",
        "                details = \"Standard\"\n",
        "\n",
        "                # Extract Fine Details\n",
        "                if hasattr(unit, \"dna_\"):\n",
        "                    d = unit.dna_\n",
        "                    if \"freq\" in d: details = f\"[SOUL] Freq:{d['freq']:.2f} G:{d['gamma']:.2f} P:{d['p']:.1f}\"\n",
        "                    elif \"n_hidden\" in d: details = f\"[NEURAL] H:{d['n_hidden']} Act:{d['activation']}\"\n",
        "                    elif \"resonance\" in d: details = f\"[FOREST] Res:{d['resonance']:.2f} Shift:{d['shift']:.1f}\"\n",
        "                elif hasattr(unit, \"get_params\"):\n",
        "                    p = unit.get_params()\n",
        "                    if \"n_estimators\" in p: details = f\"[TREE] Trees:{p['n_estimators']}\"\n",
        "                    if \"C\" in p: details = f\"[SVM] C:{p['C']} Gamma:{p.get('gamma','?')}\"\n",
        "                    if \"n_neighbors\" in p: details = f\"[KNN] K:{p['n_neighbors']} w:{p.get('weights','?')}\"\n",
        "                    if \"max_iter\" in p and \"learning_rate\" in p: details = f\"[GBM] LR:{p['learning_rate']} Iter:{p['max_iter']}\"\n",
        "\n",
        "                print(f\" {i+1:02d}   | {name:<25} | {disp_weights[i]:.1f}%    | {details}\")\n",
        "\n",
        "            if final_strat == \"DEATH_RAY\":\n",
        "                 print(f\" 26   | ResidualBridgeUnit        | 5.0%     | [SNIPER] K:{self.unit_26.k_dynamic} Strength:{self.unit_26.best_factor_:.4f}\")\n",
        "\n",
        "            print(\"-\" * 90)\n",
        "\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # --- PHASE 8: THE EPIGENETIC SHIELD (BIONIC REPAIR) ---\n",
        "    def _apply_epigenetic_shield(self, current_probs, X_scaled):\n",
        "        \"\"\"\n",
        "        THE BIONIC REPAIR PROTOCOL.\n",
        "        Aim: \"Accuracy never drops.\"\n",
        "        Mechanism: Transductive Geometric Reprogramming.\n",
        "        \"\"\"\n",
        "        # 1. SCAN FOR DAMAGE (Entropy Check)\n",
        "        # We look for rows where the model is unsure (Max prob < 75%)\n",
        "        # These are the 'Damaged Cells'.\n",
        "        max_confidence = np.max(current_probs, axis=1)\n",
        "\n",
        "        # 2. IDENTIFY STEM CELLS (The Healthy Tissue)\n",
        "        # We look for rows where the model is 95% sure.\n",
        "        # These are the 'Anchors'.\n",
        "        mask_stem_cells = max_confidence > 0.85\n",
        "        mask_damaged = max_confidence < 0.75\n",
        "\n",
        "        # Optimization: If no damage or no stem cells, return immediately (Safety First)\n",
        "        if np.sum(mask_damaged) == 0 or np.sum(mask_stem_cells) == 0:\n",
        "            return current_probs\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\" > [EPIGENETICS] Bionic Repair Active. Scanning {np.sum(mask_damaged)} damaged points...\")\n",
        "\n",
        "        # 3. GEOMETRIC MAPPING (The Bionic Link)\n",
        "        # We map the geometry of the ENTIRE Test Set.\n",
        "        # This allows damaged cells to 'see' their healthy neighbors.\n",
        "        from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "        # Use p=2 (Euclidean) for physical distance\n",
        "        nn_bionic = NearestNeighbors(n_neighbors=7, metric='minkowski', p=2, n_jobs=-1)\n",
        "        nn_bionic.fit(X_scaled)\n",
        "\n",
        "        # Find neighbors for the damaged cells only\n",
        "        distances, indices = nn_bionic.kneighbors(X_scaled[mask_damaged])\n",
        "\n",
        "        # 4. REPROGRAMMING (The Repair Loop)\n",
        "        repaired_probs = current_probs.copy()\n",
        "        repair_count = 0\n",
        "\n",
        "        # Get the indices of the damaged rows\n",
        "        damaged_indices = np.where(mask_damaged)[0]\n",
        "\n",
        "        for i, idx_d in enumerate(damaged_indices):\n",
        "            # Look at the 7 neighbors\n",
        "            neighbor_indices = indices[i]\n",
        "\n",
        "            # CRITICAL SAFETY LOCK 1: Are the neighbors Stem Cells?\n",
        "            # We count how many neighbors are \"Healthy\" (High confidence)\n",
        "            stem_votes = []\n",
        "            for n_idx in neighbor_indices:\n",
        "                if mask_stem_cells[n_idx]:\n",
        "                    # Get the class the neighbor is confident about\n",
        "                    stem_votes.append(np.argmax(current_probs[n_idx]))\n",
        "\n",
        "            # CRITICAL SAFETY LOCK 2: Consensus\n",
        "            # We ONLY repair if we have at least 4 healthy neighbors AND they all agree.\n",
        "            if len(stem_votes) >= 4:\n",
        "                # Check for unanimity (or near unanimity)\n",
        "                most_common = max(set(stem_votes), key=stem_votes.count)\n",
        "                consensus_strength = stem_votes.count(most_common)\n",
        "\n",
        "                # If 4+ healthy neighbors say \"It's Class A\", we trust the Geometry over the Logic.\n",
        "                if consensus_strength >= 4:\n",
        "                    # REPROGRAMMING:\n",
        "                    # We don't just hard flip. We blend the neighbor's confidence into the damaged cell.\n",
        "                    # This raises the probability of the correct class naturally.\n",
        "                    target_class_idx = most_common\n",
        "\n",
        "                    # Create a \"Synthetic Stem Cell\" injection\n",
        "                    injection = np.zeros_like(repaired_probs[idx_d])\n",
        "                    injection[target_class_idx] = 1.0\n",
        "\n",
        "                    # Apply Repair: 50% Original + 50% Geometry\n",
        "                    repaired_probs[idx_d] = (0.5 * repaired_probs[idx_d]) + (0.5 * injection)\n",
        "                    repair_count += 1\n",
        "\n",
        "        if self.verbose and repair_count > 0:\n",
        "            print(f\" > [EPIGENETICS] Reprogramming Complete. {repair_count} cells repaired via Geometric Anchoring.\")\n",
        "\n",
        "        return repaired_probs\n",
        "\n",
        "\n",
        "    def _predict_council_internal(self, X):\n",
        "        # Fast prediction using pre-calculated weights\n",
        "        X_sc = self.scaler_.transform(X)\n",
        "        final_pred = None\n",
        "        all_units= [\n",
        "            # 1. Logic (01-11)\n",
        "            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "\n",
        "            # 2. THE NEW GPU FORESTS (18-21)\n",
        "            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n",
        "\n",
        "            # 3. Competitors\n",
        "            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n",
        "\n",
        "            # 4. Souls (12-17)\n",
        "            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n",
        "\n",
        "            # 5. Neural (25)\n",
        "            self.unit_25,\n",
        "            self.unit_26\n",
        "        ]\n",
        "        # ... inside the loop where you iterate over units ...\n",
        "        for i, unit in enumerate(all_units):\n",
        "            if active_weights[i] > 0:\n",
        "                try:\n",
        "                    # SPECIAL HANDLING FOR UNIT 26 (HYPERNOVA)\n",
        "                    if i == 25: # Unit 26 is index 25 in the all_units list\n",
        "                        # We need the Rank 1 Elite's prediction on this NEW data\n",
        "                        # Recalculate Rank 1 Elite (It's already in self.final_elites_[0])\n",
        "                        # This works because Phase 4 retrained them on all data.\n",
        "                        elite_p = self.final_elites_[0].predict_proba(X_scaled)\n",
        "\n",
        "                        # Fuse manually for prediction\n",
        "                        X_fused_new = np.hstack([X_scaled, elite_p])\n",
        "\n",
        "                        # Unit 26 predicts on Fused Data\n",
        "                        p = unit.predict_proba(X_fused_new)\n",
        "\n",
        "                    else:\n",
        "                        # Standard Units\n",
        "                        if hasattr(unit, \"predict_proba\"):\n",
        "                            p = unit.predict_proba(X_scaled)\n",
        "                        else:\n",
        "                            d = unit.decision_function(X_scaled)\n",
        "                            max_d = np.max(d, axis=1, keepdims=True)\n",
        "                            exp_d = np.exp(d - max_d)\n",
        "                            p = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "                    # Accumulate\n",
        "                    if final_pred is None: final_pred = active_weights[i] * p\n",
        "                    else: final_pred += active_weights[i] * p\n",
        "                except: pass\n",
        "\n",
        "\n",
        "        if final_pred is None: return np.zeros(len(X)) # Fallback\n",
        "        return self.classes_[np.argmax(final_pred, axis=1)]\n",
        "\n",
        "\n",
        "    def _predict_proba_council_internal(self, X_scaled):\n",
        "        \"\"\"\n",
        "        Calculates the weighted probability matrix of the Council.\n",
        "        Essential for the Alien-Z sharpening process.\n",
        "        \"\"\"\n",
        "        final_pred = None\n",
        "        # Must match the order in your init\n",
        "        all_units= [\n",
        "            # 1. Logic (01-11)\n",
        "            self.unit_01, self.unit_02, self.unit_03, self.unit_04, self.unit_05,\n",
        "            self.unit_06, self.unit_07, self.unit_08, self.unit_09, self.unit_10, self.unit_11,\n",
        "\n",
        "            # 2. THE NEW GPU FORESTS (18-21)\n",
        "            self.unit_18, self.unit_19, self.unit_20, self.unit_21,\n",
        "\n",
        "            # 3. Competitors\n",
        "            self.unit_bench_svm, self.unit_bench_rf, self.unit_bench_xgb,\n",
        "\n",
        "            # 4. Souls (12-17)\n",
        "            self.unit_12, self.unit_13, self.unit_14, self.unit_15, self.unit_16, self.unit_17,\n",
        "\n",
        "            # 5. Neural (25)\n",
        "            self.unit_25,\n",
        "            self.unit_26\n",
        "        ]\n",
        "\n",
        "        # ... inside the loop where you iterate over units ...\n",
        "        for i, unit in enumerate(all_units):\n",
        "            if active_weights[i] > 0:\n",
        "                try:\n",
        "                    # SPECIAL HANDLING FOR UNIT 26 (HYPERNOVA)\n",
        "                    if i == 25: # Unit 26 is index 25 in the all_units list\n",
        "                        # We need the Rank 1 Elite's prediction on this NEW data\n",
        "                        # Recalculate Rank 1 Elite (It's already in self.final_elites_[0])\n",
        "                        # This works because Phase 4 retrained them on all data.\n",
        "                        elite_p = self.final_elites_[0].predict_proba(X_scaled)\n",
        "\n",
        "                        # Fuse manually for prediction\n",
        "                        X_fused_new = np.hstack([X_scaled, elite_p])\n",
        "\n",
        "                        # Unit 26 predicts on Fused Data\n",
        "                        p = unit.predict_proba(X_fused_new)\n",
        "\n",
        "                    else:\n",
        "                        # Standard Units\n",
        "                        if hasattr(unit, \"predict_proba\"):\n",
        "                            p = unit.predict_proba(X_scaled)\n",
        "                        else:\n",
        "                            d = unit.decision_function(X_scaled)\n",
        "                            max_d = np.max(d, axis=1, keepdims=True)\n",
        "                            exp_d = np.exp(d - max_d)\n",
        "                            p = exp_d / np.sum(exp_d, axis=1, keepdims=True)\n",
        "\n",
        "                    # Accumulate\n",
        "                    if final_pred is None: final_pred = active_weights[i] * p\n",
        "                    else: final_pred += active_weights[i] * p\n",
        "                except: pass\n",
        "\n",
        "        # Safety fallback\n",
        "        if final_pred is None:\n",
        "            return np.ones((len(X_scaled), len(self.classes_))) / len(self.classes_)\n",
        "\n",
        "        # Normalize to ensure sum=1.0\n",
        "        return final_pred / np.sum(final_pred, axis=1, keepdims=True)\n",
        "\n",
        "    def _get_stack_features(self, X_scaled):\n",
        "        \"\"\"\n",
        "        Helper to gather predictions for the Linear Mirror strategy.\n",
        "        \"\"\"\n",
        "        X_stack_list = []\n",
        "        for unit in self.final_elites_:\n",
        "            if hasattr(unit, \"predict_proba\"):\n",
        "                p = unit.predict_proba(X_scaled)\n",
        "            else:\n",
        "                d = unit.decision_function(X_scaled)\n",
        "                p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n",
        "            X_stack_list.append(p)\n",
        "        return np.hstack(X_stack_list)\n",
        "\n",
        "\n",
        "    def _predict_mirror_internal(self, X, mode=\"hybrid\"):\n",
        "        X_sc = self.scaler_.transform(X)\n",
        "        X_stack_list = []\n",
        "        for unit in self.final_elites_:\n",
        "            if hasattr(unit, \"predict_proba\"): p = unit.predict_proba(X_sc)\n",
        "            else:\n",
        "                d = unit.decision_function(X_sc)\n",
        "                p = np.exp(d) / np.sum(np.exp(d), axis=1, keepdims=True)\n",
        "            X_stack_list.append(p)\n",
        "        X_stack = np.hstack(X_stack_list)\n",
        "\n",
        "        model = self.unit_mirror_hybrid if mode == \"hybrid\" else self.unit_mirror_linear\n",
        "        return model.predict(X_stack)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_scaled = self.scaler_.transform(X)\n",
        "\n",
        "\n",
        "        # --- STEP 1: THE KINGS SPEAK ---\n",
        "        p1 = self.final_elites_[0].predict_proba(X_scaled)\n",
        "        p2 = self.final_elites_[1].predict_proba(X_scaled)\n",
        "\n",
        "        # Calculate initial \"Safe\" Agreement\n",
        "        # If they agree, we average them. If they disagree, we need the Meeting.\n",
        "        pred1 = np.argmax(p1, axis=1)\n",
        "        pred2 = np.argmax(p2, axis=1)\n",
        "\n",
        "        # Identify Conflict Points (Where Class A != Class B)\n",
        "        conflict_mask = (pred1 != pred2)\n",
        "        n_conflicts = np.sum(conflict_mask)\n",
        "\n",
        "        # Base Belief (50/50 split of the Kings)\n",
        "        current_belief = (0.5 * p1) + (0.5 * p2)\n",
        "\n",
        "        # --- STEP 2: THE CONFLICT RESOLUTION MEETING ---\n",
        "        if n_conflicts > 0 and hasattr(self, \"jury_\") and len(self.jury_) > 0:\n",
        "            # We only run the \"5 Internal Tests\" on the disputed points\n",
        "            # (Technically we calculate for all for speed, but weight them only on conflicts)\n",
        "\n",
        "            jury_vote = np.zeros_like(p1)\n",
        "\n",
        "            for juror in self.jury_:\n",
        "                try:\n",
        "                    # The Juror runs their test\n",
        "                    p_juror = juror.predict_proba(X_scaled)\n",
        "                    jury_vote += p_juror\n",
        "                except: pass\n",
        "\n",
        "            # Normalize Jury Vote\n",
        "            if len(self.jury_) > 0:\n",
        "                jury_vote /= len(self.jury_)\n",
        "\n",
        "            # THE VERDICT:\n",
        "            # For points in conflict, we shift authority to the Majority (Kings + Jury)\n",
        "            # New Weight: 40% King 1 + 40% King 2 + 20% Jury Consensus\n",
        "            # This tips the scale decisively.\n",
        "\n",
        "            # Apply only to conflicts (vectorized)\n",
        "            # We add the Jury's signal *heavily* where the Kings fight\n",
        "            current_belief[conflict_mask] = (\n",
        "                0.4 * p1[conflict_mask] +\n",
        "                0.4 * p2[conflict_mask] +\n",
        "                0.2 * jury_vote[conflict_mask]\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        # --- PLAN B: THE AKASHIC FIELD ---\n",
        "        akashic_stack = [p1, p2, p1*p2, p1+p2, np.abs(p1-p2)]\n",
        "        p_akashic = self.unit_akashic.attend(akashic_stack)\n",
        "\n",
        "        # --- CONSTRUCT PROBABILITIES ---\n",
        "        p_council = (0.75 * p1) + (0.25 * p2)\n",
        "        p_ace     = (0.90 * p1) + (0.10 * p2)\n",
        "        p_linear  = (0.60 * p1) + (0.40 * p2)\n",
        "\n",
        "        # Death Ray Prediction (Only calculated if needed later, but defined here for safety)\n",
        "        X_fused = np.hstack([X_scaled, p1])\n",
        "\n",
        "        current_belief = None\n",
        "\n",
        "        # --- PATH A: THE CHRONOS GATE (Dynamic Routing) ---\n",
        "        if self.strategy_ == \"chronos_dynamic\":\n",
        "            final_gate_weights = np.zeros((len(X), 5))\n",
        "            compressed_weights = self.unit_chronos.gate_keeper_.predict_proba(X_scaled)\n",
        "\n",
        "            # Expand compressed weights back to full slots\n",
        "            for i, strat_idx in enumerate(self.active_strategies_):\n",
        "                final_gate_weights[:, strat_idx] = compressed_weights[:, i]\n",
        "\n",
        "            # Note: Death Ray in Gate is risky, defaulting to Council for that slot if inactive\n",
        "            p_dr_gate = self.unit_26.predict_proba(X_fused) if self.unit_26.verified_score_ > 0 else p_council\n",
        "\n",
        "            current_belief = (\n",
        "                final_gate_weights[:, 0:1] * p_council +\n",
        "                final_gate_weights[:, 1:2] * p_ace +\n",
        "                final_gate_weights[:, 2:3] * p_linear +\n",
        "                final_gate_weights[:, 3:4] * p_dr_gate +\n",
        "                final_gate_weights[:, 4:5] * p_akashic\n",
        "            )\n",
        "\n",
        "        # --- PATH B: THE STATIC CHAMPION (Fallback/Standard) ---\n",
        "        else:\n",
        "            # Dynamically select the exact weights of the winning strategy\n",
        "            if self.strategy_ == \"ace\":         current_belief = p_ace\n",
        "            elif self.strategy_ == \"council\":   current_belief = p_council\n",
        "            elif self.strategy_ == \"linear\":    current_belief = p_linear\n",
        "            # Handle the specific Inverse/Balance strats if they won\n",
        "            elif self.strategy_ == \"balance\":     current_belief = (0.50 * p1) + (0.50 * p2)\n",
        "            elif self.strategy_ == \"inv_linear\":  current_belief = (0.40 * p1) + (0.60 * p2)\n",
        "            elif self.strategy_ == \"inv_council\": current_belief = (0.30 * p1) + (0.70 * p2)\n",
        "            else:\n",
        "                current_belief = p_council\n",
        "\n",
        "        # --- PLAN C: THE OUROBOROS REFLECTION (Conditional) ---\n",
        "        if self.reflection_loops > 0:\n",
        "            for cycle in range(self.reflection_loops):\n",
        "                current_belief = self.unit_24.sharpen_probabilities(current_belief, X_scaled)\n",
        "\n",
        "        # --- PLAN D: THE NPU INTERCEPTOR (Final Layer) ---\n",
        "        if hasattr(self, \"unit_npu\") and self.unit_npu.trained_:\n",
        "            current_belief = self.unit_npu.correct(current_belief, X_scaled)\n",
        "\n",
        "        # --- PLAN E: THE DEATH RAY (Conditional Attachment) ---\n",
        "        # THIS IS THE FIX: Only apply if it was proven better in training\n",
        "        if hasattr(self, \"death_ray_active_\") and self.death_ray_active_:\n",
        "            # Fuse the CURRENT belief (Linear/Ace/etc) with Raw Features\n",
        "            X_fused_final = np.hstack([X_scaled, current_belief])\n",
        "            # Ask Unit 26 to correct it\n",
        "            current_belief = self.unit_26.predict_proba(X_fused_final)\n",
        "\n",
        "        # --- PHASE 8: THE EPIGENETIC SHIELD (New Invention) ---\n",
        "        # This applies the 'Bionic Repair' to the final belief before output.\n",
        "        # It ensures geometric consistency on the Test Set itself.\n",
        "        current_belief = self._apply_epigenetic_shield(current_belief, X_scaled)\n",
        "\n",
        "        return current_belief\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "\n",
        "def HarmonicResonanceForest_Ultimate(n_estimators=None):\n",
        "    return HarmonicResonanceClassifier_BEAST_21D(verbose=True)\n",
        "\n",
        "# --- ADD THIS AT THE ABSOLUTE BOTTOM ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Put your data loading here\n",
        "    # X, y = load_your_data()\n",
        "\n",
        "    # 2. Put your model execution here\n",
        "    # model = HarmonicResonanceForest_Ultimate()\n",
        "    # model.fit(X, y)\n",
        "\n",
        "    print(\"✅ Titan-21 Safety Protocol Engaged. System is stable.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils import resample\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Updated to accept custom_X and custom_y\n",
        "def run_comparative_benchmark(dataset_name, openml_id, sample_limit=3000, custom_X=None, custom_y=None):\n",
        "    print(f\"\\n[DATASET] Loading {dataset_name} (ID: {openml_id})...\")\n",
        "\n",
        "    try:\n",
        "        # --- PATH A: Custom Data Provided (Pre-cleaned) ---\n",
        "        if custom_X is not None and custom_y is not None:\n",
        "            print(\"  > Using provided Custom Data...\")\n",
        "            X = custom_X\n",
        "            y = custom_y\n",
        "\n",
        "            # Ensure X is numpy (in case a DF was passed)\n",
        "            if hasattr(X, 'values'):\n",
        "                X = X.values\n",
        "\n",
        "        # --- PATH B: Fetch from OpenML ---\n",
        "        else:\n",
        "            # Fetch as DataFrame to handle types better\n",
        "            X_df, y = fetch_openml(data_id=openml_id, return_X_y=True, as_frame=True, parser='auto')\n",
        "\n",
        "            # 1. AUTO-CLEANER: Convert Objects/Strings to Numbers (Only for DataFrames)\n",
        "            for col in X_df.columns:\n",
        "                if X_df[col].dtype == 'object' or X_df[col].dtype.name == 'category':\n",
        "                    le = LabelEncoder()\n",
        "                    X_df[col] = le.fit_transform(X_df[col].astype(str))\n",
        "\n",
        "            X = X_df.values # Convert to Numpy for HRF\n",
        "\n",
        "        # --- COMMON PIPELINE (NaN Handling) ---\n",
        "        # Even if custom data is passed, we double-check for NaNs to be safe\n",
        "        if np.isnan(X).any():\n",
        "            print(\"  > NaNs detected. Imputing with Mean strategy...\")\n",
        "            imp = SimpleImputer(strategy='mean')\n",
        "            X = imp.fit_transform(X)\n",
        "\n",
        "        le_y = LabelEncoder()\n",
        "        y = le_y.fit_transform(y)\n",
        "\n",
        "        # 3. GPU Limit Check\n",
        "        if len(X) > sample_limit:\n",
        "            print(f\"  ...Downsampling from {len(X)} to {sample_limit} (GPU Limit)...\")\n",
        "            X, y = resample(X, y, n_samples=sample_limit, random_state=42, stratify=y)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "        print(f\"  Shape: {X.shape} | Classes: {len(np.unique(y))}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    competitors = {\n",
        "        \"SVM (RBF)\": make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "        \"XGBoost (GPU)\": XGBClassifier(\n",
        "            device='cuda',\n",
        "            tree_method='hist',\n",
        "            #use_label_encoder=False,\n",
        "            eval_metric='logloss',\n",
        "            random_state=42\n",
        "        ),\n",
        "        # Ensure your HRF class is defined in the notebook before running this\n",
        "        \"HRF Ultimate (GPU)\": HarmonicResonanceForest_Ultimate(n_estimators=60)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    print(f\"\\n[BENCHMARK] Executing comparisons on {dataset_name}...\")\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"{'Model Name':<25} | {'Accuracy':<10} | {'Status'}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    hrf_acc = 0\n",
        "\n",
        "    for name, model in competitors.items():\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, preds)\n",
        "            results[name] = acc\n",
        "            print(f\"{name:<25} | {acc:.4%}    | Done\")\n",
        "\n",
        "            if \"HRF\" in name:\n",
        "                hrf_acc = acc\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{name:<25} | FAILED      | {e}\")\n",
        "\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    best_competitor = 0\n",
        "    for k, v in results.items():\n",
        "        if \"HRF\" not in k and v > best_competitor:\n",
        "            best_competitor = v\n",
        "\n",
        "    margin = hrf_acc - best_competitor\n",
        "\n",
        "    if margin > 0:\n",
        "        print(f\" HRF WINNING MARGIN: +{margin:.4%}\")\n",
        "    else:\n",
        "        print(f\" HRF GAP: {margin:.4%}\")"
      ],
      "metadata": {
        "id": "5Ah4TamVqjxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 15: Hill-Valley (Geometric Landscape)\n",
        "# ID: 1479 | Rows: 1,212 | Type: Topology\n",
        "# Hypothesis: This dataset is a pure test of \"Shape Detection.\"\n",
        "#             It is famous for defeating standard Gradient Boosters.\n",
        "#             The Resonance & Geometry sectors are required for the win.\n",
        "run_comparative_benchmark(\n",
        "    dataset_name=\"Hill-Valley\",\n",
        "    openml_id=1479,\n",
        "    sample_limit=1212  #100\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHaGCa4wB8Ab",
        "outputId": "4b61f24b-3cd3-4fa1-a9cf-0e58f3db3280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DATASET] Loading Hill-Valley (ID: 1479)...\n",
            "  Shape: (1212, 100) | Classes: 2\n",
            "\n",
            "[BENCHMARK] Executing comparisons on Hill-Valley...\n",
            "-----------------------------------------------------------------\n",
            "Model Name                | Accuracy   | Status\n",
            "-----------------------------------------------------------------\n",
            "SVM (RBF)                 | 48.9712%    | Done\n",
            "Random Forest             | 56.3786%    | Done\n",
            "XGBoost (GPU)             | 54.7325%    | Done\n",
            " >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\n",
            " > Initiating The Ouroboros Protocol (Stabilized)...\n",
            " > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\n",
            "    [Standard] Geom: 52.43% | Logic: 52.12% | HARMONIC: 52.27%\n",
            "    [Robust  ] Geom: 52.32% | Logic: 52.12% | HARMONIC: 52.22%\n",
            "    [MinMax  ] Geom: 52.12% | Logic: 52.12% | HARMONIC: 52.12%\n",
            " >>> LENS LOCKED: STANDARD SCALER (Consensus Achieved) <<<\n",
            " > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\n",
            "    >>> Resonance (SVM) Tuned: {'gamma': 'auto', 'C': 50.0} | Score: 58.62%\n",
            "    >>> Nu-Warp (NuSVC) Tuned: {'nu': 0.2, 'gamma': 'scale'} | Score: 66.98%\n",
            "    >>> Logic (HG) Tuned:      {'max_leaf_nodes': 31, 'max_iter': 100, 'learning_rate': 0.05, 'l2_regularization': 0.0} | Score: 54.39%\n",
            " > Phase 1: Awakening the Souls (Rapid Evolution)...\n",
            "--------------------------------------------------------------------------------\n",
            " UNIT NAME            | ACCURACY | EVOLVED DNA PARAMETERS\n",
            "--------------------------------------------------------------------------------\n",
            " SOUL-01 (Original)   | 56.19%  | Freq: 0.48 | Gamma: 4.33 | P: 2.0\n",
            " SOUL-02 (Mirror A)   | 54.64%  | Freq: 0.44 | Gamma: 0.50 | P: 2.0\n",
            " SOUL-03 (Mirror B)   | 51.55%  | Freq: 0.30 | Gamma: 1.02 | P: 2.0\n",
            " SOUL-D (AGI Hyper)   | 55.67%  | Freq: 0.52 | Gamma: 4.85 | P: 2.0\n",
            " SOUL-E (AGI Deep)    | 53.61%  | Freq: 0.49 | Gamma: 3.17 | P: 2.0\n",
            " SOUL-F (AGI Omni)    | 55.67%  | Freq: 0.53 | Gamma: 3.80 | P: 2.0\n",
            " NEURAL-ELM (Omni)    | 69.59%  | H:96 | Act:sigmoid | Alpha:0.12\n",
            " GOLDEN-FOREST        | 50.00%  | Res: 1.618 | Decay: 1.62 | Shift: 137.5\n",
            " ENTROPY-FOREST       | 51.55%  | Components: 100\n",
            " QUANTUM-FOREST       | 52.06%  | Gamma: 0.50 | N-Comp: 200\n",
            " GRAVITY-FOREST       | 51.55%  | Horizon: 10.0% | Power: 2.00\n",
            "--------------------------------------------------------------------------------\n",
            " > Phase 2: The Grand Qualifier (Scanning All 12 Candidates)...\n",
            "\n",
            "======================================================================\n",
            " >>> THE 21D PERFORMANCE MONITOR (Phase 2 Qualification) <<<\n",
            "======================================================================\n",
            " RANK   | UNIT NAME          | SCORE      | STATUS\n",
            "----------------------------------------------------------------------\n",
            " 01     | Neural-ELM         | 69.59%    | PROMOTED\n",
            " 02     | Nu-Warp            | 68.04%    | PROMOTED\n",
            " 03     | Resonance          | 60.31%    | PROMOTED\n",
            " 04     | Logic-HG           | 57.22%    | PROMOTED\n",
            " 05     | SOUL-Orig          | 56.19%    | PROMOTED\n",
            " 06     | SOUL-D(AGI)        | 55.67%    | PROMOTED\n",
            " 07     | Space-QDA          | 55.67%    | PROMOTED\n",
            " 08     | SOUL-F(AGI)        | 55.67%    | PROMOTED\n",
            " 09     | Logic-ET           | 55.67%    | PROMOTED\n",
            " 10     | BENCH-RF           | 54.64%    | PROMOTED\n",
            " 11     | SOUL-TwinA         | 54.64%    | PROMOTED\n",
            " 12     | SOUL-E(AGI)        | 53.61%    | PROMOTED\n",
            " 13     | Logic-RF           | 53.09%    | Eliminated\n",
            " 14     | Geom-K3            | 53.09%    | Eliminated\n",
            " 15     | BENCH-SVM          | 52.58%    | Eliminated\n",
            " 16     | QUANTUM-FOREST     | 52.06%    | Eliminated\n",
            " 17     | Grad-XG2           | 52.06%    | Eliminated\n",
            " 18     | PolyKer            | 52.06%    | Eliminated\n",
            " 19     | GRAVITY-FOREST     | 51.55%    | Eliminated\n",
            " 20     | ENTROPY-FOREST     | 51.55%    | Eliminated\n",
            " 21     | SOUL-TwinB         | 51.55%    | Eliminated\n",
            " 22     | BENCH-XGB          | 51.03%    | Eliminated\n",
            " 23     | Grad-XG1           | 50.00%    | Eliminated\n",
            " 24     | GOLDEN-FOREST      | 50.00%    | Eliminated\n",
            " 25     | Geom-K9            | 48.97%    | Eliminated\n",
            " 26     | THE DEATH RAY      | 0.00%    | Eliminated\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            " >>> PHASE 3: THE OUROBOROS PROTOCOL (Top 12 Candidates - 100% Validation) <<<\n",
            "================================================================================\n",
            " RANK | UNIT NAME          | OOF ACCURACY | STATUS\n",
            "--------------------------------------------------------------------------------\n",
            " 01   | Neural-ELM         | 69.1434%   | Validated\n",
            " 02   | Nu-Warp            | 65.4283%   | Validated\n",
            " 03   | Resonance          | 57.9979%   | Validated\n",
            " 04   | Logic-HG           | 55.3148%   | Validated\n",
            " 05   | SOUL-Orig          | 50.0516%   | Validated\n",
            " 06   | SOUL-D(AGI)        | 51.5996%   | Validated\n",
            " 07   | Space-QDA          | 54.3860%   | Validated\n",
            " 08   | SOUL-F(AGI)        | 51.5996%   | Validated\n",
            " 09   | Logic-ET           | 56.5531%   | Validated\n",
            " 10   | BENCH-RF           | 54.4892%   | Validated\n",
            " 11   | SOUL-TwinA         | 50.0516%   | Validated\n",
            " 12   | SOUL-E(AGI)        | 51.5996%   | Validated\n",
            "--------------------------------------------------------------------------------\n",
            " > [SAFETY] Neural-ELM attempted to join Council. Request DENIED (Restricted to Rank 3).\n",
            " > [STRATEGY LAB] Ace: 67.1827% | Council: 65.5315% | Linear: 63.6739%\n",
            " > [STRATEGY LAB] Balance: 63.0547% | Inv-Lin: 62.9515% | Underdog: 61.0939%\n",
            " >>> ACE STRATEGY LOCKED. <<<\n",
            " > Phase 4: Final Assimilation (Retraining Elites & Jury)...\n",
            " > [MEETING ROOM] Assembling the Jury of 5 for Conflict Resolution...\n",
            " > [HYPERNOVA] Locked Source Strategy: ACE (Score: 67.1827%)\n",
            " > [HYPERNOVA] Attempting to improve the Champion...\n",
            " > [DEATH RAY] Stand Down. No gain over ACE. Keeping Safe Score.\n",
            "\n",
            "=====================================================================================\n",
            " >>> PHASE 5: THE CHRONOS GATE ASSIMILATION (Training the Router) <<<\n",
            "=====================================================================================\n",
            " > [SAFETY SCAN] Logic Peak: 65.4283% | Geometry Peak: 57.9979%\n",
            " > [SAFETY VETO] Geometry is unstable. The Chronos Gate is SUSPENDED.\n",
            " > [SAFETY VETO] Locking Strategy to Phase 4 Winner: ACE\n",
            " > [SAFETY] Disabling Ouroboros Reflection (Plan C).\n",
            " > [NPU] Analyzing Final Strategy Mistakes for Long-Term Memory...\n",
            " > [NPU] Memory Core Active. Error patterns memorized on T4 GPU.\n",
            "-------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            " >>> PHASE 7: THE FINAL CONSTITUTION (Dual-Core Analysis) <<<\n",
            "==========================================================================================\n",
            " [A] STANDARD CHAMPION: ACE          (Internal Score: 67.1827%)\n",
            " [B] THE DEATH RAY:     REJECTED     (Internal Score: 67.1827%)\n",
            "------------------------------------------------------------------------------------------\n",
            " >>> ACTIVE CONFIGURATION: ACE\n",
            " RANK | UNIT CLASS                | WEIGHT   | FINE DETAILS / DNA\n",
            "------------------------------------------------------------------------------------------\n",
            " 01   | NuSVC                     | 90.0%    | Standard\n",
            " 02   | SVC                       | 10.0%    | [SVM] C:50.0 Gamma:auto\n",
            "------------------------------------------------------------------------------------------\n",
            "HRF Ultimate (GPU)        | 67.0782%    | Done\n",
            "-----------------------------------------------------------------\n",
            " HRF WINNING MARGIN: +10.6996%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 6: Japanese Vowels\n",
        "# ID: 375\n",
        "# Type: Audio / Speech (Harmonic Time-Series)\n",
        "#*\n",
        "run_comparative_benchmark(\n",
        "    dataset_name=\"Japanese Vowels\",\n",
        "    openml_id=375,\n",
        "    sample_limit= 9961 #14\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoVDyBjgAYml",
        "outputId": "572e866b-24b6-477d-dd53-fa2eb3ef4924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DATASET] Loading Japanese Vowels (ID: 375)...\n",
            "  Shape: (9961, 14) | Classes: 9\n",
            "\n",
            "[BENCHMARK] Executing comparisons on Japanese Vowels...\n",
            "-----------------------------------------------------------------\n",
            "Model Name                | Accuracy   | Status\n",
            "-----------------------------------------------------------------\n",
            "SVM (RBF)                 | 98.2439%    | Done\n",
            "Random Forest             | 97.0898%    | Done\n",
            "XGBoost (GPU)             | 97.9428%    | Done\n",
            " >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\n",
            " > Initiating The Ouroboros Protocol (Stabilized)...\n",
            " > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\n",
            "    [Standard] Geom: 92.05% | Logic: 69.40% | HARMONIC: 79.14%\n",
            "    [Robust  ] Geom: 91.60% | Logic: 69.40% | HARMONIC: 78.97%\n",
            "    [MinMax  ] Geom: 92.15% | Logic: 69.40% | HARMONIC: 79.17%\n",
            " >>> LENS LOCKED: MINMAX SCALER (Consensus Achieved) <<<\n",
            " > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\n",
            "    >>> Resonance (SVM) Tuned: {'gamma': 'scale', 'C': 50.0} | Score: 94.30%\n",
            "    >>> Nu-Warp (NuSVC) Tuned: {'nu': 0.2, 'gamma': 'scale'} | Score: 94.70%\n",
            "    >>> Logic (HG) Tuned:      {'max_leaf_nodes': 31, 'max_iter': 100, 'learning_rate': 0.05, 'l2_regularization': 0.0} | Score: 88.10%\n",
            " > Phase 1: Awakening the Souls (Rapid Evolution)...\n",
            "--------------------------------------------------------------------------------\n",
            " UNIT NAME            | ACCURACY | EVOLVED DNA PARAMETERS\n",
            "--------------------------------------------------------------------------------\n",
            " SOUL-01 (Original)   | 99.00%  | Freq: 4.53 | Gamma: 3.74 | P: 2.0\n",
            " SOUL-02 (Mirror A)   | 99.00%  | Freq: 3.71 | Gamma: 0.35 | P: 1.6\n",
            " SOUL-03 (Mirror B)   | 99.00%  | Freq: 4.16 | Gamma: 3.41 | P: 1.8\n",
            " SOUL-D (AGI Hyper)   | 99.06%  | Freq: 3.71 | Gamma: 0.96 | P: 2.0\n",
            " SOUL-E (AGI Deep)    | 99.06%  | Freq: 4.63 | Gamma: 4.70 | P: 2.4\n",
            " SOUL-F (AGI Omni)    | 99.06%  | Freq: 3.71 | Gamma: 2.88 | P: 2.0\n",
            " NEURAL-ELM (Omni)    | 97.37%  | H:314 | Act:swish | Alpha:0.10\n",
            " GOLDEN-FOREST        | 97.62%  | Res: 1.618 | Decay: 1.62 | Shift: 137.5\n",
            " ENTROPY-FOREST       | 85.38%  | Components: 100\n",
            " QUANTUM-FOREST       | 94.98%  | Gamma: 0.50 | N-Comp: 200\n",
            " GRAVITY-FOREST       | 82.06%  | Horizon: 10.0% | Power: 2.00\n",
            "--------------------------------------------------------------------------------\n",
            " > Phase 2: The Grand Qualifier (Scanning All 12 Candidates)...\n",
            "\n",
            "======================================================================\n",
            " >>> THE 21D PERFORMANCE MONITOR (Phase 2 Qualification) <<<\n",
            "======================================================================\n",
            " RANK   | UNIT NAME          | SCORE      | STATUS\n",
            "----------------------------------------------------------------------\n",
            " 01     | SOUL-E(AGI)        | 99.06%    | PROMOTED\n",
            " 02     | SOUL-F(AGI)        | 99.06%    | PROMOTED\n",
            " 03     | SOUL-D(AGI)        | 99.06%    | PROMOTED\n",
            " 04     | SOUL-Orig          | 99.00%    | PROMOTED\n",
            " 05     | SOUL-TwinA         | 99.00%    | PROMOTED\n",
            " 06     | SOUL-TwinB         | 99.00%    | PROMOTED\n",
            " 07     | Resonance          | 98.93%    | PROMOTED\n",
            " 08     | Logic-ET           | 98.56%    | PROMOTED\n",
            " 09     | Geom-K3            | 98.49%    | PROMOTED\n",
            " 10     | BENCH-SVM          | 98.18%    | PROMOTED\n",
            " 11     | PolyKer            | 98.12%    | PROMOTED\n",
            " 12     | Grad-XG2           | 97.80%    | PROMOTED\n",
            " 13     | GOLDEN-FOREST      | 97.62%    | Eliminated\n",
            " 14     | Geom-K9            | 97.49%    | Eliminated\n",
            " 15     | Logic-HG           | 97.24%    | Eliminated\n",
            " 16     | BENCH-XGB          | 97.24%    | Eliminated\n",
            " 17     | Logic-RF           | 97.05%    | Eliminated\n",
            " 18     | Nu-Warp            | 97.05%    | Eliminated\n",
            " 19     | Grad-XG1           | 96.93%    | Eliminated\n",
            " 20     | BENCH-RF           | 96.80%    | Eliminated\n",
            " 21     | Neural-ELM         | 96.24%    | Eliminated\n",
            " 22     | QUANTUM-FOREST     | 94.98%    | Eliminated\n",
            " 23     | Space-QDA          | 94.86%    | Eliminated\n",
            " 24     | ENTROPY-FOREST     | 85.38%    | Eliminated\n",
            " 25     | GRAVITY-FOREST     | 82.06%    | Eliminated\n",
            " 26     | THE DEATH RAY      | 0.00%    | Eliminated\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            " >>> PHASE 3: THE OUROBOROS PROTOCOL (Top 12 Candidates - 100% Validation) <<<\n",
            "================================================================================\n",
            " RANK | UNIT NAME          | OOF ACCURACY | STATUS\n",
            "--------------------------------------------------------------------------------\n",
            " 01   | SOUL-E(AGI)        | 95.6325%   | Validated\n",
            " 02   | SOUL-F(AGI)        | 95.6325%   | Validated\n",
            " 03   | SOUL-D(AGI)        | 95.6325%   | Validated\n",
            " 04   | SOUL-Orig          | 96.3730%   | Validated\n",
            " 05   | SOUL-TwinA         | 96.3730%   | Validated\n",
            " 06   | SOUL-TwinB         | 96.3730%   | Validated\n",
            " 07   | Resonance          | 99.1340%   | Validated\n",
            " 08   | Logic-ET           | 98.4438%   | Validated\n",
            " 09   | Geom-K3            | 98.1300%   | Validated\n",
            " 10   | BENCH-SVM          | 98.2806%   | Validated\n",
            " 11   | PolyKer            | 98.3936%   | Validated\n",
            " 12   | Grad-XG2           | 98.0924%   | Validated\n",
            "--------------------------------------------------------------------------------\n",
            " > [STRATEGY LAB] Ace: 99.1215% | Council: 99.1466% | Linear: 99.1717%\n",
            " > [STRATEGY LAB] Balance: 99.1968% | Inv-Lin: 99.1591% | Underdog: 99.1340%\n",
            " >>> ACE STRATEGY LOCKED. <<<\n",
            " > Phase 4: Final Assimilation (Retraining Elites & Jury)...\n",
            " > [MEETING ROOM] Assembling the Jury of 5 for Conflict Resolution...\n",
            " > [HYPERNOVA] Locked Source Strategy: ACE (Score: 99.1215%)\n",
            " > [HYPERNOVA] Attempting to improve the Champion...\n",
            " > [ALERT] DEATH RAY SUCCESSFUL. (New Score: 99.1340% | Gain: +0.0126%)\n",
            " > [COMMAND] ATTACHING DEATH RAY TO ACE.\n",
            "\n",
            "=====================================================================================\n",
            " >>> PHASE 5: THE CHRONOS GATE ASSIMILATION (Training the Router) <<<\n",
            "=====================================================================================\n",
            " > [SAFETY SCAN] Logic Peak: 99.1340% | Geometry Peak: 99.1340%\n",
            " > [AKASHIC] Syncing Attention Field...\n",
            " > [CHRONOS] Active Strategies: [2 3]\n",
            " > [CHRONOS] Training Gatekeeper on GPU T4 (Compressed Mode)...\n",
            " > [CHRONOS] Gatekeeper FAILED. (Gate: 0.0000% vs Champ: 99.1215%)\n",
            " > [CHRONOS] Reverting to Static Champion: ACE\n",
            " > [NPU] Analyzing Final Strategy Mistakes for Long-Term Memory...\n",
            " > [NPU] Memory Core Active. Error patterns memorized on T4 GPU.\n",
            "-------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            " >>> PHASE 7: THE FINAL CONSTITUTION (Dual-Core Analysis) <<<\n",
            "==========================================================================================\n",
            " [A] STANDARD CHAMPION: ACE          (Internal Score: 99.1215%)\n",
            " [B] THE DEATH RAY:     VICTORIOUS   (Internal Score: 99.1340%)\n",
            "------------------------------------------------------------------------------------------\n",
            " >>> ACTIVE CONFIGURATION: DEATH_RAY\n",
            " RANK | UNIT CLASS                | WEIGHT   | FINE DETAILS / DNA\n",
            "------------------------------------------------------------------------------------------\n",
            " 01   | SVC                       | 95.0%    | [SVM] C:50.0 Gamma:scale\n",
            " 02   | ExtraTreesClassifier      | 5.0%    | [TREE] Trees:1000\n",
            " 26   | ResidualBridgeUnit        | 5.0%     | [SNIPER] K:21 Strength:0.1200\n",
            "------------------------------------------------------------------------------------------\n",
            "HRF Ultimate (GPU)        | 99.1470%    | Done\n",
            "-----------------------------------------------------------------\n",
            " HRF WINNING MARGIN: +0.9032%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 2: Phoneme (Star Noise)\n",
        "# ID: 1489\n",
        "# Type: Audio/Harmonic Time-Series\n",
        "# Though originally for speech, the high-frequency harmonics in this data mimic the acoustic oscillations of stars (Asteroseismology).\n",
        "\n",
        "run_comparative_benchmark(\n",
        "    dataset_name=\"Phoneme\",\n",
        "    openml_id=1489,\n",
        "    sample_limit=5404 #6\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5GECikmpqFc",
        "outputId": "f853e54f-98de-4f43-c265-03e8026d107a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DATASET] Loading Phoneme (ID: 1489)...\n",
            "  Shape: (5404, 5) | Classes: 2\n",
            "\n",
            "[BENCHMARK] Executing comparisons on Phoneme...\n",
            "-----------------------------------------------------------------\n",
            "Model Name                | Accuracy   | Status\n",
            "-----------------------------------------------------------------\n",
            "SVM (RBF)                 | 83.2562%    | Done\n",
            "Random Forest             | 90.1018%    | Done\n",
            "XGBoost (GPU)             | 87.0490%    | Done\n",
            " >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\n",
            " > Initiating The Ouroboros Protocol (Stabilized)...\n",
            " > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\n",
            "    [Standard] Geom: 84.10% | Logic: 80.50% | HARMONIC: 82.26%\n",
            "    [Robust  ] Geom: 84.65% | Logic: 80.50% | HARMONIC: 82.52%\n",
            "    [MinMax  ] Geom: 83.85% | Logic: 80.50% | HARMONIC: 82.14%\n",
            " >>> LENS LOCKED: ROBUST SCALER (Consensus Achieved) <<<\n",
            " > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\n",
            "    >>> Resonance (SVM) Tuned: {'gamma': 'scale', 'C': 50.0} | Score: 82.90%\n",
            "    >>> Nu-Warp (NuSVC) Tuned: {'nu': 0.1, 'gamma': 'scale'} | Score: 84.30%\n",
            "    >>> Logic (HG) Tuned:      {'max_leaf_nodes': 31, 'max_iter': 100, 'learning_rate': 0.05, 'l2_regularization': 0.0} | Score: 86.20%\n",
            " > Phase 1: Awakening the Souls (Rapid Evolution)...\n",
            "--------------------------------------------------------------------------------\n",
            " UNIT NAME            | ACCURACY | EVOLVED DNA PARAMETERS\n",
            "--------------------------------------------------------------------------------\n",
            " SOUL-01 (Original)   | 89.02%  | Freq: 2.32 | Gamma: 1.83 | P: 1.8\n",
            " SOUL-02 (Mirror A)   | 89.71%  | Freq: 2.26 | Gamma: 0.50 | P: 2.4\n",
            " SOUL-03 (Mirror B)   | 89.48%  | Freq: 3.18 | Gamma: 4.90 | P: 2.0\n",
            " SOUL-D (AGI Hyper)   | 89.25%  | Freq: 1.84 | Gamma: 4.15 | P: 2.0\n",
            " SOUL-E (AGI Deep)    | 89.83%  | Freq: 2.25 | Gamma: 2.37 | P: 2.3\n",
            " SOUL-F (AGI Omni)    | 89.71%  | Freq: 1.94 | Gamma: 1.59 | P: 2.0\n",
            " NEURAL-ELM (Omni)    | 86.01%  | H:148 | Act:tanh | Alpha:0.12\n",
            " GOLDEN-FOREST        | 89.94%  | Res: 1.618 | Decay: 1.62 | Shift: 137.5\n",
            " ENTROPY-FOREST       | 75.26%  | Components: 100\n",
            " QUANTUM-FOREST       | 81.16%  | Gamma: 0.50 | N-Comp: 200\n",
            " GRAVITY-FOREST       | 76.18%  | Horizon: 10.0% | Power: 2.00\n",
            "--------------------------------------------------------------------------------\n",
            " > Phase 2: The Grand Qualifier (Scanning All 12 Candidates)...\n",
            "\n",
            "======================================================================\n",
            " >>> THE 21D PERFORMANCE MONITOR (Phase 2 Qualification) <<<\n",
            "======================================================================\n",
            " RANK   | UNIT NAME          | SCORE      | STATUS\n",
            "----------------------------------------------------------------------\n",
            " 01     | Logic-ET           | 90.75%    | PROMOTED\n",
            " 02     | Logic-RF           | 90.52%    | PROMOTED\n",
            " 03     | BENCH-RF           | 90.29%    | PROMOTED\n",
            " 04     | GOLDEN-FOREST      | 89.94%    | PROMOTED\n",
            " 05     | SOUL-E(AGI)        | 89.83%    | PROMOTED\n",
            " 06     | SOUL-TwinA         | 89.71%    | PROMOTED\n",
            " 07     | SOUL-F(AGI)        | 89.71%    | PROMOTED\n",
            " 08     | SOUL-TwinB         | 89.48%    | PROMOTED\n",
            " 09     | BENCH-XGB          | 89.25%    | PROMOTED\n",
            " 10     | SOUL-D(AGI)        | 89.25%    | PROMOTED\n",
            " 11     | Geom-K3            | 89.13%    | PROMOTED\n",
            " 12     | SOUL-Orig          | 89.02%    | PROMOTED\n",
            " 13     | Geom-K9            | 88.67%    | Eliminated\n",
            " 14     | Grad-XG2           | 88.32%    | Eliminated\n",
            " 15     | Logic-HG           | 88.32%    | Eliminated\n",
            " 16     | Grad-XG1           | 87.98%    | Eliminated\n",
            " 17     | Neural-ELM         | 86.01%    | Eliminated\n",
            " 18     | BENCH-SVM          | 84.97%    | Eliminated\n",
            " 19     | Resonance          | 84.62%    | Eliminated\n",
            " 20     | Nu-Warp            | 83.12%    | Eliminated\n",
            " 21     | QUANTUM-FOREST     | 81.16%    | Eliminated\n",
            " 22     | Space-QDA          | 78.61%    | Eliminated\n",
            " 23     | PolyKer            | 77.92%    | Eliminated\n",
            " 24     | GRAVITY-FOREST     | 76.18%    | Eliminated\n",
            " 25     | ENTROPY-FOREST     | 75.26%    | Eliminated\n",
            " 26     | THE DEATH RAY      | 0.00%    | Eliminated\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            " >>> PHASE 3: THE OUROBOROS PROTOCOL (Top 12 Candidates - 100% Validation) <<<\n",
            "================================================================================\n",
            " RANK | UNIT NAME          | OOF ACCURACY | STATUS\n",
            "--------------------------------------------------------------------------------\n",
            " 01   | Logic-ET           | 91.1173%   | Validated\n",
            " 02   | Logic-RF           | 90.5621%   | Validated\n",
            " 03   | BENCH-RF           | 90.2614%   | Validated\n",
            " 04   | GOLDEN-FOREST      | 89.6137%   | Validated\n",
            " 05   | SOUL-E(AGI)        | 85.2417%   | Validated\n",
            " 06   | SOUL-TwinA         | 85.4962%   | Validated\n",
            " 07   | SOUL-F(AGI)        | 85.2417%   | Validated\n",
            " 08   | SOUL-TwinB         | 85.4962%   | Validated\n",
            " 09   | BENCH-XGB          | 89.3361%   | Validated\n",
            " 10   | SOUL-D(AGI)        | 85.2417%   | Validated\n",
            " 11   | Geom-K3            | 88.6190%   | Validated\n",
            " 12   | SOUL-Orig          | 85.4962%   | Validated\n",
            "--------------------------------------------------------------------------------\n",
            " > [STRATEGY LAB] Ace: 91.0941% | Council: 91.0248% | Linear: 91.1173%\n",
            " > [STRATEGY LAB] Balance: 91.0248% | Inv-Lin: 91.0941% | Underdog: 91.0941%\n",
            " >>> LINEAR STRATEGY LOCKED. <<<\n",
            " > Phase 4: Final Assimilation (Retraining Elites & Jury)...\n",
            " > [MEETING ROOM] Assembling the Jury of 5 for Conflict Resolution...\n",
            " > [HYPERNOVA] Locked Source Strategy: LINEAR (Score: 91.1173%)\n",
            " > [HYPERNOVA] Attempting to improve the Champion...\n",
            " > [DEATH RAY] Stand Down. No gain over LINEAR. Keeping Safe Score.\n",
            "\n",
            "=====================================================================================\n",
            " >>> PHASE 5: THE CHRONOS GATE ASSIMILATION (Training the Router) <<<\n",
            "=====================================================================================\n",
            " > [SAFETY SCAN] Logic Peak: 91.1173% | Geometry Peak: 88.6190%\n",
            " > [SAFETY VETO] Geometry is unstable. The Chronos Gate is SUSPENDED.\n",
            " > [SAFETY VETO] Locking Strategy to Phase 4 Winner: LINEAR\n",
            " > [SAFETY] Disabling Ouroboros Reflection (Plan C).\n",
            " > [NPU] Analyzing Final Strategy Mistakes for Long-Term Memory...\n",
            " > [NPU] System Perfect. Memory Core Dormant.\n",
            "-------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            " >>> PHASE 7: THE FINAL CONSTITUTION (Dual-Core Analysis) <<<\n",
            "==========================================================================================\n",
            " [A] STANDARD CHAMPION: LINEAR       (Internal Score: 91.1173%)\n",
            " [B] THE DEATH RAY:     REJECTED     (Internal Score: 91.1173%)\n",
            "------------------------------------------------------------------------------------------\n",
            " >>> ACTIVE CONFIGURATION: LINEAR\n",
            " RANK | UNIT CLASS                | WEIGHT   | FINE DETAILS / DNA\n",
            "------------------------------------------------------------------------------------------\n",
            " 01   | ExtraTreesClassifier      | 60.0%    | [TREE] Trees:1000\n",
            " 02   | RandomForestClassifier    | 40.0%    | [TREE] Trees:1000\n",
            "------------------------------------------------------------------------------------------\n",
            "HRF Ultimate (GPU)        | 90.2868%    | Done\n",
            "-----------------------------------------------------------------\n",
            " HRF WINNING MARGIN: +0.1850%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 1: EEG Eye State\n",
        "# ID: 1471\n",
        "# Type: Biological Time-Series (Periodic)\n",
        "\n",
        "run_comparative_benchmark(\n",
        "    dataset_name=\"EEG Eye State\",\n",
        "    openml_id=1471,\n",
        "    sample_limit= 14980 #15  # Fast Mode Active\n",
        ")"
      ],
      "metadata": {
        "id": "_uzXUKjCqNC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53a7722-d9f4-4327-caea-de1e9939bf45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DATASET] Loading EEG Eye State (ID: 1471)...\n",
            "  Shape: (14980, 14) | Classes: 2\n",
            "\n",
            "[BENCHMARK] Executing comparisons on EEG Eye State...\n",
            "-----------------------------------------------------------------\n",
            "Model Name                | Accuracy   | Status\n",
            "-----------------------------------------------------------------\n",
            "SVM (RBF)                 | 69.3925%    | Done\n",
            "Random Forest             | 93.0908%    | Done\n",
            "XGBoost (GPU)             | 93.5915%    | Done\n",
            " >>> THE 21D SOPHISTICATED DIMENSIONALITY INITIATED <<<\n",
            " > Initiating The Ouroboros Protocol (Stabilized)...\n",
            " > Phase -1: Selecting Universal Lens (Geometry + Logic Consensus)...\n",
            "    [Standard] Geom: 86.15% | Logic: 71.15% | HARMONIC: 77.93%\n",
            "    [Robust  ] Geom: 86.35% | Logic: 71.15% | HARMONIC: 78.02%\n",
            "    [MinMax  ] Geom: 86.55% | Logic: 71.15% | HARMONIC: 78.10%\n",
            " >>> LENS LOCKED: MINMAX SCALER (Consensus Achieved) <<<\n",
            " > Phase 0: Calibrating Logic & Manifold Units (Flash-Tune)...\n",
            "    >>> Resonance (SVM) Tuned: {'gamma': 'scale', 'C': 50.0} | Score: 55.00%\n",
            "    >>> Nu-Warp (NuSVC) Tuned: {'nu': 0.2, 'gamma': 'auto'} | Score: 52.30%\n",
            "    >>> Logic (HG) Tuned:      {'max_leaf_nodes': 31, 'max_iter': 100, 'learning_rate': 0.05, 'l2_regularization': 0.0} | Score: 78.50%\n",
            " > Phase 1: Awakening the Souls (Rapid Evolution)...\n",
            "--------------------------------------------------------------------------------\n",
            " UNIT NAME            | ACCURACY | EVOLVED DNA PARAMETERS\n",
            "--------------------------------------------------------------------------------\n",
            " SOUL-01 (Original)   | 91.99%  | Freq: 129.25 | Gamma: 0.87 | P: 1.8\n",
            " SOUL-02 (Mirror A)   | 91.20%  | Freq: 129.25 | Gamma: 2.96 | P: 2.9\n",
            " SOUL-03 (Mirror B)   | 91.99%  | Freq: 129.25 | Gamma: 3.93 | P: 2.2\n",
            " SOUL-D (AGI Hyper)   | 91.66%  | Freq: 148.13 | Gamma: 0.50 | P: 2.1\n",
            " SOUL-E (AGI Deep)    | 91.86%  | Freq: 151.81 | Gamma: 3.85 | P: 1.9\n",
            " SOUL-F (AGI Omni)    | 91.20%  | Freq: 171.49 | Gamma: 0.50 | P: 2.0\n",
            " NEURAL-ELM (Omni)    | 61.45%  | H:61 | Act:tanh | Alpha:0.12\n",
            " GOLDEN-FOREST        | 88.57%  | Res: 1.618 | Decay: 1.62 | Shift: 137.5\n",
            " ENTROPY-FOREST       | 46.31%  | Components: 100\n",
            " QUANTUM-FOREST       | 57.78%  | Gamma: 0.50 | N-Comp: 200\n",
            " GRAVITY-FOREST       | 57.91%  | Horizon: 10.0% | Power: 2.00\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def generate_hydra_manifold(n_samples=5000, complexity=0.8):\n",
        "    print(f\" [GENESIS] Forging The Hydra Manifold (n={n_samples})...\")\n",
        "\n",
        "    # 1. GEOMETRY SECTOR (Spiral/Circles) - Hard for Logic, Good for Z\n",
        "    X_geom, y_geom = make_circles(n_samples=n_samples, factor=0.5, noise=0.1)\n",
        "\n",
        "    # 2. LOGIC SECTOR (Moons/XOR) - Good for Trees\n",
        "    X_logic, y_logic = make_moons(n_samples=n_samples, noise=0.2)\n",
        "\n",
        "    # 3. RESONANCE SECTOR (Sine Waves) - Good for Soul\n",
        "    # We create a frequency feature that correlates with the target\n",
        "    rng = np.random.RandomState(42)\n",
        "    freq = rng.uniform(0, 4*np.pi, n_samples)\n",
        "    X_res = np.sin(freq).reshape(-1, 1)\n",
        "\n",
        "    # --- FUSION ---\n",
        "    # We stack them. This creates a multi-dimensional conflict.\n",
        "    # The target y is a mix: mainly geometry, but flipped by resonance.\n",
        "\n",
        "    X_base = np.hstack([X_geom, X_logic, X_res])\n",
        "\n",
        "    # Add pure noise dimensions to distract weak models\n",
        "    X_noise = rng.normal(0, 1, (n_samples, 18)) # Total 21 dimensions\n",
        "    X_final = np.hstack([X_base, X_noise])\n",
        "\n",
        "    # The Truth Label: A complex interaction\n",
        "    # If inside circle AND high frequency -> Class 1, else Class 0\n",
        "    # This requires 'Gating' (switching logic based on region).\n",
        "    y_final = np.logical_xor(y_geom, (X_res.flatten() > 0)).astype(int)\n",
        "\n",
        "    # Flip 5% labels to create \"Irreducible Error\" (Death Ray bait)\n",
        "    flip_idx = rng.choice(n_samples, int(n_samples * 0.05), replace=False)\n",
        "    y_final[flip_idx] = 1 - y_final[flip_idx]\n",
        "\n",
        "    print(\" [GENESIS] Hydra Manifold Complete. Dimensions: 21 | Classes: 2\")\n",
        "    return X_final, y_final\n",
        "\n",
        "# Create the data\n",
        "X_hydra, y_hydra = generate_hydra_manifold(n_samples=4000)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "mtO9zS_3ypPG",
        "outputId": "c6436b58-67f7-437e-d8cb-98074ce16fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from sklearn.datasets import make_moons, make_circles, make_classification\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef generate_hydra_manifold(n_samples=5000, complexity=0.8):\\n    print(f\" [GENESIS] Forging The Hydra Manifold (n={n_samples})...\")\\n\\n    # 1. GEOMETRY SECTOR (Spiral/Circles) - Hard for Logic, Good for Z\\n    X_geom, y_geom = make_circles(n_samples=n_samples, factor=0.5, noise=0.1)\\n\\n    # 2. LOGIC SECTOR (Moons/XOR) - Good for Trees\\n    X_logic, y_logic = make_moons(n_samples=n_samples, noise=0.2)\\n\\n    # 3. RESONANCE SECTOR (Sine Waves) - Good for Soul\\n    # We create a frequency feature that correlates with the target\\n    rng = np.random.RandomState(42)\\n    freq = rng.uniform(0, 4*np.pi, n_samples)\\n    X_res = np.sin(freq).reshape(-1, 1)\\n\\n    # --- FUSION ---\\n    # We stack them. This creates a multi-dimensional conflict.\\n    # The target y is a mix: mainly geometry, but flipped by resonance.\\n\\n    X_base = np.hstack([X_geom, X_logic, X_res])\\n\\n    # Add pure noise dimensions to distract weak models\\n    X_noise = rng.normal(0, 1, (n_samples, 18)) # Total 21 dimensions\\n    X_final = np.hstack([X_base, X_noise])\\n\\n    # The Truth Label: A complex interaction\\n    # If inside circle AND high frequency -> Class 1, else Class 0\\n    # This requires \\'Gating\\' (switching logic based on region).\\n    y_final = np.logical_xor(y_geom, (X_res.flatten() > 0)).astype(int)\\n\\n    # Flip 5% labels to create \"Irreducible Error\" (Death Ray bait)\\n    flip_idx = rng.choice(n_samples, int(n_samples * 0.05), replace=False)\\n    y_final[flip_idx] = 1 - y_final[flip_idx]\\n\\n    print(\" [GENESIS] Hydra Manifold Complete. Dimensions: 21 | Classes: 2\")\\n    return X_final, y_final\\n\\n# Create the data\\nX_hydra, y_hydra = generate_hydra_manifold(n_samples=4000)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# --- EXECUTE TITAN PROTOCOL ON HYDRA ---\n",
        "run_comparative_benchmark(\n",
        "    dataset_name=\"The Hydra Manifold (Synthetic)\",\n",
        "    openml_id=None, # Custom mode\n",
        "    sample_limit=1000,\n",
        "    custom_X=X_hydra,\n",
        "    custom_y=y_hydra\n",
        ")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "id": "AQD4LrP1ysnk",
        "outputId": "02a607fb-f4e8-4172-8bd6-71673361686a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# --- EXECUTE TITAN PROTOCOL ON HYDRA ---\\nrun_comparative_benchmark(\\n    dataset_name=\"The Hydra Manifold (Synthetic)\",\\n    openml_id=None, # Custom mode\\n    sample_limit=1000,\\n    custom_X=X_hydra,\\n    custom_y=y_hydra\\n)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUQUN_kDpqDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQIEJMuOpqBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVqkBfLWFodt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}